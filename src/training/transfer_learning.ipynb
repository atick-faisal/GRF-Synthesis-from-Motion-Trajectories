{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/GRF-Synthesis-from-Motion-Trajectories/blob/main/src/training/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6yBebZQzQL76"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pandas==1.3.4"
      ],
      "metadata": {
        "id": "h74L_Qf_Q22z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id \"1kNGwWBQp6kmAioLP-_zWx7r42SzoEO3v\"\n",
        "# !unzip UNet.zip\n",
        "\n",
        "## DB2 High vel\n",
        "# !gdown --id \"1LRwmMUFF1Vdxlsj2mqqkVvhCLTCIr8F4\"\n",
        "\n",
        "# !gdown --id \"1Z6SiogUQExhFuEqlAzdDyGZo8bOJR3DJ\""
      ],
      "metadata": {
        "id": "vyR0w49LQ9n_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jhlBmxZIQL73"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import scipy\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from UNet_1DCNN import UNet\n",
        "from FPN_1DCNN import FPN\n",
        "from AlbuNet_1DCNN import AlbUNet\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=2.5)\n",
        "\n",
        "sns.set_style(\"white\", {'font.family':'serif', 'font.serif':'Times New Roman'})\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/Results/TF/Force/Models/Z/'\n",
        "figures_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/Results/TF/Force/Figures/Z/'\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Research/GRF Data Synthesis/Results/DB1/Force/Models/Z/MultiResUNet00_paper_v1.h5'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "def correlation_coefficient_loss(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\n",
        "    error = mae(y_true, y_pred)\n",
        "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "    \n",
        "    return (1 - r) * error"
      ],
      "metadata": {
        "id": "ocrrq_j40RWV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = joblib.load('/content/data2_f72_t3_n1_combined.joblib')\n",
        "# data1 = joblib.load('/content/data1_f15_t3_n1_combined.joblib')\n",
        "data2 = joblib.load('/content/data2_f15_t6_n1_high_vel_combined.joblib')\n",
        "# data2_hv = joblib.load('/content/data2_f72_t3_n1_high_vel_combined.joblib')\n",
        "\n",
        "X_Train = data2['test_X']\n",
        "X_Test = data2['train_X']\n",
        "Y_Train = data2['test_y']\n",
        "Y_Test = data2['train_y']\n",
        "\n",
        "# X_Train = data2['train_X']\n",
        "# X_Test = data2['test_X']\n",
        "# Y_Train = data2['train_y']\n",
        "# Y_Test = data2['test_y']\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "metadata": {
        "id": "fyuz_9UNRE2A",
        "outputId": "849e2ebe-0b5b-45ad-e884-e2b1a9350a8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(329, 1024, 15)\n",
            "(1315, 1024, 15)\n",
            "(329, 1024, 6)\n",
            "(1315, 1024, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_Train = X_Train[:, :, [6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 30, 31, 32, 33, 34]]\n",
        "# X_Test = X_Test[:, :, [6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 30, 31, 32, 33, 34]]\n",
        "X_Train = X_Train[:, :, [10, 11, 12, 13, 14]]\n",
        "X_Test = X_Test[:, :, [10, 11, 12, 13, 14]]\n",
        "Y_Train = Y_Train[:, :, 2]\n",
        "Y_Test = Y_Test[:, :, 2]\n",
        "\n",
        "X_Train = np.nan_to_num(X_Train)\n",
        "X_Test = np.nan_to_num(X_Test)\n",
        "Y_Train = np.nan_to_num(Y_Train)\n",
        "Y_Test = np.nan_to_num(Y_Test)\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "metadata": {
        "id": "lnjVfNGdRKLu",
        "outputId": "44298c1d-7917-49aa-a1ea-d26ad501e541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(329, 1024, 5)\n",
            "(1315, 1024, 5)\n",
            "(329, 1024)\n",
            "(1315, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_LEN = X_Train.shape[1]\n",
        "N_CHANNELS = X_Train.shape[2]"
      ],
      "metadata": {
        "id": "__ifKmnMROLP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'MultiResUNet' # UNet or UNetPP\n",
        "signal_length = FRAME_LEN # Length of each Segment\n",
        "model_depth = 5 # Number of Level in the CNN Model\n",
        "model_width = 16 # Width of the Initial Layer, subsequent layers start from here\n",
        "kernel_size = 3 # Size of the Kernels/Filter\n",
        "num_channel = N_CHANNELS # Number of Channels in the Model\n",
        "D_S = 0 # Turn on Deep Supervision\n",
        "A_E = 0 # Turn on AutoEncoder Mode for Feature Extraction\n",
        "A_G = 0 # Attention Guided\n",
        "problem_type = 'Regression'\n",
        "output_nums = 1 # Number of Class for Classification Problems, always '1' for Regression Problems\n",
        "'''Only required if the AutoEncoder Mode is turned on'''\n",
        "feature_number = 1024 # Number of Features to be Extracted\n",
        "'''Only required for MultiResUNet'''\n",
        "alpha = 1 # Model Width Expansion Parameter"
      ],
      "metadata": {
        "id": "X6beBxhhRQsB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
        "  def approximate(inp, w_len, signal_length):\n",
        "    op = np.zeros((len(inp),signal_length//w_len))\n",
        "    for i in range(0,signal_length,w_len):\n",
        "      try:\n",
        "        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(i)\n",
        "  \t\n",
        "    return op\n",
        "\n",
        "  out = {}\n",
        "  Y_Train_dict = {}\n",
        "  out['out'] = np.array(y)\n",
        "  Y_Train_dict['out'] = out['out']\n",
        "  for i in range(1, (model_depth+1)):\n",
        "    name = f'level{i}'\n",
        "    if ((model_name == 'UNet') or (model_name == 'MultiResUNet') or (model_name == 'FPN')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n",
        "    elif ((model_name == 'UNetE') or (model_name == 'UNetP') or (model_name == 'UNetPP')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**0, signal_length),axis = 2)\n",
        "    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
        "  \n",
        "  return out, Y_Train_dict"
      ],
      "metadata": {
        "id": "S5mLD9IwRnB3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train1 = X_Train\n",
        "X_Test1 = X_Test\n",
        "[Y_Train1, Y_Train_dict] = prepareTrainDict(Y_Train, model_depth, signal_length, model_name)\n",
        "[Y_Test1, Y_Test_dict] = prepareTrainDict(Y_Test, model_depth, signal_length, model_name)"
      ],
      "metadata": {
        "id": "PWzVDxWJRpeD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_weights = np.zeros(model_depth)\n",
        "\n",
        "for i in range(0, model_depth):\n",
        "   loss_weights[i] = 1-(i*0.1)\n",
        "   \n",
        "loss_weights"
      ],
      "metadata": {
        "id": "pr0Uht8qRr3M",
        "outputId": "398c5d0e-9e09-467e-9706-097100d5cc37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 0.9, 0.8, 0.7, 0.6])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = 0.001\n",
        ")\n",
        "\n",
        "# if D_S == 0:\n",
        "#     # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "#     # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "#     # D_S = 0, A_E = 0\n",
        "#     model = UNet(\n",
        "#         signal_length,\n",
        "#         model_depth,\n",
        "#         num_channel, \n",
        "#         model_width, \n",
        "#         kernel_size, \n",
        "#         problem_type=problem_type, \n",
        "#         output_nums=output_nums, \n",
        "#         ds=D_S,\n",
        "#         ae=A_E, \n",
        "#         ag=A_G,\n",
        "#         alpha=alpha\n",
        "#     ).UNet()\n",
        "\n",
        "#     model.compile(\n",
        "#         loss= 'mean_absolute_error', \n",
        "#         optimizer= optimizer, \n",
        "#         metrics= ['mean_squared_error','accuracy']\n",
        "#     )\n",
        "#     #\n",
        "# elif D_S == 1:\n",
        "#     # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "#     # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "#     # D_S = 1, A_E = 0\n",
        "#     model = UNet(\n",
        "#         signal_length, model_depth, \n",
        "#         num_channel, \n",
        "#         model_width, \n",
        "#         kernel_size, \n",
        "#         problem_type=problem_type, \n",
        "#         output_nums=output_nums, \n",
        "#         ds=D_S,\n",
        "#         ae=A_E,\n",
        "#         ag=A_G, \n",
        "#         alpha=alpha\n",
        "#     ).MultiResUNet()\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model(model_path, compile=False)\n",
        "\n",
        "\n",
        "\n",
        "for idx, layer in enumerate(model.layers):\n",
        "    if idx == 99:\n",
        "        layer.trainable = True\n",
        "    elif idx < 208:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "\n",
        "# model.get_layer('conv1d_50').trainable = True\n",
        "# model.get_layer('conv1d_59').trainable = True\n",
        "# model.get_layer('conv1d_60').trainable = True\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss= 'mean_absolute_error', \n",
        "    optimizer= optimizer, \n",
        "    metrics= ['mean_squared_error'], \n",
        "    loss_weights= loss_weights\n",
        ")"
      ],
      "metadata": {
        "id": "9QFSxxd2RurP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config = model_name + str(signal_length) + '_' \\\n",
        "#             + str(N_CHANNELS) + '_' + str(model_width) +'_' \\\n",
        "#             + str(model_depth) + '_' + str(num_channel) + '_all_vel_' \\\n",
        "#             + str(D_S) + str(A_G) + '_axis_corrected_cv'"
      ],
      "metadata": {
        "id": "BSDNA2ZWRxUk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = \"TLDB2_\" + model_name + str(D_S) + str(A_G) + '_paper_v1'"
      ],
      "metadata": {
        "id": "bcBFcYuUy7Df"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if D_S == 0:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min', restore_best_weights=True), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config +'.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train, Y_Train, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "elif D_S == 1:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min', restore_best_weights=True), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config + '.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train1, \n",
        "        Y_Train_dict, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "id": "IpKb2_7xR1Al",
        "outputId": "c6b5543c-0fa7-47da-ea93-b840a21f5d3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1051 - mean_squared_error: 0.0289\n",
            "Epoch 00001: val_loss improved from inf to 0.08993, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 21s 843ms/step - loss: 0.1051 - mean_squared_error: 0.0289 - val_loss: 0.0899 - val_mean_squared_error: 0.0210\n",
            "Epoch 2/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0892 - mean_squared_error: 0.0215\n",
            "Epoch 00002: val_loss improved from 0.08993 to 0.07978, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0889 - mean_squared_error: 0.0213 - val_loss: 0.0798 - val_mean_squared_error: 0.0171\n",
            "Epoch 3/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0795 - mean_squared_error: 0.0173\n",
            "Epoch 00003: val_loss improved from 0.07978 to 0.07326, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0796 - mean_squared_error: 0.0174 - val_loss: 0.0733 - val_mean_squared_error: 0.0148\n",
            "Epoch 4/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0728 - mean_squared_error: 0.0149\n",
            "Epoch 00004: val_loss improved from 0.07326 to 0.06832, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0728 - mean_squared_error: 0.0149 - val_loss: 0.0683 - val_mean_squared_error: 0.0131\n",
            "Epoch 5/300\n",
            "3/5 [=================>............] - ETA: 0s - loss: 0.0675 - mean_squared_error: 0.0130\n",
            "Epoch 00005: val_loss improved from 0.06832 to 0.06441, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0672 - mean_squared_error: 0.0129 - val_loss: 0.0644 - val_mean_squared_error: 0.0116\n",
            "Epoch 6/300\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0627 - mean_squared_error: 0.0114\n",
            "Epoch 00006: val_loss improved from 0.06441 to 0.06110, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 149ms/step - loss: 0.0627 - mean_squared_error: 0.0114 - val_loss: 0.0611 - val_mean_squared_error: 0.0104\n",
            "Epoch 7/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0595 - mean_squared_error: 0.0103\n",
            "Epoch 00007: val_loss improved from 0.06110 to 0.05820, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 144ms/step - loss: 0.0593 - mean_squared_error: 0.0102 - val_loss: 0.0582 - val_mean_squared_error: 0.0094\n",
            "Epoch 8/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0570 - mean_squared_error: 0.0094\n",
            "Epoch 00008: val_loss improved from 0.05820 to 0.05620, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 152ms/step - loss: 0.0566 - mean_squared_error: 0.0093 - val_loss: 0.0562 - val_mean_squared_error: 0.0087\n",
            "Epoch 9/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0545 - mean_squared_error: 0.0087\n",
            "Epoch 00009: val_loss improved from 0.05620 to 0.05481, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 142ms/step - loss: 0.0546 - mean_squared_error: 0.0087 - val_loss: 0.0548 - val_mean_squared_error: 0.0083\n",
            "Epoch 10/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0528 - mean_squared_error: 0.0082\n",
            "Epoch 00010: val_loss improved from 0.05481 to 0.05348, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 143ms/step - loss: 0.0529 - mean_squared_error: 0.0082 - val_loss: 0.0535 - val_mean_squared_error: 0.0079\n",
            "Epoch 11/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0516 - mean_squared_error: 0.0079\n",
            "Epoch 00011: val_loss improved from 0.05348 to 0.05244, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 141ms/step - loss: 0.0515 - mean_squared_error: 0.0078 - val_loss: 0.0524 - val_mean_squared_error: 0.0076\n",
            "Epoch 12/300\n",
            "3/5 [=================>............] - ETA: 0s - loss: 0.0507 - mean_squared_error: 0.0077\n",
            "Epoch 00012: val_loss improved from 0.05244 to 0.05160, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 155ms/step - loss: 0.0503 - mean_squared_error: 0.0075 - val_loss: 0.0516 - val_mean_squared_error: 0.0073\n",
            "Epoch 13/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.0073\n",
            "Epoch 00013: val_loss improved from 0.05160 to 0.05102, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 154ms/step - loss: 0.0493 - mean_squared_error: 0.0072 - val_loss: 0.0510 - val_mean_squared_error: 0.0071\n",
            "Epoch 14/300\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0486 - mean_squared_error: 0.0070\n",
            "Epoch 00014: val_loss improved from 0.05102 to 0.05048, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 145ms/step - loss: 0.0484 - mean_squared_error: 0.0069 - val_loss: 0.0505 - val_mean_squared_error: 0.0070\n",
            "Epoch 15/300\n",
            "3/5 [=================>............] - ETA: 0s - loss: 0.0474 - mean_squared_error: 0.0064\n",
            "Epoch 00015: val_loss improved from 0.05048 to 0.04988, saving model to trained_models/TLDB2_MultiResUNet00_paper_v1.h5\n",
            "5/5 [==============================] - 1s 146ms/step - loss: 0.0475 - mean_squared_error: 0.0067 - val_loss: 0.0499 - val_mean_squared_error: 0.0068\n",
            "Epoch 16/300\n",
            "3/5 [=================>............] - ETA: 0s - loss: 0.0472 - mean_squared_error: 0.0065"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = 0.00001\n",
        ")\n",
        "model.compile(\n",
        "        loss= correlation_coefficient_loss, \n",
        "        optimizer= optimizer, \n",
        "        metrics= ['mean_squared_error'], \n",
        "        loss_weights= loss_weights\n",
        ")"
      ],
      "metadata": {
        "id": "bdOPe3FtyxAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if D_S == 0:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min', restore_best_weights=True), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config +'.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train, Y_Train, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "elif D_S == 1:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min', restore_best_weights=True), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config + '.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train1, \n",
        "        Y_Train_dict, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "id": "tYwvW9BFyyk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if D_S == 0:\n",
        "    GRF_pred = model.predict(X_Test, verbose=1)\n",
        "    print(GRF_pred.shape)\n",
        "elif D_S == 1:\n",
        "    GRF_pred = model.predict(X_Test1, verbose=1)\n",
        "    print(GRF_pred[0].shape)"
      ],
      "metadata": {
        "id": "NYMwgI1XR6yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if D_S == 1:\n",
        "    ground_truth_mean = np.mean(Y_Test, axis=0)\n",
        "    ground_truth_std = np.std(Y_Test, axis=0)\n",
        "    prediction = np.nan_to_num(GRF_pred[0][:, :, 0])\n",
        "    prediction_mean = np.mean(prediction, axis=0)\n",
        "    prediction_std = np.std(prediction, axis=0)\n",
        "\n",
        "else:\n",
        "    ground_truth_mean = np.mean(Y_Test, axis=0)\n",
        "    ground_truth_std = np.std(Y_Test, axis=0)\n",
        "    prediction = np.nan_to_num(GRF_pred[:, :, 0])\n",
        "    prediction_mean = np.mean(prediction, axis=0)\n",
        "    prediction_std = np.std(prediction, axis=0)"
      ],
      "metadata": {
        "id": "o7fMZhlCR86Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.corrcoef(ground_truth_mean, prediction_mean)[0, 1]"
      ],
      "metadata": {
        "id": "HlICv49uR_pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sns.set_palette(\"Set1\")\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "x = np.linspace(0, 100, FRAME_LEN)\n",
        "plt.plot(x, ground_truth_mean, linewidth=3, label='Ground Truth')\n",
        "plt.fill_between(\n",
        "    x, ground_truth_mean + ground_truth_std,\n",
        "    ground_truth_mean - ground_truth_std, alpha=0.3,\n",
        "    label='Ground Truth +/- STD'\n",
        ")\n",
        "plt.plot(x, prediction_mean, linewidth=3, label='Prediction')\n",
        "plt.fill_between(\n",
        "    x, prediction_mean + prediction_std,\n",
        "    prediction_mean - prediction_std, alpha=0.3,\n",
        "    label='Prediction +/- STD'\n",
        ")\n",
        "plt.legend()\n",
        "plt.title(r\"$\\rho = $\" + f\"{round(R, 5)}\")\n",
        "plt.xlabel('Stance Phase (%)')\n",
        "plt.ylabel('Force (N)')\n",
        "plt.savefig(figures_dir + 'Results_' + config + '.png')\n",
        "plt.savefig(figures_dir + 'Results_' + config + '.eps')\n",
        "plt.savefig(figures_dir + 'Results_' + config + '.svg')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E7RUwpkLSCL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from shutil import copy\n",
        "# copy('trained_models/' + config + '.h5', models_dir)\n",
        "# joblib.dump(history, models_dir + \\\n",
        "#             'History_' + config + '.joblib')"
      ],
      "metadata": {
        "id": "vMdT1UpJSExp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def history_plot(history):\n",
        "#     # list all dictionaries in history\n",
        "#     print(history.history.keys())\n",
        "#     # summarize history for error\n",
        "#     plt.figure(figsize=(11,7))\n",
        "#     plt.subplot(2,1,1)\n",
        "#     plt.plot(history.history['out_mean_squared_error'])\n",
        "#     plt.plot(history.history['val_out_mean_squared_error'])\n",
        "#     plt.title('Model Error Performance')\n",
        "#     plt.ylabel('Error')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.legend(['Train', 'Val'], loc='upper right')\n",
        "#     #   plt.ylim([0, 3])\n",
        "#     # plt.show()\n",
        "#     # summarize history for loss\n",
        "#     plt.figure(figsize=(11,7))\n",
        "#     plt.subplot(2,1,2)\n",
        "#     plt.plot(history.history['out_loss'])\n",
        "#     plt.plot(history.history['val_out_loss'])\n",
        "#     plt.title('Model Loss')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.legend(['Train', 'Val'], loc='upper right')\n",
        "#     #   plt.ylim([0, 3])\n",
        "#     plt.savefig(figures_dir + 'LC_' + config + '.png')\n",
        "#     plt.show()\n",
        "# #\n",
        "# history_plot(history)"
      ],
      "metadata": {
        "id": "6UENS6A6SHEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if D_S == 0:\n",
        "#     random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "#     plt.figure(figsize=(14, 9))\n",
        "#     for i in range(6):\n",
        "#         y_true = Y_Test[random_idx[i]]\n",
        "#         y_pred = GRF_pred[random_idx[i]]\n",
        "#         MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "#         plt.subplot(2, 3, i + 1)\n",
        "#         plt.plot(y_true, label='Ground Truth')\n",
        "#         plt.plot(y_pred.ravel(), label='Prediction')\n",
        "#         plt.title(f\"MAE [{random_idx[i]}] : {MAE}\")\n",
        "#         plt.legend();\n",
        "#     plt.show()\n",
        "    \n",
        "# elif D_S == 1:\n",
        "#     random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "#     plt.figure(figsize=(14, 9))\n",
        "#     for i in range(6):\n",
        "#         y_true = Y_Test[random_idx[i]]\n",
        "#         y_pred = GRF_pred[0][random_idx[i]]\n",
        "#         MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "#         plt.subplot(2, 3, i + 1)\n",
        "#         plt.plot(y_true, label='Ground Truth', linewidth=3)\n",
        "#         plt.plot(y_pred.ravel(), label='Prediction', linewidth=3)\n",
        "#         plt.title(f\"MAE [{random_idx[i]}] : {round(MAE, 4)}\")\n",
        "#         plt.legend()\n",
        "#     plt.savefig(figures_dir + 'Examples_Fy_' + config + '.png')\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "ZCpo5JbBSMw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Construction_Error(GRND, Pred):\n",
        "    construction_err = []\n",
        "    rmse = []\n",
        "    corr_coef = []\n",
        "    bad_indices = []\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(GRND)):\n",
        "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
        "        RMSE = mean_squared_error(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel(), squared=False)\n",
        "        R = np.corrcoef(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel())[0, 1]\n",
        "        if MAE < 1:\n",
        "            construction_err.append(MAE)\n",
        "            rmse.append(RMSE)\n",
        "            corr_coef.append(R)\n",
        "        elif MAE >= 1:\n",
        "            count = count + 1\n",
        "            bad_indices.append(i)\n",
        "\n",
        "    print(f'Construction Error : {round(np.mean(construction_err), 3)} +/- {round(np.std(construction_err), 3)}')\n",
        "    print(f'RMSE : {round(np.mean(rmse), 3)} +/- {round(np.std(rmse), 3)}')\n",
        "    print(f'R : {round(np.mean(corr_coef), 3)} +/- {round(np.std(corr_coef), 3)}')\n",
        "    print(f'Number of Bad Predictions = {count}')\n",
        "\n",
        "    bad_indices = set(bad_indices)\n",
        "    all_indices = set(np.arange(len(GRND)))\n",
        "    good_indices = np.array(list(all_indices - bad_indices))\n",
        "    GRND_NEW = GRND[good_indices]\n",
        "    PRED_NEW = Pred[good_indices]\n",
        "\n",
        "    return GRND_NEW, PRED_NEW"
      ],
      "metadata": {
        "id": "3UI2ycsUSPAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[A,B] = Construction_Error(Y_Test, GRF_pred)"
      ],
      "metadata": {
        "id": "0VGab5XISRGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "cc12dd51c08fff59e312c49b5273cbf2a12939660509ea1a4d83cd89b0463726"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('Ai': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "transfer_learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}