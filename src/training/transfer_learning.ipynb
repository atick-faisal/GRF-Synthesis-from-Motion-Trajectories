{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/GRF-Synthesis-from-Motion-Trajectories/blob/main/src/training/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6yBebZQzQL76"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pandas==1.3.4"
      ],
      "metadata": {
        "id": "h74L_Qf_Q22z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id \"1kNGwWBQp6kmAioLP-_zWx7r42SzoEO3v\"\n",
        "# !unzip UNet.zip\n",
        "\n",
        "# !gdown --id \"1Z6SiogUQExhFuEqlAzdDyGZo8bOJR3DJ\""
      ],
      "metadata": {
        "id": "vyR0w49LQ9n_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jhlBmxZIQL73"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import scipy\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from UNet_1DCNN import UNet\n",
        "from FPN_1DCNN import FPN\n",
        "from AlbuNet_1DCNN import AlbUNet\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style(\"darkgrid\", {'font.family':'serif', 'font.serif':'Times New Roman'})\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/DB1/ModelsY/'\n",
        "figures_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/DB1/FiguresY/'\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Research/GRF Data Synthesis/DB1/ModelsY/MultiResUNet_1024_5_64_5_5_all_vel_11_axis_corrected.h5'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data = joblib.load('/content/data2_f72_t3_n1_combined.joblib')\n",
        "# data1 = joblib.load('/content/data1_f15_t3_n1_combined.joblib')\n",
        "data2 = joblib.load('/content/data2_f15_t3_n1_all_vel_combined.joblib')\n",
        "# data2_hv = joblib.load('/content/data2_f72_t3_n1_high_vel_combined.joblib')\n",
        "\n",
        "X_Train = data2['test_X']\n",
        "X_Test = data2['train_X']\n",
        "Y_Train = data2['test_y']\n",
        "Y_Test = data2['train_y']\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "metadata": {
        "id": "fyuz_9UNRE2A",
        "outputId": "37172843-0adf-44c3-8ef1-17027894feec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 1024, 15)\n",
            "(3136, 1024, 15)\n",
            "(784, 1024, 3)\n",
            "(3136, 1024, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_Train = X_Train[:, :, [6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 30, 31, 32, 33, 34]]\n",
        "# X_Test = X_Test[:, :, [6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 30, 31, 32, 33, 34]]\n",
        "X_Train = X_Train[:, :, [10, 11, 12, 13, 14]]\n",
        "X_Test = X_Test[:, :, [10, 11, 12, 13, 14]]\n",
        "Y_Train = Y_Train[:, :, 1]\n",
        "Y_Test = Y_Test[:, :, 1]\n",
        "\n",
        "X_Train = np.nan_to_num(X_Train)\n",
        "X_Test = np.nan_to_num(X_Test)\n",
        "Y_Train = np.nan_to_num(Y_Train)\n",
        "Y_Test = np.nan_to_num(Y_Test)\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "metadata": {
        "id": "lnjVfNGdRKLu",
        "outputId": "04e8e010-274d-46f4-e50c-9346aa3a0c1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 1024, 5)\n",
            "(3136, 1024, 5)\n",
            "(784, 1024)\n",
            "(3136, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_LEN = X_Train.shape[1]\n",
        "N_CHANNELS = X_Train.shape[2]"
      ],
      "metadata": {
        "id": "__ifKmnMROLP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'MultiResUNet' # UNet or UNetPP\n",
        "signal_length = FRAME_LEN # Length of each Segment\n",
        "model_depth = 5 # Number of Level in the CNN Model\n",
        "model_width = 64 # Width of the Initial Layer, subsequent layers start from here\n",
        "kernel_size = 3 # Size of the Kernels/Filter\n",
        "num_channel = N_CHANNELS # Number of Channels in the Model\n",
        "D_S = 1 # Turn on Deep Supervision\n",
        "A_E = 0 # Turn on AutoEncoder Mode for Feature Extraction\n",
        "A_G = 0 # Attention Guided\n",
        "problem_type = 'Regression'\n",
        "output_nums = 1 # Number of Class for Classification Problems, always '1' for Regression Problems\n",
        "'''Only required if the AutoEncoder Mode is turned on'''\n",
        "feature_number = 1024 # Number of Features to be Extracted\n",
        "'''Only required for MultiResUNet'''\n",
        "alpha = 1 # Model Width Expansion Parameter"
      ],
      "metadata": {
        "id": "X6beBxhhRQsB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
        "  def approximate(inp, w_len, signal_length):\n",
        "    op = np.zeros((len(inp),signal_length//w_len))\n",
        "    for i in range(0,signal_length,w_len):\n",
        "      try:\n",
        "        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(i)\n",
        "  \t\n",
        "    return op\n",
        "\n",
        "  out = {}\n",
        "  Y_Train_dict = {}\n",
        "  out['out'] = np.array(y)\n",
        "  Y_Train_dict['out'] = out['out']\n",
        "  for i in range(1, (model_depth+1)):\n",
        "    name = f'level{i}'\n",
        "    if ((model_name == 'UNet') or (model_name == 'MultiResUNet') or (model_name == 'FPN')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n",
        "    elif ((model_name == 'UNetE') or (model_name == 'UNetP') or (model_name == 'UNetPP')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**0, signal_length),axis = 2)\n",
        "    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
        "  \n",
        "  return out, Y_Train_dict"
      ],
      "metadata": {
        "id": "S5mLD9IwRnB3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train1 = X_Train\n",
        "X_Test1 = X_Test\n",
        "[Y_Train1, Y_Train_dict] = prepareTrainDict(Y_Train, model_depth, signal_length, model_name)\n",
        "[Y_Test1, Y_Test_dict] = prepareTrainDict(Y_Test, model_depth, signal_length, model_name)"
      ],
      "metadata": {
        "id": "PWzVDxWJRpeD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_weights = np.zeros(model_depth)\n",
        "\n",
        "for i in range(0, model_depth):\n",
        "   loss_weights[i] = 1-(i*0.1)\n",
        "   \n",
        "loss_weights"
      ],
      "metadata": {
        "id": "pr0Uht8qRr3M",
        "outputId": "cb73b09f-057b-4e10-9f98-124ad20e6763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 0.9, 0.8, 0.7, 0.6])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = 0.001\n",
        ")\n",
        "\n",
        "# if D_S == 0:\n",
        "#     # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "#     # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "#     # D_S = 0, A_E = 0\n",
        "#     model = UNet(\n",
        "#         signal_length,\n",
        "#         model_depth,\n",
        "#         num_channel, \n",
        "#         model_width, \n",
        "#         kernel_size, \n",
        "#         problem_type=problem_type, \n",
        "#         output_nums=output_nums, \n",
        "#         ds=D_S,\n",
        "#         ae=A_E, \n",
        "#         ag=A_G,\n",
        "#         alpha=alpha\n",
        "#     ).UNet()\n",
        "\n",
        "#     model.compile(\n",
        "#         loss= 'mean_absolute_error', \n",
        "#         optimizer= optimizer, \n",
        "#         metrics= ['mean_squared_error','accuracy']\n",
        "#     )\n",
        "#     #\n",
        "# elif D_S == 1:\n",
        "#     # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "#     # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "#     # D_S = 1, A_E = 0\n",
        "#     model = UNet(\n",
        "#         signal_length, model_depth, \n",
        "#         num_channel, \n",
        "#         model_width, \n",
        "#         kernel_size, \n",
        "#         problem_type=problem_type, \n",
        "#         output_nums=output_nums, \n",
        "#         ds=D_S,\n",
        "#         ae=A_E,\n",
        "#         ag=A_G, \n",
        "#         alpha=alpha\n",
        "#     ).MultiResUNet()\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "\n",
        "\n",
        "for idx, layer in enumerate(model.layers):\n",
        "    if idx == 99:\n",
        "        layer.trainable = True\n",
        "    elif idx < 208:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "\n",
        "# model.get_layer('conv1d_50').trainable = True\n",
        "# model.get_layer('conv1d_59').trainable = True\n",
        "# model.get_layer('conv1d_60').trainable = True\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss= 'mean_absolute_error', \n",
        "    optimizer= optimizer, \n",
        "    metrics= ['mean_squared_error'], \n",
        "    loss_weights= loss_weights\n",
        ")"
      ],
      "metadata": {
        "id": "9QFSxxd2RurP",
        "outputId": "8942d19b-9843-4de8-f04e-4a6da9d3db05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1024, 5)]    0           []                               \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 1024, 10)     160         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 1024, 10)    40          ['conv1d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1024, 10)     0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 1024, 21)     651         ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 1024, 21)    84          ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1024, 21)     0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 1024, 32)     2048        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 1024, 32)    128         ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 1024, 63)     378         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1024, 32)     0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1024, 63)    252         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1024, 63)     0           ['activation_1[0][0]',           \n",
            "                                                                  'activation_2[0][0]',           \n",
            "                                                                  'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1024, 63)     0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 1024, 63)    252         ['concatenate[0][0]']            \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 1024, 63)     0           ['activation[0][0]',             \n",
            "                                                                  'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1024, 63)     0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 1024, 63)    252         ['activation_4[0][0]']           \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 512, 63)      0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 512, 20)      3800        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 512, 20)     80          ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 512, 20)      0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 512, 42)      2562        ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 512, 42)     168         ['conv1d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 512, 42)      0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 512, 64)      8128        ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 512, 64)     256         ['conv1d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 512, 126)     8064        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 512, 64)      0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 512, 126)    504         ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 512, 126)     0           ['activation_21[0][0]',          \n",
            "                                                                  'activation_22[0][0]',          \n",
            "                                                                  'activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 512, 126)     0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 512, 126)    504         ['concatenate_1[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 512, 126)     0           ['activation_20[0][0]',          \n",
            "                                                                  'batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 512, 126)     0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 512, 126)    504         ['activation_24[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 256, 126)    0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 256, 40)      15160       ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 256, 40)     160         ['conv1d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 256, 40)      0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 256, 84)      10164       ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 256, 84)     336         ['conv1d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 256, 84)      0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 256, 128)     32384       ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 256, 128)    512         ['conv1d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 256, 252)     32004       ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 256, 128)     0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 256, 252)    1008        ['conv1d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 256, 252)     0           ['activation_38[0][0]',          \n",
            "                                                                  'activation_39[0][0]',          \n",
            "                                                                  'activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 256, 252)     0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 256, 252)    1008        ['concatenate_2[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 256, 252)     0           ['activation_37[0][0]',          \n",
            "                                                                  'batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 256, 252)     0           ['add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 256, 252)    1008        ['activation_41[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 128, 252)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_37 (Conv1D)             (None, 128, 80)      60560       ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 128, 80)     320         ['conv1d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 128, 80)      0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_38 (Conv1D)             (None, 128, 168)     40488       ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 128, 168)    672         ['conv1d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 128, 168)     0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_39 (Conv1D)             (None, 128, 256)     129280      ['activation_53[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 128, 256)    1024        ['conv1d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_36 (Conv1D)             (None, 128, 504)     127512      ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 128, 256)     0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 128, 504)    2016        ['conv1d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 128, 504)     0           ['activation_52[0][0]',          \n",
            "                                                                  'activation_53[0][0]',          \n",
            "                                                                  'activation_54[0][0]']          \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 128, 504)     0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 128, 504)    2016        ['concatenate_3[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 128, 504)     0           ['activation_51[0][0]',          \n",
            "                                                                  'batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 128, 504)     0           ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 128, 504)    2016        ['activation_55[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 64, 504)     0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_45 (Conv1D)             (None, 64, 160)      242080      ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 64, 160)     640         ['conv1d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 64, 160)      0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_46 (Conv1D)             (None, 64, 336)      161616      ['activation_63[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 64, 336)     1344        ['conv1d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 64, 336)      0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_47 (Conv1D)             (None, 64, 512)      516608      ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 64, 512)     2048        ['conv1d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_44 (Conv1D)             (None, 64, 1008)     509040      ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 64, 512)      0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 64, 1008)    4032        ['conv1d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 64, 1008)     0           ['activation_63[0][0]',          \n",
            "                                                                  'activation_64[0][0]',          \n",
            "                                                                  'activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 64, 1008)     0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 64, 1008)    4032        ['concatenate_4[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 64, 1008)     0           ['activation_62[0][0]',          \n",
            "                                                                  'batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 64, 1008)     0           ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 64, 1008)    4032        ['activation_66[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 32, 1008)    0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 32, 320)      968000      ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 32, 320)     1280        ['conv1d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 32, 320)      0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 32, 672)      645792      ['activation_71[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 32, 672)     2688        ['conv1d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 32, 672)      0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 32, 1024)     2065408     ['activation_72[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 32, 1024)    4096        ['conv1d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 32, 2016)     2034144     ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 32, 1024)     0           ['batch_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 32, 2016)    8064        ['conv1d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 2016)     0           ['activation_71[0][0]',          \n",
            "                                                                  'activation_72[0][0]',          \n",
            "                                                                  'activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 32, 2016)     0           ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 32, 2016)    8064        ['concatenate_5[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 32, 2016)     0           ['activation_70[0][0]',          \n",
            "                                                                  'batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 32, 2016)     0           ['add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 32, 2016)    8064        ['activation_74[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_transpose_1 (Conv1DTran  (None, 64, 1024)    4129792     ['batch_normalization_80[0][0]'] \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, 64, 1024)    4096        ['conv1d_transpose_1[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 64, 1024)     0           ['batch_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_58 (Conv1D)             (None, 64, 160)      491680      ['activation_78[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, 64, 160)     640         ['conv1d_58[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 64, 160)      0           ['batch_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_59 (Conv1D)             (None, 64, 336)      161616      ['activation_80[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, 64, 336)     1344        ['conv1d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, 64, 336)      0           ['batch_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_60 (Conv1D)             (None, 64, 512)      516608      ['activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, 64, 512)     2048        ['conv1d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_57 (Conv1D)             (None, 64, 1008)     1033200     ['activation_78[0][0]']          \n",
            "                                                                                                  \n",
            " activation_82 (Activation)     (None, 64, 512)      0           ['batch_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, 64, 1008)    4032        ['conv1d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 64, 1008)     0           ['activation_80[0][0]',          \n",
            "                                                                  'activation_81[0][0]',          \n",
            "                                                                  'activation_82[0][0]']          \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 64, 1008)     0           ['batch_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, 64, 1008)    4032        ['concatenate_6[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 64, 1008)     0           ['activation_79[0][0]',          \n",
            "                                                                  'batch_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " activation_83 (Activation)     (None, 64, 1008)     0           ['add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, 64, 1008)    4032        ['activation_83[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_transpose_3 (Conv1DTran  (None, 128, 512)    1032704     ['batch_normalization_91[0][0]'] \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_96 (BatchN  (None, 128, 512)    2048        ['conv1d_transpose_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_87 (Activation)     (None, 128, 512)     0           ['batch_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_65 (Conv1D)             (None, 128, 80)      122960      ['activation_87[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_98 (BatchN  (None, 128, 80)     320         ['conv1d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_89 (Activation)     (None, 128, 80)      0           ['batch_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_66 (Conv1D)             (None, 128, 168)     40488       ['activation_89[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_99 (BatchN  (None, 128, 168)    672         ['conv1d_66[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_90 (Activation)     (None, 128, 168)     0           ['batch_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_67 (Conv1D)             (None, 128, 256)     129280      ['activation_90[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_100 (Batch  (None, 128, 256)    1024        ['conv1d_67[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_64 (Conv1D)             (None, 128, 504)     258552      ['activation_87[0][0]']          \n",
            "                                                                                                  \n",
            " activation_91 (Activation)     (None, 128, 256)     0           ['batch_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_97 (BatchN  (None, 128, 504)    2016        ['conv1d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 128, 504)     0           ['activation_89[0][0]',          \n",
            "                                                                  'activation_90[0][0]',          \n",
            "                                                                  'activation_91[0][0]']          \n",
            "                                                                                                  \n",
            " activation_88 (Activation)     (None, 128, 504)     0           ['batch_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_101 (Batch  (None, 128, 504)    2016        ['concatenate_7[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 128, 504)     0           ['activation_88[0][0]',          \n",
            "                                                                  'batch_normalization_101[0][0]']\n",
            "                                                                                                  \n",
            " activation_92 (Activation)     (None, 128, 504)     0           ['add_26[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_102 (Batch  (None, 128, 504)    2016        ['activation_92[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_transpose_5 (Conv1DTran  (None, 256, 256)    258304      ['batch_normalization_102[0][0]']\n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_107 (Batch  (None, 256, 256)    1024        ['conv1d_transpose_5[0][0]']     \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_96 (Activation)     (None, 256, 256)     0           ['batch_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_72 (Conv1D)             (None, 256, 40)      30760       ['activation_96[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_109 (Batch  (None, 256, 40)     160         ['conv1d_72[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_98 (Activation)     (None, 256, 40)      0           ['batch_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_73 (Conv1D)             (None, 256, 84)      10164       ['activation_98[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_110 (Batch  (None, 256, 84)     336         ['conv1d_73[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_99 (Activation)     (None, 256, 84)      0           ['batch_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_74 (Conv1D)             (None, 256, 128)     32384       ['activation_99[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_111 (Batch  (None, 256, 128)    512         ['conv1d_74[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_71 (Conv1D)             (None, 256, 252)     64764       ['activation_96[0][0]']          \n",
            "                                                                                                  \n",
            " activation_100 (Activation)    (None, 256, 128)     0           ['batch_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_108 (Batch  (None, 256, 252)    1008        ['conv1d_71[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 256, 252)     0           ['activation_98[0][0]',          \n",
            "                                                                  'activation_99[0][0]',          \n",
            "                                                                  'activation_100[0][0]']         \n",
            "                                                                                                  \n",
            " activation_97 (Activation)     (None, 256, 252)     0           ['batch_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_112 (Batch  (None, 256, 252)    1008        ['concatenate_8[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 256, 252)     0           ['activation_97[0][0]',          \n",
            "                                                                  'batch_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " activation_101 (Activation)    (None, 256, 252)     0           ['add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_113 (Batch  (None, 256, 252)    1008        ['activation_101[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_transpose_7 (Conv1DTran  (None, 512, 128)    64640       ['batch_normalization_113[0][0]']\n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_118 (Batch  (None, 512, 128)    512         ['conv1d_transpose_7[0][0]']     \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_105 (Activation)    (None, 512, 128)     0           ['batch_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_79 (Conv1D)             (None, 512, 20)      7700        ['activation_105[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_120 (Batch  (None, 512, 20)     80          ['conv1d_79[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_107 (Activation)    (None, 512, 20)      0           ['batch_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_80 (Conv1D)             (None, 512, 42)      2562        ['activation_107[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_121 (Batch  (None, 512, 42)     168         ['conv1d_80[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_108 (Activation)    (None, 512, 42)      0           ['batch_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_81 (Conv1D)             (None, 512, 64)      8128        ['activation_108[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_122 (Batch  (None, 512, 64)     256         ['conv1d_81[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_78 (Conv1D)             (None, 512, 126)     16254       ['activation_105[0][0]']         \n",
            "                                                                                                  \n",
            " activation_109 (Activation)    (None, 512, 64)      0           ['batch_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_119 (Batch  (None, 512, 126)    504         ['conv1d_78[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 512, 126)     0           ['activation_107[0][0]',         \n",
            "                                                                  'activation_108[0][0]',         \n",
            "                                                                  'activation_109[0][0]']         \n",
            "                                                                                                  \n",
            " activation_106 (Activation)    (None, 512, 126)     0           ['batch_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_123 (Batch  (None, 512, 126)    504         ['concatenate_9[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 512, 126)     0           ['activation_106[0][0]',         \n",
            "                                                                  'batch_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " activation_110 (Activation)    (None, 512, 126)     0           ['add_32[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_124 (Batch  (None, 512, 126)    504         ['activation_110[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_transpose_9 (Conv1DTran  (None, 1024, 64)    16192       ['batch_normalization_124[0][0]']\n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_129 (Batch  (None, 1024, 64)    256         ['conv1d_transpose_9[0][0]']     \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_114 (Activation)    (None, 1024, 64)     0           ['batch_normalization_129[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_86 (Conv1D)             (None, 1024, 10)     1930        ['activation_114[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_131 (Batch  (None, 1024, 10)    40          ['conv1d_86[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_116 (Activation)    (None, 1024, 10)     0           ['batch_normalization_131[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_87 (Conv1D)             (None, 1024, 21)     651         ['activation_116[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_132 (Batch  (None, 1024, 21)    84          ['conv1d_87[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_117 (Activation)    (None, 1024, 21)     0           ['batch_normalization_132[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_88 (Conv1D)             (None, 1024, 32)     2048        ['activation_117[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_133 (Batch  (None, 1024, 32)    128         ['conv1d_88[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv1d_85 (Conv1D)             (None, 1024, 63)     4095        ['activation_114[0][0]']         \n",
            "                                                                                                  \n",
            " activation_118 (Activation)    (None, 1024, 32)     0           ['batch_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_130 (Batch  (None, 1024, 63)    252         ['conv1d_85[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 1024, 63)     0           ['activation_116[0][0]',         \n",
            "                                                                  'activation_117[0][0]',         \n",
            "                                                                  'activation_118[0][0]']         \n",
            "                                                                                                  \n",
            " activation_115 (Activation)    (None, 1024, 63)     0           ['batch_normalization_130[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_134 (Batch  (None, 1024, 63)    252         ['concatenate_10[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_35 (Add)                   (None, 1024, 63)     0           ['activation_115[0][0]',         \n",
            "                                                                  'batch_normalization_134[0][0]']\n",
            "                                                                                                  \n",
            " activation_119 (Activation)    (None, 1024, 63)     0           ['add_35[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_135 (Batch  (None, 1024, 63)    252         ['activation_119[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " out (Conv1D)                   (None, 1024, 1)      64          ['batch_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " level1 (Conv1D)                (None, 512, 1)       127         ['batch_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " level2 (Conv1D)                (None, 256, 1)       253         ['batch_normalization_113[0][0]']\n",
            "                                                                                                  \n",
            " level3 (Conv1D)                (None, 128, 1)       505         ['batch_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " level4 (Conv1D)                (None, 64, 1)        1009        ['batch_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " level5 (Conv1D)                (None, 32, 1)        2017        ['batch_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16,160,150\n",
            "Trainable params: 2,038,119\n",
            "Non-trainable params: 14,122,031\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model_name + str(signal_length) + '_' \\\n",
        "            + str(N_CHANNELS) + '_' + str(model_width) +'_' \\\n",
        "            + str(model_depth) + '_' + str(num_channel) + '_all_vel_' \\\n",
        "            + str(D_S) + str(A_G) + '_axis_corrected_cv'"
      ],
      "metadata": {
        "id": "BSDNA2ZWRxUk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if D_S == 0:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config +'.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train, Y_Train, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "elif D_S == 1:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config + '.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train1, \n",
        "        Y_Train_dict, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "id": "IpKb2_7xR1Al",
        "outputId": "b2e3e572-1d88-4194-9017-600452a8ee5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.1860 - out_loss: 0.1816 - level1_loss: 0.1836 - level2_loss: 0.1820 - level3_loss: 0.1865 - level4_loss: 0.1900 - level5_loss: 0.5587 - out_mean_squared_error: 0.0532 - level1_mean_squared_error: 0.0569 - level2_mean_squared_error: 0.0543 - level3_mean_squared_error: 0.0640 - level4_mean_squared_error: 0.0819 - level5_mean_squared_error: 8.1607\n",
            "Epoch 00001: val_loss improved from inf to 0.85212, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 31s 969ms/step - loss: 1.1860 - out_loss: 0.1816 - level1_loss: 0.1836 - level2_loss: 0.1820 - level3_loss: 0.1865 - level4_loss: 0.1900 - level5_loss: 0.5587 - out_mean_squared_error: 0.0532 - level1_mean_squared_error: 0.0569 - level2_mean_squared_error: 0.0543 - level3_mean_squared_error: 0.0640 - level4_mean_squared_error: 0.0819 - level5_mean_squared_error: 8.1607 - val_loss: 0.8521 - val_out_loss: 0.1580 - val_level1_loss: 0.1586 - val_level2_loss: 0.1601 - val_level3_loss: 0.1638 - val_level4_loss: 0.1539 - val_level5_loss: 0.2708 - val_out_mean_squared_error: 0.0400 - val_level1_mean_squared_error: 0.0405 - val_level2_mean_squared_error: 0.0415 - val_level3_mean_squared_error: 0.0442 - val_level4_mean_squared_error: 0.0392 - val_level5_mean_squared_error: 0.4025\n",
            "Epoch 2/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.9028 - out_loss: 0.1445 - level1_loss: 0.1458 - level2_loss: 0.1457 - level3_loss: 0.1510 - level4_loss: 0.1457 - level5_loss: 0.3957 - out_mean_squared_error: 0.0367 - level1_mean_squared_error: 0.0397 - level2_mean_squared_error: 0.0374 - level3_mean_squared_error: 0.0468 - level4_mean_squared_error: 0.0370 - level5_mean_squared_error: 2.6046\n",
            "Epoch 00002: val_loss improved from 0.85212 to 0.82533, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 443ms/step - loss: 0.9028 - out_loss: 0.1445 - level1_loss: 0.1458 - level2_loss: 0.1457 - level3_loss: 0.1510 - level4_loss: 0.1457 - level5_loss: 0.3957 - out_mean_squared_error: 0.0367 - level1_mean_squared_error: 0.0397 - level2_mean_squared_error: 0.0374 - level3_mean_squared_error: 0.0468 - level4_mean_squared_error: 0.0370 - level5_mean_squared_error: 2.6046 - val_loss: 0.8253 - val_out_loss: 0.1287 - val_level1_loss: 0.1284 - val_level2_loss: 0.1301 - val_level3_loss: 0.1346 - val_level4_loss: 0.1291 - val_level5_loss: 0.3807 - val_out_mean_squared_error: 0.0278 - val_level1_mean_squared_error: 0.0276 - val_level2_mean_squared_error: 0.0283 - val_level3_mean_squared_error: 0.0333 - val_level4_mean_squared_error: 0.0286 - val_level5_mean_squared_error: 1.4980\n",
            "Epoch 3/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7065 - out_loss: 0.1239 - level1_loss: 0.1247 - level2_loss: 0.1242 - level3_loss: 0.1273 - level4_loss: 0.1257 - level5_loss: 0.2572 - out_mean_squared_error: 0.0279 - level1_mean_squared_error: 0.0283 - level2_mean_squared_error: 0.0281 - level3_mean_squared_error: 0.0318 - level4_mean_squared_error: 0.0295 - level5_mean_squared_error: 0.8510\n",
            "Epoch 00003: val_loss improved from 0.82533 to 0.62905, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 443ms/step - loss: 0.7065 - out_loss: 0.1239 - level1_loss: 0.1247 - level2_loss: 0.1242 - level3_loss: 0.1273 - level4_loss: 0.1257 - level5_loss: 0.2572 - out_mean_squared_error: 0.0279 - level1_mean_squared_error: 0.0283 - level2_mean_squared_error: 0.0281 - level3_mean_squared_error: 0.0318 - level4_mean_squared_error: 0.0295 - level5_mean_squared_error: 0.8510 - val_loss: 0.6291 - val_out_loss: 0.1153 - val_level1_loss: 0.1150 - val_level2_loss: 0.1151 - val_level3_loss: 0.1173 - val_level4_loss: 0.1146 - val_level5_loss: 0.2090 - val_out_mean_squared_error: 0.0234 - val_level1_mean_squared_error: 0.0231 - val_level2_mean_squared_error: 0.0231 - val_level3_mean_squared_error: 0.0244 - val_level4_mean_squared_error: 0.0238 - val_level5_mean_squared_error: 0.5954\n",
            "Epoch 4/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5972 - out_loss: 0.1110 - level1_loss: 0.1111 - level2_loss: 0.1107 - level3_loss: 0.1122 - level4_loss: 0.1150 - level5_loss: 0.1865 - out_mean_squared_error: 0.0233 - level1_mean_squared_error: 0.0233 - level2_mean_squared_error: 0.0230 - level3_mean_squared_error: 0.0246 - level4_mean_squared_error: 0.0270 - level5_mean_squared_error: 0.3982\n",
            "Epoch 00004: val_loss improved from 0.62905 to 0.52202, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 458ms/step - loss: 0.5972 - out_loss: 0.1110 - level1_loss: 0.1111 - level2_loss: 0.1107 - level3_loss: 0.1122 - level4_loss: 0.1150 - level5_loss: 0.1865 - out_mean_squared_error: 0.0233 - level1_mean_squared_error: 0.0233 - level2_mean_squared_error: 0.0230 - level3_mean_squared_error: 0.0246 - level4_mean_squared_error: 0.0270 - level5_mean_squared_error: 0.3982 - val_loss: 0.5220 - val_out_loss: 0.1048 - val_level1_loss: 0.1049 - val_level2_loss: 0.1041 - val_level3_loss: 0.1057 - val_level4_loss: 0.1104 - val_level5_loss: 0.1226 - val_out_mean_squared_error: 0.0200 - val_level1_mean_squared_error: 0.0199 - val_level2_mean_squared_error: 0.0197 - val_level3_mean_squared_error: 0.0209 - val_level4_mean_squared_error: 0.0247 - val_level5_mean_squared_error: 0.0317\n",
            "Epoch 5/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5249 - out_loss: 0.1017 - level1_loss: 0.1016 - level2_loss: 0.1011 - level3_loss: 0.1021 - level4_loss: 0.1070 - level5_loss: 0.1426 - out_mean_squared_error: 0.0199 - level1_mean_squared_error: 0.0197 - level2_mean_squared_error: 0.0197 - level3_mean_squared_error: 0.0203 - level4_mean_squared_error: 0.0232 - level5_mean_squared_error: 0.1122\n",
            "Epoch 00005: val_loss improved from 0.52202 to 0.52063, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 444ms/step - loss: 0.5249 - out_loss: 0.1017 - level1_loss: 0.1016 - level2_loss: 0.1011 - level3_loss: 0.1021 - level4_loss: 0.1070 - level5_loss: 0.1426 - out_mean_squared_error: 0.0199 - level1_mean_squared_error: 0.0197 - level2_mean_squared_error: 0.0197 - level3_mean_squared_error: 0.0203 - level4_mean_squared_error: 0.0232 - level5_mean_squared_error: 0.1122 - val_loss: 0.5206 - val_out_loss: 0.0979 - val_level1_loss: 0.0977 - val_level2_loss: 0.0967 - val_level3_loss: 0.0981 - val_level4_loss: 0.1007 - val_level5_loss: 0.1598 - val_out_mean_squared_error: 0.0174 - val_level1_mean_squared_error: 0.0172 - val_level2_mean_squared_error: 0.0171 - val_level3_mean_squared_error: 0.0176 - val_level4_mean_squared_error: 0.0187 - val_level5_mean_squared_error: 0.2030\n",
            "Epoch 6/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5030 - out_loss: 0.0954 - level1_loss: 0.0953 - level2_loss: 0.0946 - level3_loss: 0.0954 - level4_loss: 0.0995 - level5_loss: 0.1485 - out_mean_squared_error: 0.0175 - level1_mean_squared_error: 0.0174 - level2_mean_squared_error: 0.0174 - level3_mean_squared_error: 0.0176 - level4_mean_squared_error: 0.0191 - level5_mean_squared_error: 0.2055\n",
            "Epoch 00006: val_loss improved from 0.52063 to 0.47882, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 447ms/step - loss: 0.5030 - out_loss: 0.0954 - level1_loss: 0.0953 - level2_loss: 0.0946 - level3_loss: 0.0954 - level4_loss: 0.0995 - level5_loss: 0.1485 - out_mean_squared_error: 0.0175 - level1_mean_squared_error: 0.0174 - level2_mean_squared_error: 0.0174 - level3_mean_squared_error: 0.0176 - level4_mean_squared_error: 0.0191 - level5_mean_squared_error: 0.2055 - val_loss: 0.4788 - val_out_loss: 0.0933 - val_level1_loss: 0.0931 - val_level2_loss: 0.0923 - val_level3_loss: 0.0937 - val_level4_loss: 0.0969 - val_level5_loss: 0.1291 - val_out_mean_squared_error: 0.0161 - val_level1_mean_squared_error: 0.0159 - val_level2_mean_squared_error: 0.0158 - val_level3_mean_squared_error: 0.0164 - val_level4_mean_squared_error: 0.0174 - val_level5_mean_squared_error: 0.0859\n",
            "Epoch 7/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4781 - out_loss: 0.0909 - level1_loss: 0.0908 - level2_loss: 0.0904 - level3_loss: 0.0916 - level4_loss: 0.0965 - level5_loss: 0.1374 - out_mean_squared_error: 0.0161 - level1_mean_squared_error: 0.0160 - level2_mean_squared_error: 0.0160 - level3_mean_squared_error: 0.0167 - level4_mean_squared_error: 0.0191 - level5_mean_squared_error: 0.1765\n",
            "Epoch 00007: val_loss improved from 0.47882 to 0.47813, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 445ms/step - loss: 0.4781 - out_loss: 0.0909 - level1_loss: 0.0908 - level2_loss: 0.0904 - level3_loss: 0.0916 - level4_loss: 0.0965 - level5_loss: 0.1374 - out_mean_squared_error: 0.0161 - level1_mean_squared_error: 0.0160 - level2_mean_squared_error: 0.0160 - level3_mean_squared_error: 0.0167 - level4_mean_squared_error: 0.0191 - level5_mean_squared_error: 0.1765 - val_loss: 0.4781 - val_out_loss: 0.0904 - val_level1_loss: 0.0901 - val_level2_loss: 0.0893 - val_level3_loss: 0.0903 - val_level4_loss: 0.0942 - val_level5_loss: 0.1435 - val_out_mean_squared_error: 0.0153 - val_level1_mean_squared_error: 0.0151 - val_level2_mean_squared_error: 0.0150 - val_level3_mean_squared_error: 0.0152 - val_level4_mean_squared_error: 0.0167 - val_level5_mean_squared_error: 0.1347\n",
            "Epoch 8/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4503 - out_loss: 0.0874 - level1_loss: 0.0873 - level2_loss: 0.0870 - level3_loss: 0.0879 - level4_loss: 0.0939 - level5_loss: 0.1193 - out_mean_squared_error: 0.0149 - level1_mean_squared_error: 0.0148 - level2_mean_squared_error: 0.0148 - level3_mean_squared_error: 0.0151 - level4_mean_squared_error: 0.0191 - level5_mean_squared_error: 0.0757\n",
            "Epoch 00008: val_loss improved from 0.47813 to 0.44499, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 457ms/step - loss: 0.4503 - out_loss: 0.0874 - level1_loss: 0.0873 - level2_loss: 0.0870 - level3_loss: 0.0879 - level4_loss: 0.0939 - level5_loss: 0.1193 - out_mean_squared_error: 0.0149 - level1_mean_squared_error: 0.0148 - level2_mean_squared_error: 0.0148 - level3_mean_squared_error: 0.0151 - level4_mean_squared_error: 0.0191 - level5_mean_squared_error: 0.0757 - val_loss: 0.4450 - val_out_loss: 0.0876 - val_level1_loss: 0.0874 - val_level2_loss: 0.0867 - val_level3_loss: 0.0878 - val_level4_loss: 0.0935 - val_level5_loss: 0.1133 - val_out_mean_squared_error: 0.0144 - val_level1_mean_squared_error: 0.0143 - val_level2_mean_squared_error: 0.0141 - val_level3_mean_squared_error: 0.0145 - val_level4_mean_squared_error: 0.0173 - val_level5_mean_squared_error: 0.0514\n",
            "Epoch 9/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4226 - out_loss: 0.0847 - level1_loss: 0.0844 - level2_loss: 0.0842 - level3_loss: 0.0852 - level4_loss: 0.0925 - level5_loss: 0.0974 - out_mean_squared_error: 0.0140 - level1_mean_squared_error: 0.0138 - level2_mean_squared_error: 0.0138 - level3_mean_squared_error: 0.0141 - level4_mean_squared_error: 0.0199 - level5_mean_squared_error: 0.0320\n",
            "Epoch 00009: val_loss improved from 0.44499 to 0.42664, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 442ms/step - loss: 0.4226 - out_loss: 0.0847 - level1_loss: 0.0844 - level2_loss: 0.0842 - level3_loss: 0.0852 - level4_loss: 0.0925 - level5_loss: 0.0974 - out_mean_squared_error: 0.0140 - level1_mean_squared_error: 0.0138 - level2_mean_squared_error: 0.0138 - level3_mean_squared_error: 0.0141 - level4_mean_squared_error: 0.0199 - level5_mean_squared_error: 0.0320 - val_loss: 0.4266 - val_out_loss: 0.0857 - val_level1_loss: 0.0854 - val_level2_loss: 0.0848 - val_level3_loss: 0.0857 - val_level4_loss: 0.0893 - val_level5_loss: 0.1024 - val_out_mean_squared_error: 0.0139 - val_level1_mean_squared_error: 0.0137 - val_level2_mean_squared_error: 0.0136 - val_level3_mean_squared_error: 0.0137 - val_level4_mean_squared_error: 0.0147 - val_level5_mean_squared_error: 0.0247\n",
            "Epoch 10/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4773 - out_loss: 0.0825 - level1_loss: 0.0823 - level2_loss: 0.0819 - level3_loss: 0.0825 - level4_loss: 0.0958 - level5_loss: 0.1717 - out_mean_squared_error: 0.0132 - level1_mean_squared_error: 0.0131 - level2_mean_squared_error: 0.0130 - level3_mean_squared_error: 0.0130 - level4_mean_squared_error: 0.0375 - level5_mean_squared_error: 0.5336\n",
            "Epoch 00010: val_loss did not improve from 0.42664\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.4773 - out_loss: 0.0825 - level1_loss: 0.0823 - level2_loss: 0.0819 - level3_loss: 0.0825 - level4_loss: 0.0958 - level5_loss: 0.1717 - out_mean_squared_error: 0.0132 - level1_mean_squared_error: 0.0131 - level2_mean_squared_error: 0.0130 - level3_mean_squared_error: 0.0130 - level4_mean_squared_error: 0.0375 - level5_mean_squared_error: 0.5336 - val_loss: 0.4513 - val_out_loss: 0.0845 - val_level1_loss: 0.0841 - val_level2_loss: 0.0841 - val_level3_loss: 0.0847 - val_level4_loss: 0.0947 - val_level5_loss: 0.1320 - val_out_mean_squared_error: 0.0136 - val_level1_mean_squared_error: 0.0134 - val_level2_mean_squared_error: 0.0134 - val_level3_mean_squared_error: 0.0136 - val_level4_mean_squared_error: 0.0180 - val_level5_mean_squared_error: 0.0567\n",
            "Epoch 11/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4548 - out_loss: 0.0808 - level1_loss: 0.0805 - level2_loss: 0.0803 - level3_loss: 0.0808 - level4_loss: 0.0896 - level5_loss: 0.1565 - out_mean_squared_error: 0.0128 - level1_mean_squared_error: 0.0126 - level2_mean_squared_error: 0.0125 - level3_mean_squared_error: 0.0126 - level4_mean_squared_error: 0.0189 - level5_mean_squared_error: 0.2440\n",
            "Epoch 00011: val_loss did not improve from 0.42664\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.4548 - out_loss: 0.0808 - level1_loss: 0.0805 - level2_loss: 0.0803 - level3_loss: 0.0808 - level4_loss: 0.0896 - level5_loss: 0.1565 - out_mean_squared_error: 0.0128 - level1_mean_squared_error: 0.0126 - level2_mean_squared_error: 0.0125 - level3_mean_squared_error: 0.0126 - level4_mean_squared_error: 0.0189 - level5_mean_squared_error: 0.2440 - val_loss: 0.4635 - val_out_loss: 0.0834 - val_level1_loss: 0.0833 - val_level2_loss: 0.0832 - val_level3_loss: 0.0840 - val_level4_loss: 0.0943 - val_level5_loss: 0.1511 - val_out_mean_squared_error: 0.0133 - val_level1_mean_squared_error: 0.0132 - val_level2_mean_squared_error: 0.0131 - val_level3_mean_squared_error: 0.0134 - val_level4_mean_squared_error: 0.0211 - val_level5_mean_squared_error: 0.2646\n",
            "Epoch 12/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4248 - out_loss: 0.0795 - level1_loss: 0.0792 - level2_loss: 0.0789 - level3_loss: 0.0793 - level4_loss: 0.0887 - level5_loss: 0.1255 - out_mean_squared_error: 0.0123 - level1_mean_squared_error: 0.0122 - level2_mean_squared_error: 0.0120 - level3_mean_squared_error: 0.0120 - level4_mean_squared_error: 0.0189 - level5_mean_squared_error: 0.1111\n",
            "Epoch 00012: val_loss improved from 0.42664 to 0.42242, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 447ms/step - loss: 0.4248 - out_loss: 0.0795 - level1_loss: 0.0792 - level2_loss: 0.0789 - level3_loss: 0.0793 - level4_loss: 0.0887 - level5_loss: 0.1255 - out_mean_squared_error: 0.0123 - level1_mean_squared_error: 0.0122 - level2_mean_squared_error: 0.0120 - level3_mean_squared_error: 0.0120 - level4_mean_squared_error: 0.0189 - level5_mean_squared_error: 0.1111 - val_loss: 0.4224 - val_out_loss: 0.0824 - val_level1_loss: 0.0821 - val_level2_loss: 0.0818 - val_level3_loss: 0.0823 - val_level4_loss: 0.0879 - val_level5_loss: 0.1116 - val_out_mean_squared_error: 0.0131 - val_level1_mean_squared_error: 0.0129 - val_level2_mean_squared_error: 0.0127 - val_level3_mean_squared_error: 0.0127 - val_level4_mean_squared_error: 0.0149 - val_level5_mean_squared_error: 0.0369\n",
            "Epoch 13/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4166 - out_loss: 0.0784 - level1_loss: 0.0782 - level2_loss: 0.0777 - level3_loss: 0.0786 - level4_loss: 0.0837 - level5_loss: 0.1241 - out_mean_squared_error: 0.0122 - level1_mean_squared_error: 0.0123 - level2_mean_squared_error: 0.0117 - level3_mean_squared_error: 0.0120 - level4_mean_squared_error: 0.0149 - level5_mean_squared_error: 0.1316\n",
            "Epoch 00013: val_loss did not improve from 0.42242\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.4166 - out_loss: 0.0784 - level1_loss: 0.0782 - level2_loss: 0.0777 - level3_loss: 0.0786 - level4_loss: 0.0837 - level5_loss: 0.1241 - out_mean_squared_error: 0.0122 - level1_mean_squared_error: 0.0123 - level2_mean_squared_error: 0.0117 - level3_mean_squared_error: 0.0120 - level4_mean_squared_error: 0.0149 - level5_mean_squared_error: 0.1316 - val_loss: 0.4396 - val_out_loss: 0.0814 - val_level1_loss: 0.0812 - val_level2_loss: 0.0810 - val_level3_loss: 0.0811 - val_level4_loss: 0.0851 - val_level5_loss: 0.1397 - val_out_mean_squared_error: 0.0127 - val_level1_mean_squared_error: 0.0126 - val_level2_mean_squared_error: 0.0125 - val_level3_mean_squared_error: 0.0125 - val_level4_mean_squared_error: 0.0135 - val_level5_mean_squared_error: 0.0926\n",
            "Epoch 14/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4083 - out_loss: 0.0770 - level1_loss: 0.0770 - level2_loss: 0.0761 - level3_loss: 0.0769 - level4_loss: 0.0804 - level5_loss: 0.1229 - out_mean_squared_error: 0.0117 - level1_mean_squared_error: 0.0117 - level2_mean_squared_error: 0.0112 - level3_mean_squared_error: 0.0115 - level4_mean_squared_error: 0.0124 - level5_mean_squared_error: 0.1160\n",
            "Epoch 00014: val_loss did not improve from 0.42242\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.4083 - out_loss: 0.0770 - level1_loss: 0.0770 - level2_loss: 0.0761 - level3_loss: 0.0769 - level4_loss: 0.0804 - level5_loss: 0.1229 - out_mean_squared_error: 0.0117 - level1_mean_squared_error: 0.0117 - level2_mean_squared_error: 0.0112 - level3_mean_squared_error: 0.0115 - level4_mean_squared_error: 0.0124 - level5_mean_squared_error: 0.1160 - val_loss: 0.4297 - val_out_loss: 0.0810 - val_level1_loss: 0.0805 - val_level2_loss: 0.0800 - val_level3_loss: 0.0812 - val_level4_loss: 0.0846 - val_level5_loss: 0.1298 - val_out_mean_squared_error: 0.0128 - val_level1_mean_squared_error: 0.0125 - val_level2_mean_squared_error: 0.0123 - val_level3_mean_squared_error: 0.0125 - val_level4_mean_squared_error: 0.0134 - val_level5_mean_squared_error: 0.0677\n",
            "Epoch 15/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3883 - out_loss: 0.0756 - level1_loss: 0.0754 - level2_loss: 0.0748 - level3_loss: 0.0758 - level4_loss: 0.0792 - level5_loss: 0.1044 - out_mean_squared_error: 0.0114 - level1_mean_squared_error: 0.0113 - level2_mean_squared_error: 0.0109 - level3_mean_squared_error: 0.0110 - level4_mean_squared_error: 0.0118 - level5_mean_squared_error: 0.0529\n",
            "Epoch 00015: val_loss improved from 0.42242 to 0.41342, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 448ms/step - loss: 0.3883 - out_loss: 0.0756 - level1_loss: 0.0754 - level2_loss: 0.0748 - level3_loss: 0.0758 - level4_loss: 0.0792 - level5_loss: 0.1044 - out_mean_squared_error: 0.0114 - level1_mean_squared_error: 0.0113 - level2_mean_squared_error: 0.0109 - level3_mean_squared_error: 0.0110 - level4_mean_squared_error: 0.0118 - level5_mean_squared_error: 0.0529 - val_loss: 0.4134 - val_out_loss: 0.0802 - val_level1_loss: 0.0799 - val_level2_loss: 0.0799 - val_level3_loss: 0.0801 - val_level4_loss: 0.0839 - val_level5_loss: 0.1127 - val_out_mean_squared_error: 0.0125 - val_level1_mean_squared_error: 0.0123 - val_level2_mean_squared_error: 0.0123 - val_level3_mean_squared_error: 0.0123 - val_level4_mean_squared_error: 0.0132 - val_level5_mean_squared_error: 0.0394\n",
            "Epoch 16/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3713 - out_loss: 0.0745 - level1_loss: 0.0741 - level2_loss: 0.0740 - level3_loss: 0.0741 - level4_loss: 0.0783 - level5_loss: 0.0892 - out_mean_squared_error: 0.0110 - level1_mean_squared_error: 0.0107 - level2_mean_squared_error: 0.0107 - level3_mean_squared_error: 0.0105 - level4_mean_squared_error: 0.0117 - level5_mean_squared_error: 0.0239\n",
            "Epoch 00016: val_loss improved from 0.41342 to 0.39869, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 441ms/step - loss: 0.3713 - out_loss: 0.0745 - level1_loss: 0.0741 - level2_loss: 0.0740 - level3_loss: 0.0741 - level4_loss: 0.0783 - level5_loss: 0.0892 - out_mean_squared_error: 0.0110 - level1_mean_squared_error: 0.0107 - level2_mean_squared_error: 0.0107 - level3_mean_squared_error: 0.0105 - level4_mean_squared_error: 0.0117 - level5_mean_squared_error: 0.0239 - val_loss: 0.3987 - val_out_loss: 0.0800 - val_level1_loss: 0.0797 - val_level2_loss: 0.0799 - val_level3_loss: 0.0797 - val_level4_loss: 0.0837 - val_level5_loss: 0.0953 - val_out_mean_squared_error: 0.0124 - val_level1_mean_squared_error: 0.0123 - val_level2_mean_squared_error: 0.0124 - val_level3_mean_squared_error: 0.0122 - val_level4_mean_squared_error: 0.0131 - val_level5_mean_squared_error: 0.0174\n",
            "Epoch 17/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3896 - out_loss: 0.0734 - level1_loss: 0.0731 - level2_loss: 0.0728 - level3_loss: 0.0730 - level4_loss: 0.0773 - level5_loss: 0.1174 - out_mean_squared_error: 0.0106 - level1_mean_squared_error: 0.0105 - level2_mean_squared_error: 0.0104 - level3_mean_squared_error: 0.0102 - level4_mean_squared_error: 0.0115 - level5_mean_squared_error: 0.1465\n",
            "Epoch 00017: val_loss did not improve from 0.39869\n",
            "10/10 [==============================] - 4s 360ms/step - loss: 0.3896 - out_loss: 0.0734 - level1_loss: 0.0731 - level2_loss: 0.0728 - level3_loss: 0.0730 - level4_loss: 0.0773 - level5_loss: 0.1174 - out_mean_squared_error: 0.0106 - level1_mean_squared_error: 0.0105 - level2_mean_squared_error: 0.0104 - level3_mean_squared_error: 0.0102 - level4_mean_squared_error: 0.0115 - level5_mean_squared_error: 0.1465 - val_loss: 0.4413 - val_out_loss: 0.0789 - val_level1_loss: 0.0786 - val_level2_loss: 0.0781 - val_level3_loss: 0.0789 - val_level4_loss: 0.0842 - val_level5_loss: 0.1529 - val_out_mean_squared_error: 0.0122 - val_level1_mean_squared_error: 0.0120 - val_level2_mean_squared_error: 0.0118 - val_level3_mean_squared_error: 0.0118 - val_level4_mean_squared_error: 0.0137 - val_level5_mean_squared_error: 0.2441\n",
            "Epoch 18/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3995 - out_loss: 0.0727 - level1_loss: 0.0725 - level2_loss: 0.0721 - level3_loss: 0.0725 - level4_loss: 0.0773 - level5_loss: 0.1323 - out_mean_squared_error: 0.0105 - level1_mean_squared_error: 0.0103 - level2_mean_squared_error: 0.0102 - level3_mean_squared_error: 0.0100 - level4_mean_squared_error: 0.0116 - level5_mean_squared_error: 0.1857\n",
            "Epoch 00018: val_loss did not improve from 0.39869\n",
            "10/10 [==============================] - 4s 379ms/step - loss: 0.3995 - out_loss: 0.0727 - level1_loss: 0.0725 - level2_loss: 0.0721 - level3_loss: 0.0725 - level4_loss: 0.0773 - level5_loss: 0.1323 - out_mean_squared_error: 0.0105 - level1_mean_squared_error: 0.0103 - level2_mean_squared_error: 0.0102 - level3_mean_squared_error: 0.0100 - level4_mean_squared_error: 0.0116 - level5_mean_squared_error: 0.1857 - val_loss: 0.4224 - val_out_loss: 0.0790 - val_level1_loss: 0.0789 - val_level2_loss: 0.0784 - val_level3_loss: 0.0788 - val_level4_loss: 0.0836 - val_level5_loss: 0.1292 - val_out_mean_squared_error: 0.0121 - val_level1_mean_squared_error: 0.0120 - val_level2_mean_squared_error: 0.0118 - val_level3_mean_squared_error: 0.0118 - val_level4_mean_squared_error: 0.0131 - val_level5_mean_squared_error: 0.0851\n",
            "Epoch 19/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3746 - out_loss: 0.0719 - level1_loss: 0.0716 - level2_loss: 0.0718 - level3_loss: 0.0729 - level4_loss: 0.0764 - level5_loss: 0.1035 - out_mean_squared_error: 0.0102 - level1_mean_squared_error: 0.0100 - level2_mean_squared_error: 0.0102 - level3_mean_squared_error: 0.0108 - level4_mean_squared_error: 0.0115 - level5_mean_squared_error: 0.0573\n",
            "Epoch 00019: val_loss improved from 0.39869 to 0.39000, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 449ms/step - loss: 0.3746 - out_loss: 0.0719 - level1_loss: 0.0716 - level2_loss: 0.0718 - level3_loss: 0.0729 - level4_loss: 0.0764 - level5_loss: 0.1035 - out_mean_squared_error: 0.0102 - level1_mean_squared_error: 0.0100 - level2_mean_squared_error: 0.0102 - level3_mean_squared_error: 0.0108 - level4_mean_squared_error: 0.0115 - level5_mean_squared_error: 0.0573 - val_loss: 0.3900 - val_out_loss: 0.0783 - val_level1_loss: 0.0780 - val_level2_loss: 0.0778 - val_level3_loss: 0.0789 - val_level4_loss: 0.0828 - val_level5_loss: 0.0918 - val_out_mean_squared_error: 0.0120 - val_level1_mean_squared_error: 0.0118 - val_level2_mean_squared_error: 0.0117 - val_level3_mean_squared_error: 0.0121 - val_level4_mean_squared_error: 0.0134 - val_level5_mean_squared_error: 0.0158\n",
            "Epoch 20/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3765 - out_loss: 0.0709 - level1_loss: 0.0706 - level2_loss: 0.0704 - level3_loss: 0.0721 - level4_loss: 0.0769 - level5_loss: 0.1096 - out_mean_squared_error: 0.0100 - level1_mean_squared_error: 0.0099 - level2_mean_squared_error: 0.0098 - level3_mean_squared_error: 0.0109 - level4_mean_squared_error: 0.0134 - level5_mean_squared_error: 0.1224\n",
            "Epoch 00020: val_loss did not improve from 0.39000\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.3765 - out_loss: 0.0709 - level1_loss: 0.0706 - level2_loss: 0.0704 - level3_loss: 0.0721 - level4_loss: 0.0769 - level5_loss: 0.1096 - out_mean_squared_error: 0.0100 - level1_mean_squared_error: 0.0099 - level2_mean_squared_error: 0.0098 - level3_mean_squared_error: 0.0109 - level4_mean_squared_error: 0.0134 - level5_mean_squared_error: 0.1224 - val_loss: 0.3960 - val_out_loss: 0.0782 - val_level1_loss: 0.0780 - val_level2_loss: 0.0774 - val_level3_loss: 0.0800 - val_level4_loss: 0.0824 - val_level5_loss: 0.0990 - val_out_mean_squared_error: 0.0119 - val_level1_mean_squared_error: 0.0118 - val_level2_mean_squared_error: 0.0116 - val_level3_mean_squared_error: 0.0133 - val_level4_mean_squared_error: 0.0131 - val_level5_mean_squared_error: 0.0220\n",
            "Epoch 21/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3947 - out_loss: 0.0702 - level1_loss: 0.0698 - level2_loss: 0.0694 - level3_loss: 0.0723 - level4_loss: 0.0763 - level5_loss: 0.1355 - out_mean_squared_error: 0.0098 - level1_mean_squared_error: 0.0096 - level2_mean_squared_error: 0.0094 - level3_mean_squared_error: 0.0116 - level4_mean_squared_error: 0.0137 - level5_mean_squared_error: 0.2611\n",
            "Epoch 00021: val_loss did not improve from 0.39000\n",
            "10/10 [==============================] - 4s 362ms/step - loss: 0.3947 - out_loss: 0.0702 - level1_loss: 0.0698 - level2_loss: 0.0694 - level3_loss: 0.0723 - level4_loss: 0.0763 - level5_loss: 0.1355 - out_mean_squared_error: 0.0098 - level1_mean_squared_error: 0.0096 - level2_mean_squared_error: 0.0094 - level3_mean_squared_error: 0.0116 - level4_mean_squared_error: 0.0137 - level5_mean_squared_error: 0.2611 - val_loss: 0.4549 - val_out_loss: 0.0777 - val_level1_loss: 0.0773 - val_level2_loss: 0.0768 - val_level3_loss: 0.0784 - val_level4_loss: 0.0819 - val_level5_loss: 0.1765 - val_out_mean_squared_error: 0.0119 - val_level1_mean_squared_error: 0.0117 - val_level2_mean_squared_error: 0.0115 - val_level3_mean_squared_error: 0.0121 - val_level4_mean_squared_error: 0.0130 - val_level5_mean_squared_error: 0.3769\n",
            "Epoch 22/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3982 - out_loss: 0.0697 - level1_loss: 0.0694 - level2_loss: 0.0688 - level3_loss: 0.0716 - level4_loss: 0.0744 - level5_loss: 0.1439 - out_mean_squared_error: 0.0098 - level1_mean_squared_error: 0.0096 - level2_mean_squared_error: 0.0093 - level3_mean_squared_error: 0.0113 - level4_mean_squared_error: 0.0110 - level5_mean_squared_error: 0.2632\n",
            "Epoch 00022: val_loss did not improve from 0.39000\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.3982 - out_loss: 0.0697 - level1_loss: 0.0694 - level2_loss: 0.0688 - level3_loss: 0.0716 - level4_loss: 0.0744 - level5_loss: 0.1439 - out_mean_squared_error: 0.0098 - level1_mean_squared_error: 0.0096 - level2_mean_squared_error: 0.0093 - level3_mean_squared_error: 0.0113 - level4_mean_squared_error: 0.0110 - level5_mean_squared_error: 0.2632 - val_loss: 0.4045 - val_out_loss: 0.0772 - val_level1_loss: 0.0773 - val_level2_loss: 0.0773 - val_level3_loss: 0.0780 - val_level4_loss: 0.0815 - val_level5_loss: 0.1143 - val_out_mean_squared_error: 0.0118 - val_level1_mean_squared_error: 0.0117 - val_level2_mean_squared_error: 0.0117 - val_level3_mean_squared_error: 0.0118 - val_level4_mean_squared_error: 0.0128 - val_level5_mean_squared_error: 0.0491\n",
            "Epoch 23/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3649 - out_loss: 0.0690 - level1_loss: 0.0690 - level2_loss: 0.0685 - level3_loss: 0.0702 - level4_loss: 0.0723 - level5_loss: 0.1070 - out_mean_squared_error: 0.0096 - level1_mean_squared_error: 0.0095 - level2_mean_squared_error: 0.0093 - level3_mean_squared_error: 0.0108 - level4_mean_squared_error: 0.0100 - level5_mean_squared_error: 0.0875\n",
            "Epoch 00023: val_loss did not improve from 0.39000\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.3649 - out_loss: 0.0690 - level1_loss: 0.0690 - level2_loss: 0.0685 - level3_loss: 0.0702 - level4_loss: 0.0723 - level5_loss: 0.1070 - out_mean_squared_error: 0.0096 - level1_mean_squared_error: 0.0095 - level2_mean_squared_error: 0.0093 - level3_mean_squared_error: 0.0108 - level4_mean_squared_error: 0.0100 - level5_mean_squared_error: 0.0875 - val_loss: 0.4237 - val_out_loss: 0.0776 - val_level1_loss: 0.0772 - val_level2_loss: 0.0766 - val_level3_loss: 0.0799 - val_level4_loss: 0.0810 - val_level5_loss: 0.1373 - val_out_mean_squared_error: 0.0118 - val_level1_mean_squared_error: 0.0115 - val_level2_mean_squared_error: 0.0113 - val_level3_mean_squared_error: 0.0132 - val_level4_mean_squared_error: 0.0125 - val_level5_mean_squared_error: 0.1665\n",
            "Epoch 24/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3663 - out_loss: 0.0683 - level1_loss: 0.0681 - level2_loss: 0.0682 - level3_loss: 0.0702 - level4_loss: 0.0744 - level5_loss: 0.1087 - out_mean_squared_error: 0.0094 - level1_mean_squared_error: 0.0094 - level2_mean_squared_error: 0.0093 - level3_mean_squared_error: 0.0110 - level4_mean_squared_error: 0.0117 - level5_mean_squared_error: 0.1187\n",
            "Epoch 00024: val_loss did not improve from 0.39000\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.3663 - out_loss: 0.0683 - level1_loss: 0.0681 - level2_loss: 0.0682 - level3_loss: 0.0702 - level4_loss: 0.0744 - level5_loss: 0.1087 - out_mean_squared_error: 0.0094 - level1_mean_squared_error: 0.0094 - level2_mean_squared_error: 0.0093 - level3_mean_squared_error: 0.0110 - level4_mean_squared_error: 0.0117 - level5_mean_squared_error: 0.1187 - val_loss: 0.4066 - val_out_loss: 0.0770 - val_level1_loss: 0.0767 - val_level2_loss: 0.0762 - val_level3_loss: 0.0770 - val_level4_loss: 0.0806 - val_level5_loss: 0.1208 - val_out_mean_squared_error: 0.0117 - val_level1_mean_squared_error: 0.0115 - val_level2_mean_squared_error: 0.0113 - val_level3_mean_squared_error: 0.0114 - val_level4_mean_squared_error: 0.0124 - val_level5_mean_squared_error: 0.0685\n",
            "Epoch 25/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3685 - out_loss: 0.0678 - level1_loss: 0.0676 - level2_loss: 0.0670 - level3_loss: 0.0693 - level4_loss: 0.0765 - level5_loss: 0.1124 - out_mean_squared_error: 0.0093 - level1_mean_squared_error: 0.0091 - level2_mean_squared_error: 0.0088 - level3_mean_squared_error: 0.0101 - level4_mean_squared_error: 0.0145 - level5_mean_squared_error: 0.1081\n",
            "Epoch 00025: val_loss did not improve from 0.39000\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.3685 - out_loss: 0.0678 - level1_loss: 0.0676 - level2_loss: 0.0670 - level3_loss: 0.0693 - level4_loss: 0.0765 - level5_loss: 0.1124 - out_mean_squared_error: 0.0093 - level1_mean_squared_error: 0.0091 - level2_mean_squared_error: 0.0088 - level3_mean_squared_error: 0.0101 - level4_mean_squared_error: 0.0145 - level5_mean_squared_error: 0.1081 - val_loss: 0.4395 - val_out_loss: 0.0774 - val_level1_loss: 0.0770 - val_level2_loss: 0.0764 - val_level3_loss: 0.0778 - val_level4_loss: 0.0810 - val_level5_loss: 0.1597 - val_out_mean_squared_error: 0.0118 - val_level1_mean_squared_error: 0.0116 - val_level2_mean_squared_error: 0.0113 - val_level3_mean_squared_error: 0.0118 - val_level4_mean_squared_error: 0.0125 - val_level5_mean_squared_error: 0.2307\n",
            "Epoch 26/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3646 - out_loss: 0.0674 - level1_loss: 0.0670 - level2_loss: 0.0666 - level3_loss: 0.0672 - level4_loss: 0.0760 - level5_loss: 0.1116 - out_mean_squared_error: 0.0091 - level1_mean_squared_error: 0.0090 - level2_mean_squared_error: 0.0087 - level3_mean_squared_error: 0.0088 - level4_mean_squared_error: 0.0148 - level5_mean_squared_error: 0.0984\n",
            "Epoch 00026: val_loss did not improve from 0.39000\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.3646 - out_loss: 0.0674 - level1_loss: 0.0670 - level2_loss: 0.0666 - level3_loss: 0.0672 - level4_loss: 0.0760 - level5_loss: 0.1116 - out_mean_squared_error: 0.0091 - level1_mean_squared_error: 0.0090 - level2_mean_squared_error: 0.0087 - level3_mean_squared_error: 0.0088 - level4_mean_squared_error: 0.0148 - level5_mean_squared_error: 0.0984 - val_loss: 0.4247 - val_out_loss: 0.0764 - val_level1_loss: 0.0763 - val_level2_loss: 0.0756 - val_level3_loss: 0.0760 - val_level4_loss: 0.0816 - val_level5_loss: 0.1449 - val_out_mean_squared_error: 0.0116 - val_level1_mean_squared_error: 0.0115 - val_level2_mean_squared_error: 0.0112 - val_level3_mean_squared_error: 0.0112 - val_level4_mean_squared_error: 0.0129 - val_level5_mean_squared_error: 0.2968\n",
            "Epoch 27/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3714 - out_loss: 0.0669 - level1_loss: 0.0665 - level2_loss: 0.0660 - level3_loss: 0.0663 - level4_loss: 0.0740 - level5_loss: 0.1245 - out_mean_squared_error: 0.0090 - level1_mean_squared_error: 0.0088 - level2_mean_squared_error: 0.0085 - level3_mean_squared_error: 0.0085 - level4_mean_squared_error: 0.0131 - level5_mean_squared_error: 0.1987\n",
            "Epoch 00027: val_loss improved from 0.39000 to 0.38878, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 446ms/step - loss: 0.3714 - out_loss: 0.0669 - level1_loss: 0.0665 - level2_loss: 0.0660 - level3_loss: 0.0663 - level4_loss: 0.0740 - level5_loss: 0.1245 - out_mean_squared_error: 0.0090 - level1_mean_squared_error: 0.0088 - level2_mean_squared_error: 0.0085 - level3_mean_squared_error: 0.0085 - level4_mean_squared_error: 0.0131 - level5_mean_squared_error: 0.1987 - val_loss: 0.3888 - val_out_loss: 0.0766 - val_level1_loss: 0.0763 - val_level2_loss: 0.0759 - val_level3_loss: 0.0761 - val_level4_loss: 0.0826 - val_level5_loss: 0.0986 - val_out_mean_squared_error: 0.0116 - val_level1_mean_squared_error: 0.0114 - val_level2_mean_squared_error: 0.0112 - val_level3_mean_squared_error: 0.0113 - val_level4_mean_squared_error: 0.0135 - val_level5_mean_squared_error: 0.0198\n",
            "Epoch 28/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3561 - out_loss: 0.0664 - level1_loss: 0.0660 - level2_loss: 0.0658 - level3_loss: 0.0661 - level4_loss: 0.0731 - level5_loss: 0.1077 - out_mean_squared_error: 0.0089 - level1_mean_squared_error: 0.0087 - level2_mean_squared_error: 0.0087 - level3_mean_squared_error: 0.0084 - level4_mean_squared_error: 0.0115 - level5_mean_squared_error: 0.1185\n",
            "Epoch 00028: val_loss did not improve from 0.38878\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.3561 - out_loss: 0.0664 - level1_loss: 0.0660 - level2_loss: 0.0658 - level3_loss: 0.0661 - level4_loss: 0.0731 - level5_loss: 0.1077 - out_mean_squared_error: 0.0089 - level1_mean_squared_error: 0.0087 - level2_mean_squared_error: 0.0087 - level3_mean_squared_error: 0.0084 - level4_mean_squared_error: 0.0115 - level5_mean_squared_error: 0.1185 - val_loss: 0.4399 - val_out_loss: 0.0768 - val_level1_loss: 0.0765 - val_level2_loss: 0.0758 - val_level3_loss: 0.0765 - val_level4_loss: 0.0835 - val_level5_loss: 0.1608 - val_out_mean_squared_error: 0.0117 - val_level1_mean_squared_error: 0.0115 - val_level2_mean_squared_error: 0.0112 - val_level3_mean_squared_error: 0.0113 - val_level4_mean_squared_error: 0.0134 - val_level5_mean_squared_error: 0.3194\n",
            "Epoch 29/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4078 - out_loss: 0.0657 - level1_loss: 0.0653 - level2_loss: 0.0649 - level3_loss: 0.0657 - level4_loss: 0.0707 - level5_loss: 0.1775 - out_mean_squared_error: 0.0088 - level1_mean_squared_error: 0.0086 - level2_mean_squared_error: 0.0084 - level3_mean_squared_error: 0.0090 - level4_mean_squared_error: 0.0104 - level5_mean_squared_error: 0.8316\n",
            "Epoch 00029: val_loss did not improve from 0.38878\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.4078 - out_loss: 0.0657 - level1_loss: 0.0653 - level2_loss: 0.0649 - level3_loss: 0.0657 - level4_loss: 0.0707 - level5_loss: 0.1775 - out_mean_squared_error: 0.0088 - level1_mean_squared_error: 0.0086 - level2_mean_squared_error: 0.0084 - level3_mean_squared_error: 0.0090 - level4_mean_squared_error: 0.0104 - level5_mean_squared_error: 0.8316 - val_loss: 0.4132 - val_out_loss: 0.0765 - val_level1_loss: 0.0762 - val_level2_loss: 0.0757 - val_level3_loss: 0.0766 - val_level4_loss: 0.0836 - val_level5_loss: 0.1277 - val_out_mean_squared_error: 0.0116 - val_level1_mean_squared_error: 0.0114 - val_level2_mean_squared_error: 0.0112 - val_level3_mean_squared_error: 0.0113 - val_level4_mean_squared_error: 0.0141 - val_level5_mean_squared_error: 0.0540\n",
            "Epoch 30/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3588 - out_loss: 0.0653 - level1_loss: 0.0649 - level2_loss: 0.0645 - level3_loss: 0.0654 - level4_loss: 0.0709 - level5_loss: 0.1174 - out_mean_squared_error: 0.0087 - level1_mean_squared_error: 0.0085 - level2_mean_squared_error: 0.0082 - level3_mean_squared_error: 0.0085 - level4_mean_squared_error: 0.0111 - level5_mean_squared_error: 0.1198\n",
            "Epoch 00030: val_loss improved from 0.38878 to 0.38395, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 446ms/step - loss: 0.3588 - out_loss: 0.0653 - level1_loss: 0.0649 - level2_loss: 0.0645 - level3_loss: 0.0654 - level4_loss: 0.0709 - level5_loss: 0.1174 - out_mean_squared_error: 0.0087 - level1_mean_squared_error: 0.0085 - level2_mean_squared_error: 0.0082 - level3_mean_squared_error: 0.0085 - level4_mean_squared_error: 0.0111 - level5_mean_squared_error: 0.1198 - val_loss: 0.3840 - val_out_loss: 0.0762 - val_level1_loss: 0.0758 - val_level2_loss: 0.0751 - val_level3_loss: 0.0757 - val_level4_loss: 0.0812 - val_level5_loss: 0.0958 - val_out_mean_squared_error: 0.0115 - val_level1_mean_squared_error: 0.0113 - val_level2_mean_squared_error: 0.0110 - val_level3_mean_squared_error: 0.0112 - val_level4_mean_squared_error: 0.0128 - val_level5_mean_squared_error: 0.0161\n",
            "Epoch 31/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3455 - out_loss: 0.0650 - level1_loss: 0.0645 - level2_loss: 0.0641 - level3_loss: 0.0645 - level4_loss: 0.0687 - level5_loss: 0.1052 - out_mean_squared_error: 0.0086 - level1_mean_squared_error: 0.0084 - level2_mean_squared_error: 0.0082 - level3_mean_squared_error: 0.0082 - level4_mean_squared_error: 0.0094 - level5_mean_squared_error: 0.0896\n",
            "Epoch 00031: val_loss did not improve from 0.38395\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.3455 - out_loss: 0.0650 - level1_loss: 0.0645 - level2_loss: 0.0641 - level3_loss: 0.0645 - level4_loss: 0.0687 - level5_loss: 0.1052 - out_mean_squared_error: 0.0086 - level1_mean_squared_error: 0.0084 - level2_mean_squared_error: 0.0082 - level3_mean_squared_error: 0.0082 - level4_mean_squared_error: 0.0094 - level5_mean_squared_error: 0.0896 - val_loss: 0.4160 - val_out_loss: 0.0761 - val_level1_loss: 0.0759 - val_level2_loss: 0.0753 - val_level3_loss: 0.0760 - val_level4_loss: 0.0797 - val_level5_loss: 0.1370 - val_out_mean_squared_error: 0.0115 - val_level1_mean_squared_error: 0.0113 - val_level2_mean_squared_error: 0.0111 - val_level3_mean_squared_error: 0.0113 - val_level4_mean_squared_error: 0.0122 - val_level5_mean_squared_error: 0.1713\n",
            "Epoch 32/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3493 - out_loss: 0.0646 - level1_loss: 0.0641 - level2_loss: 0.0635 - level3_loss: 0.0649 - level4_loss: 0.0679 - level5_loss: 0.1117 - out_mean_squared_error: 0.0085 - level1_mean_squared_error: 0.0083 - level2_mean_squared_error: 0.0080 - level3_mean_squared_error: 0.0086 - level4_mean_squared_error: 0.0093 - level5_mean_squared_error: 0.1051\n",
            "Epoch 00032: val_loss improved from 0.38395 to 0.38273, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 444ms/step - loss: 0.3493 - out_loss: 0.0646 - level1_loss: 0.0641 - level2_loss: 0.0635 - level3_loss: 0.0649 - level4_loss: 0.0679 - level5_loss: 0.1117 - out_mean_squared_error: 0.0085 - level1_mean_squared_error: 0.0083 - level2_mean_squared_error: 0.0080 - level3_mean_squared_error: 0.0086 - level4_mean_squared_error: 0.0093 - level5_mean_squared_error: 0.1051 - val_loss: 0.3827 - val_out_loss: 0.0760 - val_level1_loss: 0.0758 - val_level2_loss: 0.0752 - val_level3_loss: 0.0757 - val_level4_loss: 0.0805 - val_level5_loss: 0.0952 - val_out_mean_squared_error: 0.0116 - val_level1_mean_squared_error: 0.0114 - val_level2_mean_squared_error: 0.0111 - val_level3_mean_squared_error: 0.0112 - val_level4_mean_squared_error: 0.0126 - val_level5_mean_squared_error: 0.0176\n",
            "Epoch 33/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3444 - out_loss: 0.0641 - level1_loss: 0.0638 - level2_loss: 0.0633 - level3_loss: 0.0643 - level4_loss: 0.0677 - level5_loss: 0.1072 - out_mean_squared_error: 0.0085 - level1_mean_squared_error: 0.0082 - level2_mean_squared_error: 0.0080 - level3_mean_squared_error: 0.0082 - level4_mean_squared_error: 0.0093 - level5_mean_squared_error: 0.1647\n",
            "Epoch 00033: val_loss did not improve from 0.38273\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.3444 - out_loss: 0.0641 - level1_loss: 0.0638 - level2_loss: 0.0633 - level3_loss: 0.0643 - level4_loss: 0.0677 - level5_loss: 0.1072 - out_mean_squared_error: 0.0085 - level1_mean_squared_error: 0.0082 - level2_mean_squared_error: 0.0080 - level3_mean_squared_error: 0.0082 - level4_mean_squared_error: 0.0093 - level5_mean_squared_error: 0.1647 - val_loss: 0.3984 - val_out_loss: 0.0762 - val_level1_loss: 0.0759 - val_level2_loss: 0.0754 - val_level3_loss: 0.0760 - val_level4_loss: 0.0810 - val_level5_loss: 0.1135 - val_out_mean_squared_error: 0.0116 - val_level1_mean_squared_error: 0.0113 - val_level2_mean_squared_error: 0.0112 - val_level3_mean_squared_error: 0.0113 - val_level4_mean_squared_error: 0.0135 - val_level5_mean_squared_error: 0.0558\n",
            "Epoch 34/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3333 - out_loss: 0.0639 - level1_loss: 0.0635 - level2_loss: 0.0628 - level3_loss: 0.0635 - level4_loss: 0.0672 - level5_loss: 0.0956 - out_mean_squared_error: 0.0084 - level1_mean_squared_error: 0.0082 - level2_mean_squared_error: 0.0079 - level3_mean_squared_error: 0.0080 - level4_mean_squared_error: 0.0090 - level5_mean_squared_error: 0.0700\n",
            "Epoch 00034: val_loss did not improve from 0.38273\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.3333 - out_loss: 0.0639 - level1_loss: 0.0635 - level2_loss: 0.0628 - level3_loss: 0.0635 - level4_loss: 0.0672 - level5_loss: 0.0956 - out_mean_squared_error: 0.0084 - level1_mean_squared_error: 0.0082 - level2_mean_squared_error: 0.0079 - level3_mean_squared_error: 0.0080 - level4_mean_squared_error: 0.0090 - level5_mean_squared_error: 0.0700 - val_loss: 0.4048 - val_out_loss: 0.0755 - val_level1_loss: 0.0753 - val_level2_loss: 0.0746 - val_level3_loss: 0.0749 - val_level4_loss: 0.0789 - val_level5_loss: 0.1269 - val_out_mean_squared_error: 0.0114 - val_level1_mean_squared_error: 0.0112 - val_level2_mean_squared_error: 0.0109 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0119 - val_level5_mean_squared_error: 0.0961\n",
            "Epoch 35/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3260 - out_loss: 0.0632 - level1_loss: 0.0628 - level2_loss: 0.0622 - level3_loss: 0.0629 - level4_loss: 0.0659 - level5_loss: 0.0905 - out_mean_squared_error: 0.0082 - level1_mean_squared_error: 0.0081 - level2_mean_squared_error: 0.0078 - level3_mean_squared_error: 0.0079 - level4_mean_squared_error: 0.0084 - level5_mean_squared_error: 0.0468\n",
            "Epoch 00035: val_loss improved from 0.38273 to 0.37929, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 447ms/step - loss: 0.3260 - out_loss: 0.0632 - level1_loss: 0.0628 - level2_loss: 0.0622 - level3_loss: 0.0629 - level4_loss: 0.0659 - level5_loss: 0.0905 - out_mean_squared_error: 0.0082 - level1_mean_squared_error: 0.0081 - level2_mean_squared_error: 0.0078 - level3_mean_squared_error: 0.0079 - level4_mean_squared_error: 0.0084 - level5_mean_squared_error: 0.0468 - val_loss: 0.3793 - val_out_loss: 0.0756 - val_level1_loss: 0.0755 - val_level2_loss: 0.0748 - val_level3_loss: 0.0753 - val_level4_loss: 0.0800 - val_level5_loss: 0.0929 - val_out_mean_squared_error: 0.0114 - val_level1_mean_squared_error: 0.0113 - val_level2_mean_squared_error: 0.0110 - val_level3_mean_squared_error: 0.0110 - val_level4_mean_squared_error: 0.0126 - val_level5_mean_squared_error: 0.0263\n",
            "Epoch 36/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3196 - out_loss: 0.0628 - level1_loss: 0.0623 - level2_loss: 0.0618 - level3_loss: 0.0626 - level4_loss: 0.0669 - level5_loss: 0.0830 - out_mean_squared_error: 0.0082 - level1_mean_squared_error: 0.0079 - level2_mean_squared_error: 0.0076 - level3_mean_squared_error: 0.0079 - level4_mean_squared_error: 0.0095 - level5_mean_squared_error: 0.0550\n",
            "Epoch 00036: val_loss did not improve from 0.37929\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.3196 - out_loss: 0.0628 - level1_loss: 0.0623 - level2_loss: 0.0618 - level3_loss: 0.0626 - level4_loss: 0.0669 - level5_loss: 0.0830 - out_mean_squared_error: 0.0082 - level1_mean_squared_error: 0.0079 - level2_mean_squared_error: 0.0076 - level3_mean_squared_error: 0.0079 - level4_mean_squared_error: 0.0095 - level5_mean_squared_error: 0.0550 - val_loss: 0.3904 - val_out_loss: 0.0754 - val_level1_loss: 0.0750 - val_level2_loss: 0.0744 - val_level3_loss: 0.0750 - val_level4_loss: 0.0789 - val_level5_loss: 0.1093 - val_out_mean_squared_error: 0.0113 - val_level1_mean_squared_error: 0.0111 - val_level2_mean_squared_error: 0.0109 - val_level3_mean_squared_error: 0.0110 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0389\n",
            "Epoch 37/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3173 - out_loss: 0.0625 - level1_loss: 0.0620 - level2_loss: 0.0616 - level3_loss: 0.0622 - level4_loss: 0.0660 - level5_loss: 0.0822 - out_mean_squared_error: 0.0081 - level1_mean_squared_error: 0.0078 - level2_mean_squared_error: 0.0076 - level3_mean_squared_error: 0.0077 - level4_mean_squared_error: 0.0087 - level5_mean_squared_error: 0.0334\n",
            "Epoch 00037: val_loss did not improve from 0.37929\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.3173 - out_loss: 0.0625 - level1_loss: 0.0620 - level2_loss: 0.0616 - level3_loss: 0.0622 - level4_loss: 0.0660 - level5_loss: 0.0822 - out_mean_squared_error: 0.0081 - level1_mean_squared_error: 0.0078 - level2_mean_squared_error: 0.0076 - level3_mean_squared_error: 0.0077 - level4_mean_squared_error: 0.0087 - level5_mean_squared_error: 0.0334 - val_loss: 0.4004 - val_out_loss: 0.0756 - val_level1_loss: 0.0755 - val_level2_loss: 0.0750 - val_level3_loss: 0.0751 - val_level4_loss: 0.0790 - val_level5_loss: 0.1203 - val_out_mean_squared_error: 0.0114 - val_level1_mean_squared_error: 0.0113 - val_level2_mean_squared_error: 0.0110 - val_level3_mean_squared_error: 0.0110 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.1309\n",
            "Epoch 38/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3386 - out_loss: 0.0623 - level1_loss: 0.0619 - level2_loss: 0.0615 - level3_loss: 0.0616 - level4_loss: 0.0653 - level5_loss: 0.1107 - out_mean_squared_error: 0.0081 - level1_mean_squared_error: 0.0079 - level2_mean_squared_error: 0.0077 - level3_mean_squared_error: 0.0074 - level4_mean_squared_error: 0.0084 - level5_mean_squared_error: 0.1487\n",
            "Epoch 00038: val_loss did not improve from 0.37929\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.3386 - out_loss: 0.0623 - level1_loss: 0.0619 - level2_loss: 0.0615 - level3_loss: 0.0616 - level4_loss: 0.0653 - level5_loss: 0.1107 - out_mean_squared_error: 0.0081 - level1_mean_squared_error: 0.0079 - level2_mean_squared_error: 0.0077 - level3_mean_squared_error: 0.0074 - level4_mean_squared_error: 0.0084 - level5_mean_squared_error: 0.1487 - val_loss: 0.4154 - val_out_loss: 0.0754 - val_level1_loss: 0.0750 - val_level2_loss: 0.0748 - val_level3_loss: 0.0752 - val_level4_loss: 0.0792 - val_level5_loss: 0.1396 - val_out_mean_squared_error: 0.0114 - val_level1_mean_squared_error: 0.0111 - val_level2_mean_squared_error: 0.0110 - val_level3_mean_squared_error: 0.0111 - val_level4_mean_squared_error: 0.0121 - val_level5_mean_squared_error: 0.1573\n",
            "Epoch 39/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3330 - out_loss: 0.0618 - level1_loss: 0.0615 - level2_loss: 0.0612 - level3_loss: 0.0616 - level4_loss: 0.0650 - level5_loss: 0.1052 - out_mean_squared_error: 0.0080 - level1_mean_squared_error: 0.0077 - level2_mean_squared_error: 0.0075 - level3_mean_squared_error: 0.0075 - level4_mean_squared_error: 0.0085 - level5_mean_squared_error: 0.1290\n",
            "Epoch 00039: val_loss improved from 0.37929 to 0.37918, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 451ms/step - loss: 0.3330 - out_loss: 0.0618 - level1_loss: 0.0615 - level2_loss: 0.0612 - level3_loss: 0.0616 - level4_loss: 0.0650 - level5_loss: 0.1052 - out_mean_squared_error: 0.0080 - level1_mean_squared_error: 0.0077 - level2_mean_squared_error: 0.0075 - level3_mean_squared_error: 0.0075 - level4_mean_squared_error: 0.0085 - level5_mean_squared_error: 0.1290 - val_loss: 0.3792 - val_out_loss: 0.0756 - val_level1_loss: 0.0754 - val_level2_loss: 0.0749 - val_level3_loss: 0.0753 - val_level4_loss: 0.0795 - val_level5_loss: 0.0932 - val_out_mean_squared_error: 0.0114 - val_level1_mean_squared_error: 0.0112 - val_level2_mean_squared_error: 0.0110 - val_level3_mean_squared_error: 0.0110 - val_level4_mean_squared_error: 0.0123 - val_level5_mean_squared_error: 0.0218\n",
            "Epoch 40/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3199 - out_loss: 0.0615 - level1_loss: 0.0610 - level2_loss: 0.0610 - level3_loss: 0.0608 - level4_loss: 0.0653 - level5_loss: 0.0903 - out_mean_squared_error: 0.0079 - level1_mean_squared_error: 0.0076 - level2_mean_squared_error: 0.0076 - level3_mean_squared_error: 0.0073 - level4_mean_squared_error: 0.0086 - level5_mean_squared_error: 0.0672\n",
            "Epoch 00040: val_loss did not improve from 0.37918\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.3199 - out_loss: 0.0615 - level1_loss: 0.0610 - level2_loss: 0.0610 - level3_loss: 0.0608 - level4_loss: 0.0653 - level5_loss: 0.0903 - out_mean_squared_error: 0.0079 - level1_mean_squared_error: 0.0076 - level2_mean_squared_error: 0.0076 - level3_mean_squared_error: 0.0073 - level4_mean_squared_error: 0.0086 - level5_mean_squared_error: 0.0672 - val_loss: 0.3915 - val_out_loss: 0.0752 - val_level1_loss: 0.0749 - val_level2_loss: 0.0742 - val_level3_loss: 0.0747 - val_level4_loss: 0.0782 - val_level5_loss: 0.1123 - val_out_mean_squared_error: 0.0113 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0705\n",
            "Epoch 41/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3170 - out_loss: 0.0611 - level1_loss: 0.0606 - level2_loss: 0.0601 - level3_loss: 0.0604 - level4_loss: 0.0647 - level5_loss: 0.0894 - out_mean_squared_error: 0.0078 - level1_mean_squared_error: 0.0075 - level2_mean_squared_error: 0.0073 - level3_mean_squared_error: 0.0072 - level4_mean_squared_error: 0.0085 - level5_mean_squared_error: 0.0488\n",
            "Epoch 00041: val_loss did not improve from 0.37918\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.3170 - out_loss: 0.0611 - level1_loss: 0.0606 - level2_loss: 0.0601 - level3_loss: 0.0604 - level4_loss: 0.0647 - level5_loss: 0.0894 - out_mean_squared_error: 0.0078 - level1_mean_squared_error: 0.0075 - level2_mean_squared_error: 0.0073 - level3_mean_squared_error: 0.0072 - level4_mean_squared_error: 0.0085 - level5_mean_squared_error: 0.0488 - val_loss: 0.3796 - val_out_loss: 0.0753 - val_level1_loss: 0.0752 - val_level2_loss: 0.0745 - val_level3_loss: 0.0747 - val_level4_loss: 0.0797 - val_level5_loss: 0.0951 - val_out_mean_squared_error: 0.0113 - val_level1_mean_squared_error: 0.0112 - val_level2_mean_squared_error: 0.0109 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0125 - val_level5_mean_squared_error: 0.0157\n",
            "Epoch 42/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3093 - out_loss: 0.0607 - level1_loss: 0.0603 - level2_loss: 0.0597 - level3_loss: 0.0602 - level4_loss: 0.0638 - level5_loss: 0.0819 - out_mean_squared_error: 0.0077 - level1_mean_squared_error: 0.0075 - level2_mean_squared_error: 0.0072 - level3_mean_squared_error: 0.0072 - level4_mean_squared_error: 0.0080 - level5_mean_squared_error: 0.0262\n",
            "Epoch 00042: val_loss did not improve from 0.37918\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.3093 - out_loss: 0.0607 - level1_loss: 0.0603 - level2_loss: 0.0597 - level3_loss: 0.0602 - level4_loss: 0.0638 - level5_loss: 0.0819 - out_mean_squared_error: 0.0077 - level1_mean_squared_error: 0.0075 - level2_mean_squared_error: 0.0072 - level3_mean_squared_error: 0.0072 - level4_mean_squared_error: 0.0080 - level5_mean_squared_error: 0.0262 - val_loss: 0.3826 - val_out_loss: 0.0755 - val_level1_loss: 0.0751 - val_level2_loss: 0.0751 - val_level3_loss: 0.0748 - val_level4_loss: 0.0786 - val_level5_loss: 0.0991 - val_out_mean_squared_error: 0.0114 - val_level1_mean_squared_error: 0.0112 - val_level2_mean_squared_error: 0.0112 - val_level3_mean_squared_error: 0.0110 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0166\n",
            "Epoch 43/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3068 - out_loss: 0.0605 - level1_loss: 0.0601 - level2_loss: 0.0598 - level3_loss: 0.0600 - level4_loss: 0.0634 - level5_loss: 0.0797 - out_mean_squared_error: 0.0077 - level1_mean_squared_error: 0.0074 - level2_mean_squared_error: 0.0072 - level3_mean_squared_error: 0.0071 - level4_mean_squared_error: 0.0079 - level5_mean_squared_error: 0.0197\n",
            "Epoch 00043: val_loss improved from 0.37918 to 0.37776, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 450ms/step - loss: 0.3068 - out_loss: 0.0605 - level1_loss: 0.0601 - level2_loss: 0.0598 - level3_loss: 0.0600 - level4_loss: 0.0634 - level5_loss: 0.0797 - out_mean_squared_error: 0.0077 - level1_mean_squared_error: 0.0074 - level2_mean_squared_error: 0.0072 - level3_mean_squared_error: 0.0071 - level4_mean_squared_error: 0.0079 - level5_mean_squared_error: 0.0197 - val_loss: 0.3778 - val_out_loss: 0.0750 - val_level1_loss: 0.0747 - val_level2_loss: 0.0741 - val_level3_loss: 0.0747 - val_level4_loss: 0.0788 - val_level5_loss: 0.0950 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0261\n",
            "Epoch 44/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3027 - out_loss: 0.0603 - level1_loss: 0.0597 - level2_loss: 0.0595 - level3_loss: 0.0597 - level4_loss: 0.0633 - level5_loss: 0.0758 - out_mean_squared_error: 0.0077 - level1_mean_squared_error: 0.0074 - level2_mean_squared_error: 0.0072 - level3_mean_squared_error: 0.0071 - level4_mean_squared_error: 0.0080 - level5_mean_squared_error: 0.0341\n",
            "Epoch 00044: val_loss did not improve from 0.37776\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.3027 - out_loss: 0.0603 - level1_loss: 0.0597 - level2_loss: 0.0595 - level3_loss: 0.0597 - level4_loss: 0.0633 - level5_loss: 0.0758 - out_mean_squared_error: 0.0077 - level1_mean_squared_error: 0.0074 - level2_mean_squared_error: 0.0072 - level3_mean_squared_error: 0.0071 - level4_mean_squared_error: 0.0080 - level5_mean_squared_error: 0.0341 - val_loss: 0.3794 - val_out_loss: 0.0749 - val_level1_loss: 0.0747 - val_level2_loss: 0.0740 - val_level3_loss: 0.0750 - val_level4_loss: 0.0781 - val_level5_loss: 0.0974 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0111 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0317\n",
            "Epoch 45/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3074 - out_loss: 0.0597 - level1_loss: 0.0592 - level2_loss: 0.0588 - level3_loss: 0.0591 - level4_loss: 0.0625 - level5_loss: 0.0850 - out_mean_squared_error: 0.0075 - level1_mean_squared_error: 0.0072 - level2_mean_squared_error: 0.0071 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0076 - level5_mean_squared_error: 0.0733\n",
            "Epoch 00045: val_loss did not improve from 0.37776\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.3074 - out_loss: 0.0597 - level1_loss: 0.0592 - level2_loss: 0.0588 - level3_loss: 0.0591 - level4_loss: 0.0625 - level5_loss: 0.0850 - out_mean_squared_error: 0.0075 - level1_mean_squared_error: 0.0072 - level2_mean_squared_error: 0.0071 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0076 - level5_mean_squared_error: 0.0733 - val_loss: 0.3805 - val_out_loss: 0.0751 - val_level1_loss: 0.0748 - val_level2_loss: 0.0742 - val_level3_loss: 0.0746 - val_level4_loss: 0.0787 - val_level5_loss: 0.0981 - val_out_mean_squared_error: 0.0113 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0121 - val_level5_mean_squared_error: 0.0196\n",
            "Epoch 46/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3040 - out_loss: 0.0597 - level1_loss: 0.0593 - level2_loss: 0.0587 - level3_loss: 0.0591 - level4_loss: 0.0625 - level5_loss: 0.0808 - out_mean_squared_error: 0.0076 - level1_mean_squared_error: 0.0073 - level2_mean_squared_error: 0.0070 - level3_mean_squared_error: 0.0070 - level4_mean_squared_error: 0.0077 - level5_mean_squared_error: 0.0267\n",
            "Epoch 00046: val_loss improved from 0.37776 to 0.37274, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 449ms/step - loss: 0.3040 - out_loss: 0.0597 - level1_loss: 0.0593 - level2_loss: 0.0587 - level3_loss: 0.0591 - level4_loss: 0.0625 - level5_loss: 0.0808 - out_mean_squared_error: 0.0076 - level1_mean_squared_error: 0.0073 - level2_mean_squared_error: 0.0070 - level3_mean_squared_error: 0.0070 - level4_mean_squared_error: 0.0077 - level5_mean_squared_error: 0.0267 - val_loss: 0.3727 - val_out_loss: 0.0746 - val_level1_loss: 0.0745 - val_level2_loss: 0.0744 - val_level3_loss: 0.0745 - val_level4_loss: 0.0783 - val_level5_loss: 0.0896 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0110 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0119 - val_level5_mean_squared_error: 0.0142\n",
            "Epoch 47/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2999 - out_loss: 0.0593 - level1_loss: 0.0590 - level2_loss: 0.0587 - level3_loss: 0.0589 - level4_loss: 0.0621 - level5_loss: 0.0769 - out_mean_squared_error: 0.0075 - level1_mean_squared_error: 0.0072 - level2_mean_squared_error: 0.0070 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0076 - level5_mean_squared_error: 0.0212\n",
            "Epoch 00047: val_loss did not improve from 0.37274\n",
            "10/10 [==============================] - 4s 379ms/step - loss: 0.2999 - out_loss: 0.0593 - level1_loss: 0.0590 - level2_loss: 0.0587 - level3_loss: 0.0589 - level4_loss: 0.0621 - level5_loss: 0.0769 - out_mean_squared_error: 0.0075 - level1_mean_squared_error: 0.0072 - level2_mean_squared_error: 0.0070 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0076 - level5_mean_squared_error: 0.0212 - val_loss: 0.3894 - val_out_loss: 0.0747 - val_level1_loss: 0.0744 - val_level2_loss: 0.0738 - val_level3_loss: 0.0742 - val_level4_loss: 0.0782 - val_level5_loss: 0.1114 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0119 - val_level5_mean_squared_error: 0.0623\n",
            "Epoch 48/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2997 - out_loss: 0.0592 - level1_loss: 0.0586 - level2_loss: 0.0580 - level3_loss: 0.0584 - level4_loss: 0.0622 - level5_loss: 0.0782 - out_mean_squared_error: 0.0074 - level1_mean_squared_error: 0.0072 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0076 - level5_mean_squared_error: 0.0238\n",
            "Epoch 00048: val_loss improved from 0.37274 to 0.37113, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 441ms/step - loss: 0.2997 - out_loss: 0.0592 - level1_loss: 0.0586 - level2_loss: 0.0580 - level3_loss: 0.0584 - level4_loss: 0.0622 - level5_loss: 0.0782 - out_mean_squared_error: 0.0074 - level1_mean_squared_error: 0.0072 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0076 - level5_mean_squared_error: 0.0238 - val_loss: 0.3711 - val_out_loss: 0.0746 - val_level1_loss: 0.0744 - val_level2_loss: 0.0739 - val_level3_loss: 0.0745 - val_level4_loss: 0.0780 - val_level5_loss: 0.0884 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0139\n",
            "Epoch 49/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2910 - out_loss: 0.0587 - level1_loss: 0.0584 - level2_loss: 0.0580 - level3_loss: 0.0586 - level4_loss: 0.0614 - level5_loss: 0.0686 - out_mean_squared_error: 0.0073 - level1_mean_squared_error: 0.0071 - level2_mean_squared_error: 0.0069 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0168\n",
            "Epoch 00049: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2910 - out_loss: 0.0587 - level1_loss: 0.0584 - level2_loss: 0.0580 - level3_loss: 0.0586 - level4_loss: 0.0614 - level5_loss: 0.0686 - out_mean_squared_error: 0.0073 - level1_mean_squared_error: 0.0071 - level2_mean_squared_error: 0.0069 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0168 - val_loss: 0.3854 - val_out_loss: 0.0749 - val_level1_loss: 0.0745 - val_level2_loss: 0.0739 - val_level3_loss: 0.0745 - val_level4_loss: 0.0781 - val_level5_loss: 0.1060 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0560\n",
            "Epoch 50/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3015 - out_loss: 0.0584 - level1_loss: 0.0579 - level2_loss: 0.0575 - level3_loss: 0.0581 - level4_loss: 0.0610 - level5_loss: 0.0840 - out_mean_squared_error: 0.0073 - level1_mean_squared_error: 0.0070 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0588\n",
            "Epoch 00050: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.3015 - out_loss: 0.0584 - level1_loss: 0.0579 - level2_loss: 0.0575 - level3_loss: 0.0581 - level4_loss: 0.0610 - level5_loss: 0.0840 - out_mean_squared_error: 0.0073 - level1_mean_squared_error: 0.0070 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0069 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0588 - val_loss: 0.3717 - val_out_loss: 0.0747 - val_level1_loss: 0.0746 - val_level2_loss: 0.0737 - val_level3_loss: 0.0741 - val_level4_loss: 0.0777 - val_level5_loss: 0.0899 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0158\n",
            "Epoch 51/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2880 - out_loss: 0.0581 - level1_loss: 0.0576 - level2_loss: 0.0573 - level3_loss: 0.0579 - level4_loss: 0.0614 - level5_loss: 0.0677 - out_mean_squared_error: 0.0072 - level1_mean_squared_error: 0.0069 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0075 - level5_mean_squared_error: 0.0139\n",
            "Epoch 00051: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2880 - out_loss: 0.0581 - level1_loss: 0.0576 - level2_loss: 0.0573 - level3_loss: 0.0579 - level4_loss: 0.0614 - level5_loss: 0.0677 - out_mean_squared_error: 0.0072 - level1_mean_squared_error: 0.0069 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0075 - level5_mean_squared_error: 0.0139 - val_loss: 0.3762 - val_out_loss: 0.0746 - val_level1_loss: 0.0743 - val_level2_loss: 0.0739 - val_level3_loss: 0.0742 - val_level4_loss: 0.0779 - val_level5_loss: 0.0953 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0252\n",
            "Epoch 52/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2930 - out_loss: 0.0581 - level1_loss: 0.0576 - level2_loss: 0.0572 - level3_loss: 0.0579 - level4_loss: 0.0608 - level5_loss: 0.0747 - out_mean_squared_error: 0.0072 - level1_mean_squared_error: 0.0069 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0067 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0238\n",
            "Epoch 00052: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2930 - out_loss: 0.0581 - level1_loss: 0.0576 - level2_loss: 0.0572 - level3_loss: 0.0579 - level4_loss: 0.0608 - level5_loss: 0.0747 - out_mean_squared_error: 0.0072 - level1_mean_squared_error: 0.0069 - level2_mean_squared_error: 0.0068 - level3_mean_squared_error: 0.0067 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0238 - val_loss: 0.3849 - val_out_loss: 0.0749 - val_level1_loss: 0.0746 - val_level2_loss: 0.0739 - val_level3_loss: 0.0742 - val_level4_loss: 0.0780 - val_level5_loss: 0.1056 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0509\n",
            "Epoch 53/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3098 - out_loss: 0.0579 - level1_loss: 0.0574 - level2_loss: 0.0569 - level3_loss: 0.0575 - level4_loss: 0.0603 - level5_loss: 0.0973 - out_mean_squared_error: 0.0072 - level1_mean_squared_error: 0.0069 - level2_mean_squared_error: 0.0067 - level3_mean_squared_error: 0.0067 - level4_mean_squared_error: 0.0071 - level5_mean_squared_error: 0.1307\n",
            "Epoch 00053: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.3098 - out_loss: 0.0579 - level1_loss: 0.0574 - level2_loss: 0.0569 - level3_loss: 0.0575 - level4_loss: 0.0603 - level5_loss: 0.0973 - out_mean_squared_error: 0.0072 - level1_mean_squared_error: 0.0069 - level2_mean_squared_error: 0.0067 - level3_mean_squared_error: 0.0067 - level4_mean_squared_error: 0.0071 - level5_mean_squared_error: 0.1307 - val_loss: 0.4330 - val_out_loss: 0.0749 - val_level1_loss: 0.0746 - val_level2_loss: 0.0739 - val_level3_loss: 0.0744 - val_level4_loss: 0.0785 - val_level5_loss: 0.1649 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0122 - val_level5_mean_squared_error: 0.5081\n",
            "Epoch 54/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3269 - out_loss: 0.0576 - level1_loss: 0.0571 - level2_loss: 0.0568 - level3_loss: 0.0572 - level4_loss: 0.0606 - level5_loss: 0.1194 - out_mean_squared_error: 0.0071 - level1_mean_squared_error: 0.0068 - level2_mean_squared_error: 0.0066 - level3_mean_squared_error: 0.0066 - level4_mean_squared_error: 0.0072 - level5_mean_squared_error: 0.2713\n",
            "Epoch 00054: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.3269 - out_loss: 0.0576 - level1_loss: 0.0571 - level2_loss: 0.0568 - level3_loss: 0.0572 - level4_loss: 0.0606 - level5_loss: 0.1194 - out_mean_squared_error: 0.0071 - level1_mean_squared_error: 0.0068 - level2_mean_squared_error: 0.0066 - level3_mean_squared_error: 0.0066 - level4_mean_squared_error: 0.0072 - level5_mean_squared_error: 0.2713 - val_loss: 0.4075 - val_out_loss: 0.0747 - val_level1_loss: 0.0745 - val_level2_loss: 0.0738 - val_level3_loss: 0.0748 - val_level4_loss: 0.0784 - val_level5_loss: 0.1332 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0110 - val_level4_mean_squared_error: 0.0119 - val_level5_mean_squared_error: 0.1435\n",
            "Epoch 55/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3181 - out_loss: 0.0575 - level1_loss: 0.0570 - level2_loss: 0.0564 - level3_loss: 0.0573 - level4_loss: 0.0606 - level5_loss: 0.1089 - out_mean_squared_error: 0.0071 - level1_mean_squared_error: 0.0068 - level2_mean_squared_error: 0.0066 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0072 - level5_mean_squared_error: 0.1607\n",
            "Epoch 00055: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.3181 - out_loss: 0.0575 - level1_loss: 0.0570 - level2_loss: 0.0564 - level3_loss: 0.0573 - level4_loss: 0.0606 - level5_loss: 0.1089 - out_mean_squared_error: 0.0071 - level1_mean_squared_error: 0.0068 - level2_mean_squared_error: 0.0066 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0072 - level5_mean_squared_error: 0.1607 - val_loss: 0.3916 - val_out_loss: 0.0752 - val_level1_loss: 0.0749 - val_level2_loss: 0.0746 - val_level3_loss: 0.0754 - val_level4_loss: 0.0788 - val_level5_loss: 0.1106 - val_out_mean_squared_error: 0.0113 - val_level1_mean_squared_error: 0.0111 - val_level2_mean_squared_error: 0.0110 - val_level3_mean_squared_error: 0.0112 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0506\n",
            "Epoch 56/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2983 - out_loss: 0.0572 - level1_loss: 0.0566 - level2_loss: 0.0567 - level3_loss: 0.0569 - level4_loss: 0.0606 - level5_loss: 0.0849 - out_mean_squared_error: 0.0070 - level1_mean_squared_error: 0.0067 - level2_mean_squared_error: 0.0067 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0074 - level5_mean_squared_error: 0.0480\n",
            "Epoch 00056: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2983 - out_loss: 0.0572 - level1_loss: 0.0566 - level2_loss: 0.0567 - level3_loss: 0.0569 - level4_loss: 0.0606 - level5_loss: 0.0849 - out_mean_squared_error: 0.0070 - level1_mean_squared_error: 0.0067 - level2_mean_squared_error: 0.0067 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0074 - level5_mean_squared_error: 0.0480 - val_loss: 0.3867 - val_out_loss: 0.0746 - val_level1_loss: 0.0744 - val_level2_loss: 0.0740 - val_level3_loss: 0.0744 - val_level4_loss: 0.0778 - val_level5_loss: 0.1083 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0109 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0535\n",
            "Epoch 57/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3006 - out_loss: 0.0569 - level1_loss: 0.0564 - level2_loss: 0.0560 - level3_loss: 0.0569 - level4_loss: 0.0598 - level5_loss: 0.0897 - out_mean_squared_error: 0.0070 - level1_mean_squared_error: 0.0067 - level2_mean_squared_error: 0.0065 - level3_mean_squared_error: 0.0066 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0743\n",
            "Epoch 00057: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 381ms/step - loss: 0.3006 - out_loss: 0.0569 - level1_loss: 0.0564 - level2_loss: 0.0560 - level3_loss: 0.0569 - level4_loss: 0.0598 - level5_loss: 0.0897 - out_mean_squared_error: 0.0070 - level1_mean_squared_error: 0.0067 - level2_mean_squared_error: 0.0065 - level3_mean_squared_error: 0.0066 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0743 - val_loss: 0.3718 - val_out_loss: 0.0744 - val_level1_loss: 0.0741 - val_level2_loss: 0.0738 - val_level3_loss: 0.0748 - val_level4_loss: 0.0770 - val_level5_loss: 0.0906 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0112 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0144\n",
            "Epoch 58/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2884 - out_loss: 0.0567 - level1_loss: 0.0561 - level2_loss: 0.0556 - level3_loss: 0.0569 - level4_loss: 0.0597 - level5_loss: 0.0756 - out_mean_squared_error: 0.0070 - level1_mean_squared_error: 0.0066 - level2_mean_squared_error: 0.0064 - level3_mean_squared_error: 0.0067 - level4_mean_squared_error: 0.0071 - level5_mean_squared_error: 0.0275\n",
            "Epoch 00058: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2884 - out_loss: 0.0567 - level1_loss: 0.0561 - level2_loss: 0.0556 - level3_loss: 0.0569 - level4_loss: 0.0597 - level5_loss: 0.0756 - out_mean_squared_error: 0.0070 - level1_mean_squared_error: 0.0066 - level2_mean_squared_error: 0.0064 - level3_mean_squared_error: 0.0067 - level4_mean_squared_error: 0.0071 - level5_mean_squared_error: 0.0275 - val_loss: 0.3721 - val_out_loss: 0.0746 - val_level1_loss: 0.0744 - val_level2_loss: 0.0738 - val_level3_loss: 0.0747 - val_level4_loss: 0.0780 - val_level5_loss: 0.0896 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0110 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0183\n",
            "Epoch 59/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2818 - out_loss: 0.0565 - level1_loss: 0.0559 - level2_loss: 0.0554 - level3_loss: 0.0562 - level4_loss: 0.0591 - level5_loss: 0.0691 - out_mean_squared_error: 0.0069 - level1_mean_squared_error: 0.0066 - level2_mean_squared_error: 0.0064 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0276\n",
            "Epoch 00059: val_loss did not improve from 0.37113\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2818 - out_loss: 0.0565 - level1_loss: 0.0559 - level2_loss: 0.0554 - level3_loss: 0.0562 - level4_loss: 0.0591 - level5_loss: 0.0691 - out_mean_squared_error: 0.0069 - level1_mean_squared_error: 0.0066 - level2_mean_squared_error: 0.0064 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0276 - val_loss: 0.3753 - val_out_loss: 0.0745 - val_level1_loss: 0.0743 - val_level2_loss: 0.0738 - val_level3_loss: 0.0740 - val_level4_loss: 0.0775 - val_level5_loss: 0.0951 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0344\n",
            "Epoch 60/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2857 - out_loss: 0.0562 - level1_loss: 0.0558 - level2_loss: 0.0553 - level3_loss: 0.0560 - level4_loss: 0.0589 - level5_loss: 0.0749 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0064 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0069 - level5_mean_squared_error: 0.0351\n",
            "Epoch 00060: val_loss improved from 0.37113 to 0.36837, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 447ms/step - loss: 0.2857 - out_loss: 0.0562 - level1_loss: 0.0558 - level2_loss: 0.0553 - level3_loss: 0.0560 - level4_loss: 0.0589 - level5_loss: 0.0749 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0064 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0069 - level5_mean_squared_error: 0.0351 - val_loss: 0.3684 - val_out_loss: 0.0744 - val_level1_loss: 0.0740 - val_level2_loss: 0.0737 - val_level3_loss: 0.0741 - val_level4_loss: 0.0776 - val_level5_loss: 0.0867 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0142\n",
            "Epoch 61/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2807 - out_loss: 0.0560 - level1_loss: 0.0554 - level2_loss: 0.0551 - level3_loss: 0.0562 - level4_loss: 0.0592 - level5_loss: 0.0690 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0063 - level3_mean_squared_error: 0.0066 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0123\n",
            "Epoch 00061: val_loss did not improve from 0.36837\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2807 - out_loss: 0.0560 - level1_loss: 0.0554 - level2_loss: 0.0551 - level3_loss: 0.0562 - level4_loss: 0.0592 - level5_loss: 0.0690 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0063 - level3_mean_squared_error: 0.0066 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0123 - val_loss: 0.3692 - val_out_loss: 0.0743 - val_level1_loss: 0.0741 - val_level2_loss: 0.0733 - val_level3_loss: 0.0741 - val_level4_loss: 0.0769 - val_level5_loss: 0.0888 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0142\n",
            "Epoch 62/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2780 - out_loss: 0.0560 - level1_loss: 0.0554 - level2_loss: 0.0550 - level3_loss: 0.0567 - level4_loss: 0.0592 - level5_loss: 0.0652 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0062 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0135\n",
            "Epoch 00062: val_loss improved from 0.36837 to 0.36642, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 448ms/step - loss: 0.2780 - out_loss: 0.0560 - level1_loss: 0.0554 - level2_loss: 0.0550 - level3_loss: 0.0567 - level4_loss: 0.0592 - level5_loss: 0.0652 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0062 - level3_mean_squared_error: 0.0068 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0135 - val_loss: 0.3664 - val_out_loss: 0.0745 - val_level1_loss: 0.0743 - val_level2_loss: 0.0737 - val_level3_loss: 0.0755 - val_level4_loss: 0.0777 - val_level5_loss: 0.0822 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0112 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0122\n",
            "Epoch 63/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2757 - out_loss: 0.0559 - level1_loss: 0.0553 - level2_loss: 0.0548 - level3_loss: 0.0561 - level4_loss: 0.0602 - level5_loss: 0.0624 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0062 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0074 - level5_mean_squared_error: 0.0142\n",
            "Epoch 00063: val_loss did not improve from 0.36642\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2757 - out_loss: 0.0559 - level1_loss: 0.0553 - level2_loss: 0.0548 - level3_loss: 0.0561 - level4_loss: 0.0602 - level5_loss: 0.0624 - out_mean_squared_error: 0.0068 - level1_mean_squared_error: 0.0065 - level2_mean_squared_error: 0.0062 - level3_mean_squared_error: 0.0065 - level4_mean_squared_error: 0.0074 - level5_mean_squared_error: 0.0142 - val_loss: 0.3712 - val_out_loss: 0.0744 - val_level1_loss: 0.0743 - val_level2_loss: 0.0736 - val_level3_loss: 0.0746 - val_level4_loss: 0.0797 - val_level5_loss: 0.0874 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0126 - val_level5_mean_squared_error: 0.0136\n",
            "Epoch 64/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2756 - out_loss: 0.0554 - level1_loss: 0.0548 - level2_loss: 0.0542 - level3_loss: 0.0550 - level4_loss: 0.0598 - level5_loss: 0.0654 - out_mean_squared_error: 0.0067 - level1_mean_squared_error: 0.0064 - level2_mean_squared_error: 0.0061 - level3_mean_squared_error: 0.0062 - level4_mean_squared_error: 0.0078 - level5_mean_squared_error: 0.0168\n",
            "Epoch 00064: val_loss did not improve from 0.36642\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2756 - out_loss: 0.0554 - level1_loss: 0.0548 - level2_loss: 0.0542 - level3_loss: 0.0550 - level4_loss: 0.0598 - level5_loss: 0.0654 - out_mean_squared_error: 0.0067 - level1_mean_squared_error: 0.0064 - level2_mean_squared_error: 0.0061 - level3_mean_squared_error: 0.0062 - level4_mean_squared_error: 0.0078 - level5_mean_squared_error: 0.0168 - val_loss: 0.3743 - val_out_loss: 0.0739 - val_level1_loss: 0.0736 - val_level2_loss: 0.0730 - val_level3_loss: 0.0737 - val_level4_loss: 0.0785 - val_level5_loss: 0.0952 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0124 - val_level5_mean_squared_error: 0.0355\n",
            "Epoch 65/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2741 - out_loss: 0.0553 - level1_loss: 0.0548 - level2_loss: 0.0542 - level3_loss: 0.0547 - level4_loss: 0.0585 - level5_loss: 0.0651 - out_mean_squared_error: 0.0067 - level1_mean_squared_error: 0.0063 - level2_mean_squared_error: 0.0061 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0069 - level5_mean_squared_error: 0.0192\n",
            "Epoch 00065: val_loss did not improve from 0.36642\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2741 - out_loss: 0.0553 - level1_loss: 0.0548 - level2_loss: 0.0542 - level3_loss: 0.0547 - level4_loss: 0.0585 - level5_loss: 0.0651 - out_mean_squared_error: 0.0067 - level1_mean_squared_error: 0.0063 - level2_mean_squared_error: 0.0061 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0069 - level5_mean_squared_error: 0.0192 - val_loss: 0.3687 - val_out_loss: 0.0750 - val_level1_loss: 0.0747 - val_level2_loss: 0.0741 - val_level3_loss: 0.0744 - val_level4_loss: 0.0777 - val_level5_loss: 0.0849 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0129\n",
            "Epoch 66/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2706 - out_loss: 0.0551 - level1_loss: 0.0546 - level2_loss: 0.0539 - level3_loss: 0.0545 - level4_loss: 0.0577 - level5_loss: 0.0624 - out_mean_squared_error: 0.0067 - level1_mean_squared_error: 0.0064 - level2_mean_squared_error: 0.0060 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0066 - level5_mean_squared_error: 0.0163\n",
            "Epoch 00066: val_loss did not improve from 0.36642\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2706 - out_loss: 0.0551 - level1_loss: 0.0546 - level2_loss: 0.0539 - level3_loss: 0.0545 - level4_loss: 0.0577 - level5_loss: 0.0624 - out_mean_squared_error: 0.0067 - level1_mean_squared_error: 0.0064 - level2_mean_squared_error: 0.0060 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0066 - level5_mean_squared_error: 0.0163 - val_loss: 0.3683 - val_out_loss: 0.0742 - val_level1_loss: 0.0740 - val_level2_loss: 0.0733 - val_level3_loss: 0.0738 - val_level4_loss: 0.0772 - val_level5_loss: 0.0879 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0155\n",
            "Epoch 67/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2783 - out_loss: 0.0548 - level1_loss: 0.0543 - level2_loss: 0.0538 - level3_loss: 0.0545 - level4_loss: 0.0584 - level5_loss: 0.0721 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0060 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0369\n",
            "Epoch 00067: val_loss did not improve from 0.36642\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.2783 - out_loss: 0.0548 - level1_loss: 0.0543 - level2_loss: 0.0538 - level3_loss: 0.0545 - level4_loss: 0.0584 - level5_loss: 0.0721 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0060 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0369 - val_loss: 0.3734 - val_out_loss: 0.0744 - val_level1_loss: 0.0741 - val_level2_loss: 0.0739 - val_level3_loss: 0.0743 - val_level4_loss: 0.0769 - val_level5_loss: 0.0932 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0108 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0248\n",
            "Epoch 68/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2793 - out_loss: 0.0546 - level1_loss: 0.0540 - level2_loss: 0.0534 - level3_loss: 0.0540 - level4_loss: 0.0587 - level5_loss: 0.0744 - out_mean_squared_error: 0.0066 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0060 - level3_mean_squared_error: 0.0060 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0372\n",
            "Epoch 00068: val_loss did not improve from 0.36642\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2793 - out_loss: 0.0546 - level1_loss: 0.0540 - level2_loss: 0.0534 - level3_loss: 0.0540 - level4_loss: 0.0587 - level5_loss: 0.0744 - out_mean_squared_error: 0.0066 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0060 - level3_mean_squared_error: 0.0060 - level4_mean_squared_error: 0.0073 - level5_mean_squared_error: 0.0372 - val_loss: 0.3736 - val_out_loss: 0.0741 - val_level1_loss: 0.0739 - val_level2_loss: 0.0732 - val_level3_loss: 0.0737 - val_level4_loss: 0.0777 - val_level5_loss: 0.0943 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0176\n",
            "Epoch 69/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2750 - out_loss: 0.0544 - level1_loss: 0.0539 - level2_loss: 0.0532 - level3_loss: 0.0536 - level4_loss: 0.0575 - level5_loss: 0.0711 - out_mean_squared_error: 0.0066 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0069 - level5_mean_squared_error: 0.0141\n",
            "Epoch 00069: val_loss did not improve from 0.36642\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2750 - out_loss: 0.0544 - level1_loss: 0.0539 - level2_loss: 0.0532 - level3_loss: 0.0536 - level4_loss: 0.0575 - level5_loss: 0.0711 - out_mean_squared_error: 0.0066 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0069 - level5_mean_squared_error: 0.0141 - val_loss: 0.3731 - val_out_loss: 0.0742 - val_level1_loss: 0.0742 - val_level2_loss: 0.0734 - val_level3_loss: 0.0738 - val_level4_loss: 0.0779 - val_level5_loss: 0.0927 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0110 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0121 - val_level5_mean_squared_error: 0.0174\n",
            "Epoch 70/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2744 - out_loss: 0.0542 - level1_loss: 0.0536 - level2_loss: 0.0530 - level3_loss: 0.0535 - level4_loss: 0.0572 - level5_loss: 0.0714 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0066 - level5_mean_squared_error: 0.0184\n",
            "Epoch 00070: val_loss improved from 0.36642 to 0.36628, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 451ms/step - loss: 0.2744 - out_loss: 0.0542 - level1_loss: 0.0536 - level2_loss: 0.0530 - level3_loss: 0.0535 - level4_loss: 0.0572 - level5_loss: 0.0714 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0062 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0066 - level5_mean_squared_error: 0.0184 - val_loss: 0.3663 - val_out_loss: 0.0743 - val_level1_loss: 0.0740 - val_level2_loss: 0.0733 - val_level3_loss: 0.0739 - val_level4_loss: 0.0767 - val_level5_loss: 0.0856 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0139\n",
            "Epoch 71/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2769 - out_loss: 0.0541 - level1_loss: 0.0536 - level2_loss: 0.0530 - level3_loss: 0.0536 - level4_loss: 0.0568 - level5_loss: 0.0751 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0065 - level5_mean_squared_error: 0.0385\n",
            "Epoch 00071: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2769 - out_loss: 0.0541 - level1_loss: 0.0536 - level2_loss: 0.0530 - level3_loss: 0.0536 - level4_loss: 0.0568 - level5_loss: 0.0751 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0065 - level5_mean_squared_error: 0.0385 - val_loss: 0.3744 - val_out_loss: 0.0743 - val_level1_loss: 0.0740 - val_level2_loss: 0.0734 - val_level3_loss: 0.0738 - val_level4_loss: 0.0780 - val_level5_loss: 0.0946 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0229\n",
            "Epoch 72/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2757 - out_loss: 0.0543 - level1_loss: 0.0537 - level2_loss: 0.0531 - level3_loss: 0.0542 - level4_loss: 0.0587 - level5_loss: 0.0707 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0062 - level4_mean_squared_error: 0.0075 - level5_mean_squared_error: 0.0307\n",
            "Epoch 00072: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2757 - out_loss: 0.0543 - level1_loss: 0.0537 - level2_loss: 0.0531 - level3_loss: 0.0542 - level4_loss: 0.0587 - level5_loss: 0.0707 - out_mean_squared_error: 0.0065 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0062 - level4_mean_squared_error: 0.0075 - level5_mean_squared_error: 0.0307 - val_loss: 0.3717 - val_out_loss: 0.0746 - val_level1_loss: 0.0742 - val_level2_loss: 0.0735 - val_level3_loss: 0.0740 - val_level4_loss: 0.0789 - val_level5_loss: 0.0894 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0121 - val_level5_mean_squared_error: 0.0167\n",
            "Epoch 73/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2713 - out_loss: 0.0540 - level1_loss: 0.0535 - level2_loss: 0.0528 - level3_loss: 0.0534 - level4_loss: 0.0574 - level5_loss: 0.0679 - out_mean_squared_error: 0.0064 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0058 - level3_mean_squared_error: 0.0058 - level4_mean_squared_error: 0.0067 - level5_mean_squared_error: 0.0299\n",
            "Epoch 00073: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2713 - out_loss: 0.0540 - level1_loss: 0.0535 - level2_loss: 0.0528 - level3_loss: 0.0534 - level4_loss: 0.0574 - level5_loss: 0.0679 - out_mean_squared_error: 0.0064 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0058 - level3_mean_squared_error: 0.0058 - level4_mean_squared_error: 0.0067 - level5_mean_squared_error: 0.0299 - val_loss: 0.3667 - val_out_loss: 0.0746 - val_level1_loss: 0.0746 - val_level2_loss: 0.0734 - val_level3_loss: 0.0740 - val_level4_loss: 0.0773 - val_level5_loss: 0.0845 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0111 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0131\n",
            "Epoch 74/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2809 - out_loss: 0.0539 - level1_loss: 0.0533 - level2_loss: 0.0529 - level3_loss: 0.0535 - level4_loss: 0.0585 - level5_loss: 0.0792 - out_mean_squared_error: 0.0064 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0074 - level5_mean_squared_error: 0.0619\n",
            "Epoch 00074: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2809 - out_loss: 0.0539 - level1_loss: 0.0533 - level2_loss: 0.0529 - level3_loss: 0.0535 - level4_loss: 0.0585 - level5_loss: 0.0792 - out_mean_squared_error: 0.0064 - level1_mean_squared_error: 0.0061 - level2_mean_squared_error: 0.0059 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0074 - level5_mean_squared_error: 0.0619 - val_loss: 0.3849 - val_out_loss: 0.0744 - val_level1_loss: 0.0740 - val_level2_loss: 0.0734 - val_level3_loss: 0.0739 - val_level4_loss: 0.0789 - val_level5_loss: 0.1066 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0122 - val_level5_mean_squared_error: 0.0593\n",
            "Epoch 75/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2841 - out_loss: 0.0535 - level1_loss: 0.0529 - level2_loss: 0.0523 - level3_loss: 0.0531 - level4_loss: 0.0567 - level5_loss: 0.0867 - out_mean_squared_error: 0.0064 - level1_mean_squared_error: 0.0060 - level2_mean_squared_error: 0.0058 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0067 - level5_mean_squared_error: 0.0883\n",
            "Epoch 00075: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 381ms/step - loss: 0.2841 - out_loss: 0.0535 - level1_loss: 0.0529 - level2_loss: 0.0523 - level3_loss: 0.0531 - level4_loss: 0.0567 - level5_loss: 0.0867 - out_mean_squared_error: 0.0064 - level1_mean_squared_error: 0.0060 - level2_mean_squared_error: 0.0058 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0067 - level5_mean_squared_error: 0.0883 - val_loss: 0.3836 - val_out_loss: 0.0741 - val_level1_loss: 0.0738 - val_level2_loss: 0.0731 - val_level3_loss: 0.0738 - val_level4_loss: 0.0768 - val_level5_loss: 0.1081 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0541\n",
            "Epoch 76/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2830 - out_loss: 0.0533 - level1_loss: 0.0526 - level2_loss: 0.0520 - level3_loss: 0.0531 - level4_loss: 0.0560 - level5_loss: 0.0867 - out_mean_squared_error: 0.0063 - level1_mean_squared_error: 0.0060 - level2_mean_squared_error: 0.0057 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0721\n",
            "Epoch 00076: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2830 - out_loss: 0.0533 - level1_loss: 0.0526 - level2_loss: 0.0520 - level3_loss: 0.0531 - level4_loss: 0.0560 - level5_loss: 0.0867 - out_mean_squared_error: 0.0063 - level1_mean_squared_error: 0.0060 - level2_mean_squared_error: 0.0057 - level3_mean_squared_error: 0.0059 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0721 - val_loss: 0.3697 - val_out_loss: 0.0742 - val_level1_loss: 0.0740 - val_level2_loss: 0.0732 - val_level3_loss: 0.0737 - val_level4_loss: 0.0767 - val_level5_loss: 0.0903 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0149\n",
            "Epoch 77/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2801 - out_loss: 0.0530 - level1_loss: 0.0523 - level2_loss: 0.0518 - level3_loss: 0.0530 - level4_loss: 0.0560 - level5_loss: 0.0840 - out_mean_squared_error: 0.0063 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0057 - level3_mean_squared_error: 0.0060 - level4_mean_squared_error: 0.0065 - level5_mean_squared_error: 0.0991\n",
            "Epoch 00077: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2801 - out_loss: 0.0530 - level1_loss: 0.0523 - level2_loss: 0.0518 - level3_loss: 0.0530 - level4_loss: 0.0560 - level5_loss: 0.0840 - out_mean_squared_error: 0.0063 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0057 - level3_mean_squared_error: 0.0060 - level4_mean_squared_error: 0.0065 - level5_mean_squared_error: 0.0991 - val_loss: 0.3680 - val_out_loss: 0.0740 - val_level1_loss: 0.0738 - val_level2_loss: 0.0732 - val_level3_loss: 0.0734 - val_level4_loss: 0.0764 - val_level5_loss: 0.0891 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0177\n",
            "Epoch 78/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2648 - out_loss: 0.0529 - level1_loss: 0.0522 - level2_loss: 0.0516 - level3_loss: 0.0523 - level4_loss: 0.0556 - level5_loss: 0.0664 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0063 - level5_mean_squared_error: 0.0243\n",
            "Epoch 00078: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.2648 - out_loss: 0.0529 - level1_loss: 0.0522 - level2_loss: 0.0516 - level3_loss: 0.0523 - level4_loss: 0.0556 - level5_loss: 0.0664 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0063 - level5_mean_squared_error: 0.0243 - val_loss: 0.3811 - val_out_loss: 0.0741 - val_level1_loss: 0.0739 - val_level2_loss: 0.0734 - val_level3_loss: 0.0739 - val_level4_loss: 0.0766 - val_level5_loss: 0.1045 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0588\n",
            "Epoch 79/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2725 - out_loss: 0.0527 - level1_loss: 0.0520 - level2_loss: 0.0514 - level3_loss: 0.0523 - level4_loss: 0.0565 - level5_loss: 0.0758 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0064 - level5_mean_squared_error: 0.0583\n",
            "Epoch 00079: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2725 - out_loss: 0.0527 - level1_loss: 0.0520 - level2_loss: 0.0514 - level3_loss: 0.0523 - level4_loss: 0.0565 - level5_loss: 0.0758 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0064 - level5_mean_squared_error: 0.0583 - val_loss: 0.3754 - val_out_loss: 0.0747 - val_level1_loss: 0.0743 - val_level2_loss: 0.0737 - val_level3_loss: 0.0744 - val_level4_loss: 0.0783 - val_level5_loss: 0.0939 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0122 - val_level5_mean_squared_error: 0.0263\n",
            "Epoch 80/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2604 - out_loss: 0.0526 - level1_loss: 0.0520 - level2_loss: 0.0515 - level3_loss: 0.0518 - level4_loss: 0.0560 - level5_loss: 0.0615 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0063 - level5_mean_squared_error: 0.0133\n",
            "Epoch 00080: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2604 - out_loss: 0.0526 - level1_loss: 0.0520 - level2_loss: 0.0515 - level3_loss: 0.0518 - level4_loss: 0.0560 - level5_loss: 0.0615 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0063 - level5_mean_squared_error: 0.0133 - val_loss: 0.3677 - val_out_loss: 0.0743 - val_level1_loss: 0.0739 - val_level2_loss: 0.0733 - val_level3_loss: 0.0739 - val_level4_loss: 0.0790 - val_level5_loss: 0.0851 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0119 - val_level5_mean_squared_error: 0.0147\n",
            "Epoch 81/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2632 - out_loss: 0.0525 - level1_loss: 0.0518 - level2_loss: 0.0513 - level3_loss: 0.0526 - level4_loss: 0.0581 - level5_loss: 0.0627 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0058 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0089 - level5_mean_squared_error: 0.0210\n",
            "Epoch 00081: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2632 - out_loss: 0.0525 - level1_loss: 0.0518 - level2_loss: 0.0513 - level3_loss: 0.0526 - level4_loss: 0.0581 - level5_loss: 0.0627 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0058 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0061 - level4_mean_squared_error: 0.0089 - level5_mean_squared_error: 0.0210 - val_loss: 0.3702 - val_out_loss: 0.0739 - val_level1_loss: 0.0734 - val_level2_loss: 0.0728 - val_level3_loss: 0.0737 - val_level4_loss: 0.0775 - val_level5_loss: 0.0914 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0287\n",
            "Epoch 82/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2566 - out_loss: 0.0525 - level1_loss: 0.0520 - level2_loss: 0.0513 - level3_loss: 0.0517 - level4_loss: 0.0562 - level5_loss: 0.0570 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0065 - level5_mean_squared_error: 0.0114\n",
            "Epoch 00082: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2566 - out_loss: 0.0525 - level1_loss: 0.0520 - level2_loss: 0.0513 - level3_loss: 0.0517 - level4_loss: 0.0562 - level5_loss: 0.0570 - out_mean_squared_error: 0.0062 - level1_mean_squared_error: 0.0059 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0065 - level5_mean_squared_error: 0.0114 - val_loss: 0.3685 - val_out_loss: 0.0738 - val_level1_loss: 0.0739 - val_level2_loss: 0.0730 - val_level3_loss: 0.0733 - val_level4_loss: 0.0764 - val_level5_loss: 0.0902 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0197\n",
            "Epoch 83/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2678 - out_loss: 0.0522 - level1_loss: 0.0514 - level2_loss: 0.0510 - level3_loss: 0.0519 - level4_loss: 0.0560 - level5_loss: 0.0722 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0058 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0058 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0581\n",
            "Epoch 00083: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2678 - out_loss: 0.0522 - level1_loss: 0.0514 - level2_loss: 0.0510 - level3_loss: 0.0519 - level4_loss: 0.0560 - level5_loss: 0.0722 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0058 - level2_mean_squared_error: 0.0056 - level3_mean_squared_error: 0.0058 - level4_mean_squared_error: 0.0070 - level5_mean_squared_error: 0.0581 - val_loss: 0.3754 - val_out_loss: 0.0741 - val_level1_loss: 0.0737 - val_level2_loss: 0.0732 - val_level3_loss: 0.0735 - val_level4_loss: 0.0770 - val_level5_loss: 0.0977 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0420\n",
            "Epoch 84/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2656 - out_loss: 0.0518 - level1_loss: 0.0511 - level2_loss: 0.0505 - level3_loss: 0.0514 - level4_loss: 0.0551 - level5_loss: 0.0722 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0057 - level2_mean_squared_error: 0.0055 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0063 - level5_mean_squared_error: 0.0439\n",
            "Epoch 00084: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2656 - out_loss: 0.0518 - level1_loss: 0.0511 - level2_loss: 0.0505 - level3_loss: 0.0514 - level4_loss: 0.0551 - level5_loss: 0.0722 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0057 - level2_mean_squared_error: 0.0055 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0063 - level5_mean_squared_error: 0.0439 - val_loss: 0.3716 - val_out_loss: 0.0738 - val_level1_loss: 0.0736 - val_level2_loss: 0.0731 - val_level3_loss: 0.0733 - val_level4_loss: 0.0769 - val_level5_loss: 0.0938 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0279\n",
            "Epoch 85/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2649 - out_loss: 0.0518 - level1_loss: 0.0512 - level2_loss: 0.0506 - level3_loss: 0.0511 - level4_loss: 0.0564 - level5_loss: 0.0700 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0057 - level2_mean_squared_error: 0.0054 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0071 - level5_mean_squared_error: 0.0486\n",
            "Epoch 00085: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2649 - out_loss: 0.0518 - level1_loss: 0.0512 - level2_loss: 0.0506 - level3_loss: 0.0511 - level4_loss: 0.0564 - level5_loss: 0.0700 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0057 - level2_mean_squared_error: 0.0054 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0071 - level5_mean_squared_error: 0.0486 - val_loss: 0.3738 - val_out_loss: 0.0739 - val_level1_loss: 0.0737 - val_level2_loss: 0.0728 - val_level3_loss: 0.0734 - val_level4_loss: 0.0768 - val_level5_loss: 0.0967 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0238\n",
            "Epoch 86/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2662 - out_loss: 0.0516 - level1_loss: 0.0511 - level2_loss: 0.0505 - level3_loss: 0.0512 - level4_loss: 0.0545 - level5_loss: 0.0739 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0058 - level2_mean_squared_error: 0.0055 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0336\n",
            "Epoch 00086: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2662 - out_loss: 0.0516 - level1_loss: 0.0511 - level2_loss: 0.0505 - level3_loss: 0.0512 - level4_loss: 0.0545 - level5_loss: 0.0739 - out_mean_squared_error: 0.0061 - level1_mean_squared_error: 0.0058 - level2_mean_squared_error: 0.0055 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0336 - val_loss: 0.3701 - val_out_loss: 0.0742 - val_level1_loss: 0.0738 - val_level2_loss: 0.0733 - val_level3_loss: 0.0742 - val_level4_loss: 0.0773 - val_level5_loss: 0.0898 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0148\n",
            "Epoch 87/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2632 - out_loss: 0.0513 - level1_loss: 0.0507 - level2_loss: 0.0503 - level3_loss: 0.0512 - level4_loss: 0.0541 - level5_loss: 0.0713 - out_mean_squared_error: 0.0060 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0054 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0286\n",
            "Epoch 00087: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2632 - out_loss: 0.0513 - level1_loss: 0.0507 - level2_loss: 0.0503 - level3_loss: 0.0512 - level4_loss: 0.0541 - level5_loss: 0.0713 - out_mean_squared_error: 0.0060 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0054 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0286 - val_loss: 0.3731 - val_out_loss: 0.0744 - val_level1_loss: 0.0740 - val_level2_loss: 0.0732 - val_level3_loss: 0.0740 - val_level4_loss: 0.0772 - val_level5_loss: 0.0936 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0156\n",
            "Epoch 88/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2562 - out_loss: 0.0514 - level1_loss: 0.0508 - level2_loss: 0.0501 - level3_loss: 0.0512 - level4_loss: 0.0546 - level5_loss: 0.0621 - out_mean_squared_error: 0.0060 - level1_mean_squared_error: 0.0057 - level2_mean_squared_error: 0.0054 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0061 - level5_mean_squared_error: 0.0104\n",
            "Epoch 00088: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2562 - out_loss: 0.0514 - level1_loss: 0.0508 - level2_loss: 0.0501 - level3_loss: 0.0512 - level4_loss: 0.0546 - level5_loss: 0.0621 - out_mean_squared_error: 0.0060 - level1_mean_squared_error: 0.0057 - level2_mean_squared_error: 0.0054 - level3_mean_squared_error: 0.0056 - level4_mean_squared_error: 0.0061 - level5_mean_squared_error: 0.0104 - val_loss: 0.3670 - val_out_loss: 0.0740 - val_level1_loss: 0.0739 - val_level2_loss: 0.0730 - val_level3_loss: 0.0737 - val_level4_loss: 0.0767 - val_level5_loss: 0.0876 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0133\n",
            "Epoch 89/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2575 - out_loss: 0.0511 - level1_loss: 0.0504 - level2_loss: 0.0498 - level3_loss: 0.0508 - level4_loss: 0.0541 - level5_loss: 0.0657 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0118\n",
            "Epoch 00089: val_loss did not improve from 0.36628\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2575 - out_loss: 0.0511 - level1_loss: 0.0504 - level2_loss: 0.0498 - level3_loss: 0.0508 - level4_loss: 0.0541 - level5_loss: 0.0657 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0118 - val_loss: 0.3685 - val_out_loss: 0.0740 - val_level1_loss: 0.0737 - val_level2_loss: 0.0729 - val_level3_loss: 0.0736 - val_level4_loss: 0.0775 - val_level5_loss: 0.0890 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0177\n",
            "Epoch 90/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2558 - out_loss: 0.0510 - level1_loss: 0.0503 - level2_loss: 0.0496 - level3_loss: 0.0508 - level4_loss: 0.0545 - level5_loss: 0.0636 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0150\n",
            "Epoch 00090: val_loss improved from 0.36628 to 0.36328, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 446ms/step - loss: 0.2558 - out_loss: 0.0510 - level1_loss: 0.0503 - level2_loss: 0.0496 - level3_loss: 0.0508 - level4_loss: 0.0545 - level5_loss: 0.0636 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0150 - val_loss: 0.3633 - val_out_loss: 0.0742 - val_level1_loss: 0.0741 - val_level2_loss: 0.0734 - val_level3_loss: 0.0742 - val_level4_loss: 0.0771 - val_level5_loss: 0.0811 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0119\n",
            "Epoch 91/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2579 - out_loss: 0.0508 - level1_loss: 0.0502 - level2_loss: 0.0496 - level3_loss: 0.0504 - level4_loss: 0.0537 - level5_loss: 0.0677 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0383\n",
            "Epoch 00091: val_loss did not improve from 0.36328\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2579 - out_loss: 0.0508 - level1_loss: 0.0502 - level2_loss: 0.0496 - level3_loss: 0.0504 - level4_loss: 0.0537 - level5_loss: 0.0677 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0056 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0383 - val_loss: 0.3671 - val_out_loss: 0.0743 - val_level1_loss: 0.0739 - val_level2_loss: 0.0733 - val_level3_loss: 0.0737 - val_level4_loss: 0.0767 - val_level5_loss: 0.0869 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0136\n",
            "Epoch 92/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2639 - out_loss: 0.0507 - level1_loss: 0.0501 - level2_loss: 0.0496 - level3_loss: 0.0509 - level4_loss: 0.0540 - level5_loss: 0.0746 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0675\n",
            "Epoch 00092: val_loss did not improve from 0.36328\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2639 - out_loss: 0.0507 - level1_loss: 0.0501 - level2_loss: 0.0496 - level3_loss: 0.0509 - level4_loss: 0.0540 - level5_loss: 0.0746 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0055 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0675 - val_loss: 0.3909 - val_out_loss: 0.0738 - val_level1_loss: 0.0735 - val_level2_loss: 0.0729 - val_level3_loss: 0.0739 - val_level4_loss: 0.0778 - val_level5_loss: 0.1167 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0124 - val_level5_mean_squared_error: 0.0862\n",
            "Epoch 93/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2724 - out_loss: 0.0509 - level1_loss: 0.0502 - level2_loss: 0.0497 - level3_loss: 0.0506 - level4_loss: 0.0549 - level5_loss: 0.0842 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0068 - level5_mean_squared_error: 0.0882\n",
            "Epoch 00093: val_loss did not improve from 0.36328\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2724 - out_loss: 0.0509 - level1_loss: 0.0502 - level2_loss: 0.0497 - level3_loss: 0.0506 - level4_loss: 0.0549 - level5_loss: 0.0842 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0068 - level5_mean_squared_error: 0.0882 - val_loss: 0.3819 - val_out_loss: 0.0749 - val_level1_loss: 0.0746 - val_level2_loss: 0.0740 - val_level3_loss: 0.0743 - val_level4_loss: 0.0776 - val_level5_loss: 0.1020 - val_out_mean_squared_error: 0.0112 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0436\n",
            "Epoch 94/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2514 - out_loss: 0.0509 - level1_loss: 0.0502 - level2_loss: 0.0497 - level3_loss: 0.0503 - level4_loss: 0.0540 - level5_loss: 0.0592 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0147\n",
            "Epoch 00094: val_loss improved from 0.36328 to 0.36317, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 5s 473ms/step - loss: 0.2514 - out_loss: 0.0509 - level1_loss: 0.0502 - level2_loss: 0.0497 - level3_loss: 0.0503 - level4_loss: 0.0540 - level5_loss: 0.0592 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0147 - val_loss: 0.3632 - val_out_loss: 0.0738 - val_level1_loss: 0.0735 - val_level2_loss: 0.0728 - val_level3_loss: 0.0735 - val_level4_loss: 0.0769 - val_level5_loss: 0.0835 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0128\n",
            "Epoch 95/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2497 - out_loss: 0.0506 - level1_loss: 0.0500 - level2_loss: 0.0495 - level3_loss: 0.0502 - level4_loss: 0.0543 - level5_loss: 0.0576 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0053 - level4_mean_squared_error: 0.0061 - level5_mean_squared_error: 0.0087\n",
            "Epoch 00095: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2497 - out_loss: 0.0506 - level1_loss: 0.0500 - level2_loss: 0.0495 - level3_loss: 0.0502 - level4_loss: 0.0543 - level5_loss: 0.0576 - out_mean_squared_error: 0.0059 - level1_mean_squared_error: 0.0055 - level2_mean_squared_error: 0.0053 - level3_mean_squared_error: 0.0053 - level4_mean_squared_error: 0.0061 - level5_mean_squared_error: 0.0087 - val_loss: 0.3689 - val_out_loss: 0.0741 - val_level1_loss: 0.0737 - val_level2_loss: 0.0730 - val_level3_loss: 0.0743 - val_level4_loss: 0.0783 - val_level5_loss: 0.0878 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0202\n",
            "Epoch 96/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2511 - out_loss: 0.0503 - level1_loss: 0.0496 - level2_loss: 0.0490 - level3_loss: 0.0498 - level4_loss: 0.0544 - level5_loss: 0.0608 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0052 - level3_mean_squared_error: 0.0052 - level4_mean_squared_error: 0.0064 - level5_mean_squared_error: 0.0159\n",
            "Epoch 00096: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2511 - out_loss: 0.0503 - level1_loss: 0.0496 - level2_loss: 0.0490 - level3_loss: 0.0498 - level4_loss: 0.0544 - level5_loss: 0.0608 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0052 - level3_mean_squared_error: 0.0052 - level4_mean_squared_error: 0.0064 - level5_mean_squared_error: 0.0159 - val_loss: 0.3670 - val_out_loss: 0.0744 - val_level1_loss: 0.0742 - val_level2_loss: 0.0734 - val_level3_loss: 0.0744 - val_level4_loss: 0.0779 - val_level5_loss: 0.0844 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0109 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0121 - val_level5_mean_squared_error: 0.0127\n",
            "Epoch 97/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2550 - out_loss: 0.0501 - level1_loss: 0.0494 - level2_loss: 0.0487 - level3_loss: 0.0494 - level4_loss: 0.0535 - level5_loss: 0.0676 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0051 - level4_mean_squared_error: 0.0058 - level5_mean_squared_error: 0.0283\n",
            "Epoch 00097: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2550 - out_loss: 0.0501 - level1_loss: 0.0494 - level2_loss: 0.0487 - level3_loss: 0.0494 - level4_loss: 0.0535 - level5_loss: 0.0676 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0051 - level4_mean_squared_error: 0.0058 - level5_mean_squared_error: 0.0283 - val_loss: 0.3762 - val_out_loss: 0.0741 - val_level1_loss: 0.0738 - val_level2_loss: 0.0732 - val_level3_loss: 0.0740 - val_level4_loss: 0.0778 - val_level5_loss: 0.0975 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0122 - val_level5_mean_squared_error: 0.0216\n",
            "Epoch 98/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2521 - out_loss: 0.0501 - level1_loss: 0.0496 - level2_loss: 0.0488 - level3_loss: 0.0499 - level4_loss: 0.0535 - level5_loss: 0.0632 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0112\n",
            "Epoch 00098: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2521 - out_loss: 0.0501 - level1_loss: 0.0496 - level2_loss: 0.0488 - level3_loss: 0.0499 - level4_loss: 0.0535 - level5_loss: 0.0632 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0062 - level5_mean_squared_error: 0.0112 - val_loss: 0.3701 - val_out_loss: 0.0741 - val_level1_loss: 0.0738 - val_level2_loss: 0.0734 - val_level3_loss: 0.0744 - val_level4_loss: 0.0771 - val_level5_loss: 0.0898 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0109 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0180\n",
            "Epoch 99/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2504 - out_loss: 0.0499 - level1_loss: 0.0492 - level2_loss: 0.0488 - level3_loss: 0.0498 - level4_loss: 0.0531 - level5_loss: 0.0622 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0052 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0058 - level5_mean_squared_error: 0.0229\n",
            "Epoch 00099: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2504 - out_loss: 0.0499 - level1_loss: 0.0492 - level2_loss: 0.0488 - level3_loss: 0.0498 - level4_loss: 0.0531 - level5_loss: 0.0622 - out_mean_squared_error: 0.0058 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0052 - level3_mean_squared_error: 0.0054 - level4_mean_squared_error: 0.0058 - level5_mean_squared_error: 0.0229 - val_loss: 0.3691 - val_out_loss: 0.0739 - val_level1_loss: 0.0736 - val_level2_loss: 0.0731 - val_level3_loss: 0.0740 - val_level4_loss: 0.0772 - val_level5_loss: 0.0896 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0183\n",
            "Epoch 100/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2455 - out_loss: 0.0497 - level1_loss: 0.0491 - level2_loss: 0.0486 - level3_loss: 0.0492 - level4_loss: 0.0525 - level5_loss: 0.0578 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0052 - level3_mean_squared_error: 0.0052 - level4_mean_squared_error: 0.0057 - level5_mean_squared_error: 0.0165\n",
            "Epoch 00100: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2455 - out_loss: 0.0497 - level1_loss: 0.0491 - level2_loss: 0.0486 - level3_loss: 0.0492 - level4_loss: 0.0525 - level5_loss: 0.0578 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0054 - level2_mean_squared_error: 0.0052 - level3_mean_squared_error: 0.0052 - level4_mean_squared_error: 0.0057 - level5_mean_squared_error: 0.0165 - val_loss: 0.3697 - val_out_loss: 0.0739 - val_level1_loss: 0.0734 - val_level2_loss: 0.0728 - val_level3_loss: 0.0736 - val_level4_loss: 0.0766 - val_level5_loss: 0.0918 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0197\n",
            "Epoch 101/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2487 - out_loss: 0.0494 - level1_loss: 0.0488 - level2_loss: 0.0483 - level3_loss: 0.0488 - level4_loss: 0.0522 - level5_loss: 0.0634 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0053 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0265\n",
            "Epoch 00101: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2487 - out_loss: 0.0494 - level1_loss: 0.0488 - level2_loss: 0.0483 - level3_loss: 0.0488 - level4_loss: 0.0522 - level5_loss: 0.0634 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0053 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0265 - val_loss: 0.3819 - val_out_loss: 0.0739 - val_level1_loss: 0.0738 - val_level2_loss: 0.0730 - val_level3_loss: 0.0733 - val_level4_loss: 0.0765 - val_level5_loss: 0.1068 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0632\n",
            "Epoch 102/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2572 - out_loss: 0.0495 - level1_loss: 0.0488 - level2_loss: 0.0484 - level3_loss: 0.0488 - level4_loss: 0.0523 - level5_loss: 0.0737 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0053 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0057 - level5_mean_squared_error: 0.0447\n",
            "Epoch 00102: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2572 - out_loss: 0.0495 - level1_loss: 0.0488 - level2_loss: 0.0484 - level3_loss: 0.0488 - level4_loss: 0.0523 - level5_loss: 0.0737 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0053 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0057 - level5_mean_squared_error: 0.0447 - val_loss: 0.3682 - val_out_loss: 0.0735 - val_level1_loss: 0.0732 - val_level2_loss: 0.0725 - val_level3_loss: 0.0732 - val_level4_loss: 0.0767 - val_level5_loss: 0.0910 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0104 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0163\n",
            "Epoch 103/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2626 - out_loss: 0.0494 - level1_loss: 0.0487 - level2_loss: 0.0480 - level3_loss: 0.0487 - level4_loss: 0.0525 - level5_loss: 0.0810 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0053 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0875\n",
            "Epoch 00103: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2626 - out_loss: 0.0494 - level1_loss: 0.0487 - level2_loss: 0.0480 - level3_loss: 0.0487 - level4_loss: 0.0525 - level5_loss: 0.0810 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0053 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0875 - val_loss: 0.3773 - val_out_loss: 0.0743 - val_level1_loss: 0.0739 - val_level2_loss: 0.0732 - val_level3_loss: 0.0739 - val_level4_loss: 0.0767 - val_level5_loss: 0.0996 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0466\n",
            "Epoch 104/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2565 - out_loss: 0.0493 - level1_loss: 0.0485 - level2_loss: 0.0479 - level3_loss: 0.0487 - level4_loss: 0.0520 - level5_loss: 0.0742 - out_mean_squared_error: 0.0056 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0492\n",
            "Epoch 00104: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2565 - out_loss: 0.0493 - level1_loss: 0.0485 - level2_loss: 0.0479 - level3_loss: 0.0487 - level4_loss: 0.0520 - level5_loss: 0.0742 - out_mean_squared_error: 0.0056 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0492 - val_loss: 0.3747 - val_out_loss: 0.0738 - val_level1_loss: 0.0734 - val_level2_loss: 0.0728 - val_level3_loss: 0.0733 - val_level4_loss: 0.0768 - val_level5_loss: 0.0984 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0327\n",
            "Epoch 105/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2521 - out_loss: 0.0490 - level1_loss: 0.0482 - level2_loss: 0.0476 - level3_loss: 0.0482 - level4_loss: 0.0517 - level5_loss: 0.0704 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0388\n",
            "Epoch 00105: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2521 - out_loss: 0.0490 - level1_loss: 0.0482 - level2_loss: 0.0476 - level3_loss: 0.0482 - level4_loss: 0.0517 - level5_loss: 0.0704 - out_mean_squared_error: 0.0057 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0388 - val_loss: 0.3679 - val_out_loss: 0.0737 - val_level1_loss: 0.0734 - val_level2_loss: 0.0730 - val_level3_loss: 0.0734 - val_level4_loss: 0.0764 - val_level5_loss: 0.0900 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0113 - val_level5_mean_squared_error: 0.0158\n",
            "Epoch 106/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2461 - out_loss: 0.0489 - level1_loss: 0.0481 - level2_loss: 0.0478 - level3_loss: 0.0485 - level4_loss: 0.0517 - level5_loss: 0.0625 - out_mean_squared_error: 0.0056 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0164\n",
            "Epoch 00106: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2461 - out_loss: 0.0489 - level1_loss: 0.0481 - level2_loss: 0.0478 - level3_loss: 0.0485 - level4_loss: 0.0517 - level5_loss: 0.0625 - out_mean_squared_error: 0.0056 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0164 - val_loss: 0.3649 - val_out_loss: 0.0744 - val_level1_loss: 0.0740 - val_level2_loss: 0.0733 - val_level3_loss: 0.0738 - val_level4_loss: 0.0772 - val_level5_loss: 0.0836 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0124\n",
            "Epoch 107/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2407 - out_loss: 0.0493 - level1_loss: 0.0484 - level2_loss: 0.0481 - level3_loss: 0.0486 - level4_loss: 0.0524 - level5_loss: 0.0541 - out_mean_squared_error: 0.0056 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0051 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0071\n",
            "Epoch 00107: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2407 - out_loss: 0.0493 - level1_loss: 0.0484 - level2_loss: 0.0481 - level3_loss: 0.0486 - level4_loss: 0.0524 - level5_loss: 0.0541 - out_mean_squared_error: 0.0056 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0051 - level3_mean_squared_error: 0.0051 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0071 - val_loss: 0.3661 - val_out_loss: 0.0740 - val_level1_loss: 0.0739 - val_level2_loss: 0.0732 - val_level3_loss: 0.0736 - val_level4_loss: 0.0777 - val_level5_loss: 0.0852 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0144\n",
            "Epoch 108/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2409 - out_loss: 0.0487 - level1_loss: 0.0480 - level2_loss: 0.0477 - level3_loss: 0.0481 - level4_loss: 0.0517 - level5_loss: 0.0570 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0055 - level5_mean_squared_error: 0.0103\n",
            "Epoch 00108: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2409 - out_loss: 0.0487 - level1_loss: 0.0480 - level2_loss: 0.0477 - level3_loss: 0.0481 - level4_loss: 0.0517 - level5_loss: 0.0570 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0050 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0055 - level5_mean_squared_error: 0.0103 - val_loss: 0.3633 - val_out_loss: 0.0739 - val_level1_loss: 0.0735 - val_level2_loss: 0.0728 - val_level3_loss: 0.0737 - val_level4_loss: 0.0764 - val_level5_loss: 0.0839 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0127\n",
            "Epoch 109/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2402 - out_loss: 0.0486 - level1_loss: 0.0479 - level2_loss: 0.0472 - level3_loss: 0.0478 - level4_loss: 0.0516 - level5_loss: 0.0571 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0049 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0078\n",
            "Epoch 00109: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2402 - out_loss: 0.0486 - level1_loss: 0.0479 - level2_loss: 0.0472 - level3_loss: 0.0478 - level4_loss: 0.0516 - level5_loss: 0.0571 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0052 - level2_mean_squared_error: 0.0049 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0078 - val_loss: 0.3650 - val_out_loss: 0.0738 - val_level1_loss: 0.0734 - val_level2_loss: 0.0728 - val_level3_loss: 0.0732 - val_level4_loss: 0.0772 - val_level5_loss: 0.0859 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0149\n",
            "Epoch 110/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2432 - out_loss: 0.0484 - level1_loss: 0.0477 - level2_loss: 0.0472 - level3_loss: 0.0477 - level4_loss: 0.0518 - level5_loss: 0.0612 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0049 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0202\n",
            "Epoch 00110: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2432 - out_loss: 0.0484 - level1_loss: 0.0477 - level2_loss: 0.0472 - level3_loss: 0.0477 - level4_loss: 0.0518 - level5_loss: 0.0612 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0049 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0202 - val_loss: 0.3690 - val_out_loss: 0.0740 - val_level1_loss: 0.0738 - val_level2_loss: 0.0730 - val_level3_loss: 0.0736 - val_level4_loss: 0.0767 - val_level5_loss: 0.0902 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0212\n",
            "Epoch 111/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2371 - out_loss: 0.0483 - level1_loss: 0.0477 - level2_loss: 0.0469 - level3_loss: 0.0476 - level4_loss: 0.0512 - level5_loss: 0.0547 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0108\n",
            "Epoch 00111: val_loss did not improve from 0.36317\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2371 - out_loss: 0.0483 - level1_loss: 0.0477 - level2_loss: 0.0469 - level3_loss: 0.0476 - level4_loss: 0.0512 - level5_loss: 0.0547 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0108 - val_loss: 0.3724 - val_out_loss: 0.0742 - val_level1_loss: 0.0737 - val_level2_loss: 0.0732 - val_level3_loss: 0.0741 - val_level4_loss: 0.0773 - val_level5_loss: 0.0929 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0279\n",
            "Epoch 112/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2388 - out_loss: 0.0482 - level1_loss: 0.0474 - level2_loss: 0.0469 - level3_loss: 0.0478 - level4_loss: 0.0509 - level5_loss: 0.0573 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0171\n",
            "Epoch 00112: val_loss improved from 0.36317 to 0.36304, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 450ms/step - loss: 0.2388 - out_loss: 0.0482 - level1_loss: 0.0474 - level2_loss: 0.0469 - level3_loss: 0.0478 - level4_loss: 0.0509 - level5_loss: 0.0573 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0171 - val_loss: 0.3630 - val_out_loss: 0.0739 - val_level1_loss: 0.0735 - val_level2_loss: 0.0729 - val_level3_loss: 0.0735 - val_level4_loss: 0.0767 - val_level5_loss: 0.0833 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0132\n",
            "Epoch 113/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2337 - out_loss: 0.0480 - level1_loss: 0.0473 - level2_loss: 0.0469 - level3_loss: 0.0477 - level4_loss: 0.0503 - level5_loss: 0.0519 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0049 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0099\n",
            "Epoch 00113: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2337 - out_loss: 0.0480 - level1_loss: 0.0473 - level2_loss: 0.0469 - level3_loss: 0.0477 - level4_loss: 0.0503 - level5_loss: 0.0519 - out_mean_squared_error: 0.0055 - level1_mean_squared_error: 0.0051 - level2_mean_squared_error: 0.0049 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0099 - val_loss: 0.3701 - val_out_loss: 0.0738 - val_level1_loss: 0.0736 - val_level2_loss: 0.0729 - val_level3_loss: 0.0737 - val_level4_loss: 0.0767 - val_level5_loss: 0.0920 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0225\n",
            "Epoch 114/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2344 - out_loss: 0.0479 - level1_loss: 0.0471 - level2_loss: 0.0468 - level3_loss: 0.0476 - level4_loss: 0.0506 - level5_loss: 0.0531 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0105\n",
            "Epoch 00114: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2344 - out_loss: 0.0479 - level1_loss: 0.0471 - level2_loss: 0.0468 - level3_loss: 0.0476 - level4_loss: 0.0506 - level5_loss: 0.0531 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0050 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0105 - val_loss: 0.3641 - val_out_loss: 0.0739 - val_level1_loss: 0.0735 - val_level2_loss: 0.0729 - val_level3_loss: 0.0733 - val_level4_loss: 0.0770 - val_level5_loss: 0.0846 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0136\n",
            "Epoch 115/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2302 - out_loss: 0.0476 - level1_loss: 0.0468 - level2_loss: 0.0462 - level3_loss: 0.0472 - level4_loss: 0.0507 - level5_loss: 0.0492 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0083\n",
            "Epoch 00115: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2302 - out_loss: 0.0476 - level1_loss: 0.0468 - level2_loss: 0.0462 - level3_loss: 0.0472 - level4_loss: 0.0507 - level5_loss: 0.0492 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0083 - val_loss: 0.3641 - val_out_loss: 0.0738 - val_level1_loss: 0.0735 - val_level2_loss: 0.0729 - val_level3_loss: 0.0739 - val_level4_loss: 0.0764 - val_level5_loss: 0.0847 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0148\n",
            "Epoch 116/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2354 - out_loss: 0.0475 - level1_loss: 0.0468 - level2_loss: 0.0462 - level3_loss: 0.0472 - level4_loss: 0.0512 - level5_loss: 0.0555 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0057 - level5_mean_squared_error: 0.0139\n",
            "Epoch 00116: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2354 - out_loss: 0.0475 - level1_loss: 0.0468 - level2_loss: 0.0462 - level3_loss: 0.0472 - level4_loss: 0.0512 - level5_loss: 0.0555 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0057 - level5_mean_squared_error: 0.0139 - val_loss: 0.3690 - val_out_loss: 0.0742 - val_level1_loss: 0.0735 - val_level2_loss: 0.0731 - val_level3_loss: 0.0742 - val_level4_loss: 0.0772 - val_level5_loss: 0.0891 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0108 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0187\n",
            "Epoch 117/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2348 - out_loss: 0.0477 - level1_loss: 0.0469 - level2_loss: 0.0463 - level3_loss: 0.0473 - level4_loss: 0.0514 - level5_loss: 0.0538 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0067\n",
            "Epoch 00117: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 362ms/step - loss: 0.2348 - out_loss: 0.0477 - level1_loss: 0.0469 - level2_loss: 0.0463 - level3_loss: 0.0473 - level4_loss: 0.0514 - level5_loss: 0.0538 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0056 - level5_mean_squared_error: 0.0067 - val_loss: 0.3659 - val_out_loss: 0.0741 - val_level1_loss: 0.0740 - val_level2_loss: 0.0732 - val_level3_loss: 0.0739 - val_level4_loss: 0.0768 - val_level5_loss: 0.0854 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0138\n",
            "Epoch 118/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2377 - out_loss: 0.0476 - level1_loss: 0.0469 - level2_loss: 0.0462 - level3_loss: 0.0472 - level4_loss: 0.0505 - level5_loss: 0.0586 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0102\n",
            "Epoch 00118: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2377 - out_loss: 0.0476 - level1_loss: 0.0469 - level2_loss: 0.0462 - level3_loss: 0.0472 - level4_loss: 0.0505 - level5_loss: 0.0586 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0049 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0102 - val_loss: 0.3690 - val_out_loss: 0.0742 - val_level1_loss: 0.0737 - val_level2_loss: 0.0732 - val_level3_loss: 0.0738 - val_level4_loss: 0.0770 - val_level5_loss: 0.0893 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0140\n",
            "Epoch 119/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2420 - out_loss: 0.0476 - level1_loss: 0.0468 - level2_loss: 0.0463 - level3_loss: 0.0471 - level4_loss: 0.0511 - level5_loss: 0.0637 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0265\n",
            "Epoch 00119: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2420 - out_loss: 0.0476 - level1_loss: 0.0468 - level2_loss: 0.0463 - level3_loss: 0.0471 - level4_loss: 0.0511 - level5_loss: 0.0637 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0054 - level5_mean_squared_error: 0.0265 - val_loss: 0.3700 - val_out_loss: 0.0742 - val_level1_loss: 0.0737 - val_level2_loss: 0.0734 - val_level3_loss: 0.0735 - val_level4_loss: 0.0775 - val_level5_loss: 0.0904 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0201\n",
            "Epoch 120/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2412 - out_loss: 0.0474 - level1_loss: 0.0466 - level2_loss: 0.0460 - level3_loss: 0.0469 - level4_loss: 0.0502 - level5_loss: 0.0644 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0245\n",
            "Epoch 00120: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 369ms/step - loss: 0.2412 - out_loss: 0.0474 - level1_loss: 0.0466 - level2_loss: 0.0460 - level3_loss: 0.0469 - level4_loss: 0.0502 - level5_loss: 0.0644 - out_mean_squared_error: 0.0054 - level1_mean_squared_error: 0.0050 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0245 - val_loss: 0.3696 - val_out_loss: 0.0738 - val_level1_loss: 0.0737 - val_level2_loss: 0.0737 - val_level3_loss: 0.0735 - val_level4_loss: 0.0785 - val_level5_loss: 0.0888 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0107 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0123 - val_level5_mean_squared_error: 0.0143\n",
            "Epoch 121/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2382 - out_loss: 0.0473 - level1_loss: 0.0465 - level2_loss: 0.0464 - level3_loss: 0.0466 - level4_loss: 0.0526 - level5_loss: 0.0584 - out_mean_squared_error: 0.0053 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0047 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0129\n",
            "Epoch 00121: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2382 - out_loss: 0.0473 - level1_loss: 0.0465 - level2_loss: 0.0464 - level3_loss: 0.0466 - level4_loss: 0.0526 - level5_loss: 0.0584 - out_mean_squared_error: 0.0053 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0048 - level3_mean_squared_error: 0.0047 - level4_mean_squared_error: 0.0060 - level5_mean_squared_error: 0.0129 - val_loss: 0.3655 - val_out_loss: 0.0742 - val_level1_loss: 0.0736 - val_level2_loss: 0.0732 - val_level3_loss: 0.0736 - val_level4_loss: 0.0770 - val_level5_loss: 0.0853 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0157\n",
            "Epoch 122/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2482 - out_loss: 0.0474 - level1_loss: 0.0466 - level2_loss: 0.0460 - level3_loss: 0.0473 - level4_loss: 0.0544 - level5_loss: 0.0684 - out_mean_squared_error: 0.0053 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0072 - level5_mean_squared_error: 0.0451\n",
            "Epoch 00122: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 363ms/step - loss: 0.2482 - out_loss: 0.0474 - level1_loss: 0.0466 - level2_loss: 0.0460 - level3_loss: 0.0473 - level4_loss: 0.0544 - level5_loss: 0.0684 - out_mean_squared_error: 0.0053 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0048 - level4_mean_squared_error: 0.0072 - level5_mean_squared_error: 0.0451 - val_loss: 0.3669 - val_out_loss: 0.0740 - val_level1_loss: 0.0738 - val_level2_loss: 0.0732 - val_level3_loss: 0.0737 - val_level4_loss: 0.0773 - val_level5_loss: 0.0866 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0137\n",
            "Epoch 123/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2414 - out_loss: 0.0471 - level1_loss: 0.0464 - level2_loss: 0.0459 - level3_loss: 0.0465 - level4_loss: 0.0514 - level5_loss: 0.0644 - out_mean_squared_error: 0.0053 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0047 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0208\n",
            "Epoch 00123: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2414 - out_loss: 0.0471 - level1_loss: 0.0464 - level2_loss: 0.0459 - level3_loss: 0.0465 - level4_loss: 0.0514 - level5_loss: 0.0644 - out_mean_squared_error: 0.0053 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0047 - level4_mean_squared_error: 0.0059 - level5_mean_squared_error: 0.0208 - val_loss: 0.3797 - val_out_loss: 0.0736 - val_level1_loss: 0.0734 - val_level2_loss: 0.0728 - val_level3_loss: 0.0735 - val_level4_loss: 0.0773 - val_level5_loss: 0.1041 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0120 - val_level5_mean_squared_error: 0.0473\n",
            "Epoch 124/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2348 - out_loss: 0.0467 - level1_loss: 0.0459 - level2_loss: 0.0453 - level3_loss: 0.0460 - level4_loss: 0.0503 - level5_loss: 0.0591 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0197\n",
            "Epoch 00124: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2348 - out_loss: 0.0467 - level1_loss: 0.0459 - level2_loss: 0.0453 - level3_loss: 0.0460 - level4_loss: 0.0503 - level5_loss: 0.0591 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0053 - level5_mean_squared_error: 0.0197 - val_loss: 0.3682 - val_out_loss: 0.0738 - val_level1_loss: 0.0733 - val_level2_loss: 0.0730 - val_level3_loss: 0.0734 - val_level4_loss: 0.0764 - val_level5_loss: 0.0903 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0179\n",
            "Epoch 125/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2352 - out_loss: 0.0466 - level1_loss: 0.0459 - level2_loss: 0.0455 - level3_loss: 0.0459 - level4_loss: 0.0497 - level5_loss: 0.0603 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0175\n",
            "Epoch 00125: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 365ms/step - loss: 0.2352 - out_loss: 0.0466 - level1_loss: 0.0459 - level2_loss: 0.0455 - level3_loss: 0.0459 - level4_loss: 0.0497 - level5_loss: 0.0603 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0047 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0175 - val_loss: 0.3669 - val_out_loss: 0.0742 - val_level1_loss: 0.0738 - val_level2_loss: 0.0732 - val_level3_loss: 0.0741 - val_level4_loss: 0.0768 - val_level5_loss: 0.0864 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0142\n",
            "Epoch 126/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2319 - out_loss: 0.0467 - level1_loss: 0.0459 - level2_loss: 0.0454 - level3_loss: 0.0461 - level4_loss: 0.0496 - level5_loss: 0.0562 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0050 - level5_mean_squared_error: 0.0113\n",
            "Epoch 00126: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2319 - out_loss: 0.0467 - level1_loss: 0.0459 - level2_loss: 0.0454 - level3_loss: 0.0461 - level4_loss: 0.0496 - level5_loss: 0.0562 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0050 - level5_mean_squared_error: 0.0113 - val_loss: 0.3720 - val_out_loss: 0.0736 - val_level1_loss: 0.0732 - val_level2_loss: 0.0724 - val_level3_loss: 0.0729 - val_level4_loss: 0.0766 - val_level5_loss: 0.0963 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0104 - val_level3_mean_squared_error: 0.0104 - val_level4_mean_squared_error: 0.0115 - val_level5_mean_squared_error: 0.0252\n",
            "Epoch 127/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2298 - out_loss: 0.0466 - level1_loss: 0.0459 - level2_loss: 0.0451 - level3_loss: 0.0460 - level4_loss: 0.0494 - level5_loss: 0.0544 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0110\n",
            "Epoch 00127: val_loss did not improve from 0.36304\n",
            "10/10 [==============================] - 4s 364ms/step - loss: 0.2298 - out_loss: 0.0466 - level1_loss: 0.0459 - level2_loss: 0.0451 - level3_loss: 0.0460 - level4_loss: 0.0494 - level5_loss: 0.0544 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0049 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0110 - val_loss: 0.3643 - val_out_loss: 0.0739 - val_level1_loss: 0.0737 - val_level2_loss: 0.0730 - val_level3_loss: 0.0734 - val_level4_loss: 0.0770 - val_level5_loss: 0.0844 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0105 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0142\n",
            "Epoch 128/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2283 - out_loss: 0.0464 - level1_loss: 0.0458 - level2_loss: 0.0450 - level3_loss: 0.0459 - level4_loss: 0.0497 - level5_loss: 0.0525 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0120\n",
            "Epoch 00128: val_loss improved from 0.36304 to 0.36245, saving model to trained_models/MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected_cv.h5\n",
            "10/10 [==============================] - 4s 447ms/step - loss: 0.2283 - out_loss: 0.0464 - level1_loss: 0.0458 - level2_loss: 0.0450 - level3_loss: 0.0459 - level4_loss: 0.0497 - level5_loss: 0.0525 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0052 - level5_mean_squared_error: 0.0120 - val_loss: 0.3625 - val_out_loss: 0.0740 - val_level1_loss: 0.0735 - val_level2_loss: 0.0730 - val_level3_loss: 0.0736 - val_level4_loss: 0.0765 - val_level5_loss: 0.0824 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0107 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0114 - val_level5_mean_squared_error: 0.0120\n",
            "Epoch 129/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2321 - out_loss: 0.0463 - level1_loss: 0.0456 - level2_loss: 0.0449 - level3_loss: 0.0455 - level4_loss: 0.0491 - level5_loss: 0.0586 - out_mean_squared_error: 0.0051 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0045 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0211\n",
            "Epoch 00129: val_loss did not improve from 0.36245\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2321 - out_loss: 0.0463 - level1_loss: 0.0456 - level2_loss: 0.0449 - level3_loss: 0.0455 - level4_loss: 0.0491 - level5_loss: 0.0586 - out_mean_squared_error: 0.0051 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0045 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0211 - val_loss: 0.3755 - val_out_loss: 0.0738 - val_level1_loss: 0.0734 - val_level2_loss: 0.0729 - val_level3_loss: 0.0737 - val_level4_loss: 0.0773 - val_level5_loss: 0.0982 - val_out_mean_squared_error: 0.0109 - val_level1_mean_squared_error: 0.0106 - val_level2_mean_squared_error: 0.0105 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0464\n",
            "Epoch 130/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2352 - out_loss: 0.0462 - level1_loss: 0.0455 - level2_loss: 0.0448 - level3_loss: 0.0453 - level4_loss: 0.0493 - level5_loss: 0.0629 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0045 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0341\n",
            "Epoch 00130: val_loss did not improve from 0.36245\n",
            "10/10 [==============================] - 4s 368ms/step - loss: 0.2352 - out_loss: 0.0462 - level1_loss: 0.0455 - level2_loss: 0.0448 - level3_loss: 0.0453 - level4_loss: 0.0493 - level5_loss: 0.0629 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0048 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0045 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0341 - val_loss: 0.3725 - val_out_loss: 0.0740 - val_level1_loss: 0.0738 - val_level2_loss: 0.0731 - val_level3_loss: 0.0739 - val_level4_loss: 0.0773 - val_level5_loss: 0.0934 - val_out_mean_squared_error: 0.0110 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0118 - val_level5_mean_squared_error: 0.0233\n",
            "Epoch 131/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2378 - out_loss: 0.0460 - level1_loss: 0.0452 - level2_loss: 0.0447 - level3_loss: 0.0455 - level4_loss: 0.0494 - level5_loss: 0.0665 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0047 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0045 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0310\n",
            "Epoch 00131: val_loss did not improve from 0.36245\n",
            "10/10 [==============================] - 4s 367ms/step - loss: 0.2378 - out_loss: 0.0460 - level1_loss: 0.0452 - level2_loss: 0.0447 - level3_loss: 0.0455 - level4_loss: 0.0494 - level5_loss: 0.0665 - out_mean_squared_error: 0.0052 - level1_mean_squared_error: 0.0047 - level2_mean_squared_error: 0.0045 - level3_mean_squared_error: 0.0045 - level4_mean_squared_error: 0.0051 - level5_mean_squared_error: 0.0310 - val_loss: 0.3688 - val_out_loss: 0.0743 - val_level1_loss: 0.0738 - val_level2_loss: 0.0732 - val_level3_loss: 0.0739 - val_level4_loss: 0.0772 - val_level5_loss: 0.0886 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0107 - val_level4_mean_squared_error: 0.0117 - val_level5_mean_squared_error: 0.0196\n",
            "Epoch 132/300\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2363 - out_loss: 0.0461 - level1_loss: 0.0452 - level2_loss: 0.0449 - level3_loss: 0.0456 - level4_loss: 0.0487 - level5_loss: 0.0649 - out_mean_squared_error: 0.0051 - level1_mean_squared_error: 0.0047 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0049 - level5_mean_squared_error: 0.0341\n",
            "Epoch 00132: val_loss did not improve from 0.36245\n",
            "10/10 [==============================] - 4s 366ms/step - loss: 0.2363 - out_loss: 0.0461 - level1_loss: 0.0452 - level2_loss: 0.0449 - level3_loss: 0.0456 - level4_loss: 0.0487 - level5_loss: 0.0649 - out_mean_squared_error: 0.0051 - level1_mean_squared_error: 0.0047 - level2_mean_squared_error: 0.0046 - level3_mean_squared_error: 0.0046 - level4_mean_squared_error: 0.0049 - level5_mean_squared_error: 0.0341 - val_loss: 0.3748 - val_out_loss: 0.0740 - val_level1_loss: 0.0736 - val_level2_loss: 0.0730 - val_level3_loss: 0.0734 - val_level4_loss: 0.0769 - val_level5_loss: 0.0976 - val_out_mean_squared_error: 0.0111 - val_level1_mean_squared_error: 0.0108 - val_level2_mean_squared_error: 0.0106 - val_level3_mean_squared_error: 0.0106 - val_level4_mean_squared_error: 0.0116 - val_level5_mean_squared_error: 0.0314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if D_S == 0:\n",
        "    GRF_pred = model.predict(X_Test, verbose=1)\n",
        "    print(GRF_pred.shape)\n",
        "elif D_S == 1:\n",
        "    GRF_pred = model.predict(X_Test1, verbose=1)\n",
        "    print(GRF_pred[0].shape)"
      ],
      "metadata": {
        "id": "NYMwgI1XR6yP",
        "outputId": "e5c938ec-5d86-44e8-959c-72b7fba39e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98/98 [==============================] - 13s 105ms/step\n",
            "(3136, 1024, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_mean = np.mean(Y_Test, axis=0)\n",
        "ground_truth_std = np.std(Y_Test, axis=0)\n",
        "prediction = np.nan_to_num(GRF_pred[0][:, :, 0])\n",
        "prediction_mean = np.mean(prediction, axis=0)\n",
        "prediction_std = np.std(prediction, axis=0)"
      ],
      "metadata": {
        "id": "o7fMZhlCR86Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.corrcoef(ground_truth_mean, prediction_mean)[0, 1]"
      ],
      "metadata": {
        "id": "HlICv49uR_pp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "x = np.linspace(0, 100, FRAME_LEN)\n",
        "plt.plot(x, ground_truth_mean, linewidth=3, label='Ground Truth')\n",
        "plt.fill_between(\n",
        "    x, ground_truth_mean + ground_truth_std,\n",
        "    ground_truth_mean - ground_truth_std, alpha=0.3,\n",
        "    label='Ground Truth +/- STD'\n",
        ")\n",
        "plt.plot(x, prediction_mean, linewidth=3, label='Prediction')\n",
        "plt.fill_between(\n",
        "    x, prediction_mean + prediction_std,\n",
        "    prediction_mean - prediction_std, alpha=0.3,\n",
        "    label='Prediction +/- STD'\n",
        ")\n",
        "plt.legend()\n",
        "plt.title(f'R = {round(R, 5)}')\n",
        "plt.xlabel('Stance Phase (%)')\n",
        "plt.ylabel('GRF N/Kg (Normalized)')\n",
        "plt.savefig(figures_dir + 'Results_' + config + '.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E7RUwpkLSCL9",
        "outputId": "1598ac34-1211-49aa-dfb4-a6cc0294ab5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuYAAAHPCAYAAAAf0h8wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5xU1fn48c+9M7OzvbIssHSQKkXEICWWoGKIJhpL7IIgCiLE5JugQYM1PzRGBUQERVZAVERjBY1dQUVUBEGRupRle5s+c9vvj9kddtylLduA5/16+XrN3jn3nDNz3eWZM895rmJZloUQQgghhBCiWanNPQEhhBBCCCGEBOZCCCGEEEK0CBKYCyGEEEII0QJIYC6EEEIIIUQLIIG5EEIIIYQQLYAE5kIIIYQQQrQA9uaegBBCnIj8fj/nnXcegUAAj8dDYmIisbGxAHg8HjIyMhgyZAh//vOfycrKata5vvvuuyxYsID8/HxiYmIYPXo0U6ZMIS4u7oj7eOedd1i8eDG7du3Csiz69u3LlClTGDRoUK22RUVFzJkzh08//RS/309SUhIXX3wxkyZNwul01mr/zTffMG/ePLZt24au62RmZnLllVfypz/9Cbs9+p8xr9fLc889x6pVqygvLyc2Npb+/fszZcoUunXrFtV2zpw55OTkRK5LTf/85z8ZNWrUEb9+IYRoEJYQQohG8+qrr1o9evSwXn311cgxXdetVatWWX379rWGDx9ulZeXN9v8XnnlFatnz57WG2+8YVmWZe3Zs8c6//zzreuvv97Sdf2I+pgzZ47Vo0cP67nnnrOCwaDl9Xqte++91+rbt6+1du3aqLYFBQXWr3/9a+uiiy6ycnNzLcuyrPXr11vDhg2zxo0bZ5mmGdX+k08+sXr16mX95S9/sdxut2WaprVq1Sqrd+/e1rRp06LaGoZhXXXVVdZpp51mffnll5ZlWVZpaal1ww03WAMHDrS2bt0a1X727NlR10UIIZqbpLIIIUQTs9lsXHjhhVx88cUUFxfz+uuvN8s8KisrmTlzJqNGjeL3v/89AB06dGDatGmsXbv2iOaVl5fHvHnz+NWvfsXYsWOJiYkhPj6e6dOnk5aWxowZM7Bq3Mdu9uzZFBYWcs8999CpUycABg4cyKRJk/j888955513ovp/7rnnUBSF++67j8TERBRF4cILL+S8887jv//9L4WFhZG23333Hd999x1/+tOfOPPMMwFIT09n+vTp+Hw+Fi9efMzvmRBCNCYJzIUQopn07t0bgJ07dzbL+KtWrcLtdnPBBRdEHT/rrLOIjY3llVdeOWwfq1evRtd1hgwZEnXcbrfzq1/9ip07d7J+/frI8U8//RS73c7gwYOj2g8fPhyA1157Lep4YWEhqampJCYmRh3v0KEDAAUFBVFtaz5XrWPHjgDk5+cf9vUIIURzksBcCCGaiWmaAKSlpTXL+OvWrQOgZ8+eUccdDgfdunVjw4YNhEKhQ/ZRVlYG1P0aMjIyANiwYUPkWHl5OcnJyaiqeti2AD169KC8vJzKysqo47m5uTgcjqggvHv37pHnftkWoGvXrod8LUII0dxk86cQQjSTn376CVVVj3iT4WWXXRa1Qnwo06dPZ/To0YdsUx2wZmZm1nqudevWbN68mb1799baNFlTdUBeHaDXVFFRAYTTXWq2LysrwzAMbDZbrbYej4fKykpSUlIAmDp1KuvXr+eee+7h3nvvJTExkZUrV/Lxxx8zdepU0tPTI3307NmTsWPH8vLLLzN8+HBGjBhBQUEB999/P+3bt2fcuHG15rhmzRpee+019uzZg2VZnHrqqYwbN67Wir4QQjQFCcyFEKKJBQIB3n33XX744QdmzZpFnz59jui8V199tUHn4fF4AOqsvlJ9zOVyHbKPYcOGoaoqa9as4fbbb48cNwyDb775BghXqKk2fPhwXn/9db788ktGjBgROf7VV19FHvv9/khg3q1bNxYtWsT06dMZOnQoDoeDhIQEHnroIS699NJa8/n73/9OWloaU6ZMwTAMNE3jnHPO4YknnqB169a12ufn53P//ffTvXt38vLymDlzJtdffz0PP/xwJO9eCCGaiqSyCCFEE3jooYcYPnw4Q4YMYeDAgdx///2MHz++Vn738aZjx46MGTOG9evX8+STT0ZWvB944IFIQF4z8L/99ttJTU3lgQce4KeffkLXddatW8eTTz5JcnIyQFT5wlWrVnH55ZfTv39/1q5dy7fffstdd93FjBkzePjhh6Pm4vV6GTNmDC+++CILFy5kw4YNfPDBBwSDQa666iq2bdsW1X7s2LHk5OREUmCys7P5z3/+Q6tWrXjggQfwer2N8p4JIcTBSGAuhBBNYPr06axZs4a1a9eydOlSbDYb//jHP/juu++abU7VGyprrmhXqz6WlJR02H6mTZvGfffdx0cffcTIkSO54oorSEhIYPr06cCB/HGA9u3bs2LFCvr168ctt9zCWWedxZw5c3jkkUdo3749MTExkTHLy8uZPn06HTt25B//+Aepqak4nU4uueQSrrrqKp577jk+++yzSN8LFixg7dq1/OMf/2Dw4MHYbDY6dOjAo48+SklJSWQ+NV9/TExM1LGYmBhGjBiBy+Xi22+/PZK3UQghGoyksgghRBMbPHgwf/vb37jnnnuYO3cuCxcuPKLzGjrHvHPnzmzatIni4uJI6ki1oqIiVFWtVeHkYK666iquuuqqqGMvvfQSAL169Yo6Xh0s/1JxcTGnnHJKJPd848aNeL1eTj/9dBRFiWo7ePBgnn/+edasWcNZZ50FhPPFq5+rqVWrVnTq1IkNGzZEbvZ0KNUfJOrKmxdCiMYkgbkQQjSDyy67jGeffZbVq1ezZcuWWsFrXRo6x/yMM87g7bff5ueff46kcwBomsbOnTsZMGBAnXfiPFI//vgj8fHxkZrih1JYWEhxcXFUcF+dSvLLoByIVHXx+XyRYzUfH6y91+slMTERl8vF8uXLGT9+fK22paWlQPNVyxFCnLwklUUIIZqBzWZj4sSJQPgmOs3hwgsvJDExkffffz/q+GeffYbf7+fyyy+POm6aZp0r9nfccQdffvll1DGPx8N7773HddddF5Vjvm7dOqZMmVKrj1dffZWkpCSuvvrqyLFTTz0VRVH4/vvva7Wvro3et2/fqPZArfbl5eXk5ubSunXryAZQl8vFo48+Snl5eVTbUCjEF198QXx8PIMGDao1rhBCNCYJzIUQoplcfPHFdOzYkZUrVx5xikpDSk1N5c477+S9997jzTffBGDfvn088sgjDBkypFbVk/vvv5+zzz671geJ3bt38/jjj1NcXAzA/v37mTx5Mt26deO2226LalsdsL/22mtYloVhGLz++us888wzzJw5MyofvWPHjlx55ZVs3ryZWbNmEQwGMU2TDz74gGXLltG9e3cuueSSSPtbbrmFpKQkZs6cyZYtW4BwOso//vEPAoEAf/nLX6JW3y3L4u9//3vkxkRlZWXcddddFBQUMG3atCPKrxdCiIakWDXvlSyEEKJB+P1+zjvvPAKBQCSvOTY2ljlz5kStxL722mvcddddJCUlkZyczEcffdTkc121ahULFiygoKAAh8PB6NGjmTp1aq0yivPmzWPBggXcf//9XHzxxZHjOTk5vPPOO+zZsweHw0FqaioXXXQRN910U63Nlbm5uTzxxBNs3LgRj8dDXFwcffv2ZeLEifTr16/W3EzT5MUXX+TVV18lNzcXRVFIS0vjN7/5Dbfddlut3Phdu3Yxd+5cvvrqK4LBIAB9+vRh7NixnHPOOZF2hmHwySef8Oabb/Ljjz/i8XjQNI1+/foxduzYSN66EEI0JQnMhRBCCCGEaAEklUUIIYQQQogWQAJzIYQQQgghWgAJzIUQQgghhGgBJDAXQgghhBCiBZDAXAghhBBCiBZAAnMhhBBCCCFaAHtzT6ClKC/3YppNXzkyIyOR0lJPk48rmpZc5xOfXOOTg1znk4Nc5xNfc11jVVVIS0s46PMSmFcxTatZAvPqscWJT67ziU+u8clBrvPJQa7zia8lXmNJZRFCCCGEEKIFkMBcCCGEEEKIFkACcyGEEEIIIVoACcyFEEIIIYRoASQwF0IIIYQQogWQqixHwe/34vFUYBh6g/VZVKRimmaD9SdaJrnOLYfNZicxMZW4uIOXqxJCCCGagwTmR8jv9+J2l5OamonDEYOiKA3Sr92uousSsJ3o5Dq3DJZloWkhKiqKASQ4F0II0aJIKssR8ngqSE3NJCbG2WBBuRCiaSmKQkyMk9TUTDyeiuaejhBCCBFFAvMjZBg6DkdMc09DCNEAHI6YBk1JE0IIIRqCBOZHQVbKhTgxyO+yEEKIlkgCcyGEEEIIIVoACcyFEEIIIYRoASQwFye0qVMn8fvfj2LEiMHNPZWjsmHDesaMuYZzzjmThx66t7mnI4QQQogmIOUSBfv35/Hyyy/w/ffrUVUFwzBQFJUuXbpy5pnDGDZsBMnJKc09zXqZNespFi6cz6JFzxyy3eTJEyguLiIuLh4Av99HXt4+WrfOirx2XdcIBAKsWPFWg85x5cpwf6NHXxw5NmDAaeTkLOPyyy8+2GlCCCGEOMFIYH6S+/TTj3nwwRlcd92NzJ+/iNjYWADKykqZN28ODz44g0mTpnLNNdc380wb37RpdzNoUHhl/bvvvmHKlFsZP/7WSMCcn7+f22+/pcHHrSswF0IIIQ5FUahaTLMiP1tWM09KHDMJzE9iO3Zs5777pnPFFVdz443jop5LT8/grrv+yY4d25tpdk2rT5++h/1WwOl0MnDgoCaakRBCCHEoCoplACqKoqCqRIJ0cfySwPwktmjRM2iaxpVXXl3n86qqMnXq/+F0huu3b9iwnscf/ze5uTs5//wL6dmzFx9++D67d+ficlWyevU3APz44yYWLHiKffv2AtC+fQcmTJhEnz6nAvD88wtZufIt8vL2MXv20wwaNJi8vH1Mn/73SN/Tp98LwH333c2GDespKipkzpz5LF/+Inv37iEUCnLDDWO56KJLoua8ffs2nnji3+zatYN27bIZPHgIqnr4rRSTJk09bJv09Azuvvs+PvzwfyxZksP27VsZM2Y8AOvWrWXnzh2kpKQwdOgI1qz5jKKiQl555U3atm3H999/x6OPPsL27VsZO/Zmxo27BZ/Py6RJN5OXF36fxoy5BoCRIy/g+uvHRI29YsVLfPjh++Tn76dHj15MmzadjIxWh52zEEKIE4eqKtgUE81QUBTArA7MQTENHA47WBaaLgH68Uo2f56kTNNk7dovadu23SEDvAEDBtKrV5+qx+G851atMvn66y9xOGKYN28hL7ywAqfTCcDmzZuYPHkCAwacxooVb7FixVv06zeAyZMnsHnzJgBuvHEc06bdHTVOdnb7SN81zZjxIOPH3wrA8uXLuPvue1m6dDlXXHE1jzzyL/bu3RNp63K5mDr1VhISEnj99Xd55pnF9OnTl7feev3Y37AaRo68gJycZQC8/fYb9O3bj/nzFzF//nMA/PWv0yJzrjZw4KDIOdXi4xPIyVlGz5696dmzNzk5y8jJWVYrKP/6669IS0tn3ryFLFmynNzcnTz11OwGfU1CCCGal8OuoqqHvseCagSwQv4DB8waN0qzjHAui2U20gxFU5AV82Pw7to9vLFmF8GQ0azzcMbY+MPwLlw4pOMRn1NZWYHf76Nbt+71GjMxMYk//OGPAKSlpbF48csAzJs3m6SkZG644aZI2xtvHMebb77GvHmzefLJBfUaD2DUqNEkJCQCcN55FzBr1qOsX/8tHTqEX/fy5cuorKzkttum4nA4APj1r89hxYrllJWV1nvcQ+natTtDhw6PPJ41a16Dj5GSksLIkRcAkJSUxJAhw/j8808afBwhhBBNz6ZaKHoQTAeKcpiwTNcAE2xVP5vGgSVW08CyzKqV8xhM05LUluOQrJgfg/fW7Wn2oBwgGDJ4b92ewzesg1XHThHDMBgz5hrGjLmGP/7xd4wff0OtNl26dI36OTu7PYFAgI0bv6dnz97YbLbIc3a7nR49erFx4/cEg4F6zROgY8fOkccpKakAUQH3Dz9sID4+IaodQLdu3eo95uF07Rrdd3Z2+wYf45evJyUlpdE+aAghhGgaiqJgU5VwcB30YukailKVonIwloGlh2r8bEY/tiwsy6xKcZE7HB+PJDA/BqPO6Igzxnb4ho3MGWNj1BlHvloO4cA2Li6+zgDPZrNF0iratcumoqK8VpvqsoI1ud0uTNMkOTm51nPJySmYponL5TqqeUaPGRd5XJ03bhgHPhiVlBSTlJRU67zqVfbGUHNOTTWGoiiYpnxVKYQQxzNFAcUIgmViGRoYIRTLwI5+0HMsQ6sK4Amnrpg1FtdMA0wTDB0MPZyPbpMw73gjqSzH4MIhHY8qfaQudruKrjd9kKWqKmeeOYyPP/6AwsICsrLaHHOfSUnJqKqKy1VZ6zmXqxKbzRYJ2muuqNfk8/nqPX6rVpn8+OPmWsc9Hk+9+6yv6g8ONb+R8Pm8TT4PIYQQLZgRAsWGpYdQ7I5wOkrAA87UWk0VBSxDx9SCqIqCYlpA1eKUZQJm1aq5iWUaKJaJqkRaiOOEfJQ6iY0ZM56YmBheeOH5BukvNjaW/v0H8vPPW9D1A5/4dV1n69Yt9Os3AKczXCc9LS0dICqILy8vqzOoP1L9+g3A7/exe3du1PGdO5u+5GN6egZA1DcEv5xXNbv9wOdjv9/P6tWfNurchBBCNJ3qeuPhxwo2tWrBxjLANFCwMEMBrKqVbkwNu13BZlNQ1fB/4fQWBUvTsLRguE8FMM2q5yyoXuMzjfCmUMuM3hwqjgsSmJ/EunXrzowZD7Jy5VvMnz83akXX6/Xw8ssvsGPHduLja6etHMzEiVNwu10sXvxc5NiSJYtwu91MnDglciw7uz1t2rTl448/xLIsLMti6dIcEhIS6v16rrzyGlJSUnjqqVlomgbA559/UucqemPr3bsvcXHxfPTR+0D4rqFvvll3dZi2bbMpLi7Csiw2bvyeWbMea8qpCiGEaESqAqoRrqSiKKBofhTFwqZYKJaOaRoEAkH8Xh/llW6Ki8rZsd/Nlt0VbNtXye4CN8WVAYJBP6GgH8s0UI0Qlh6I5Jgrlkl4bdyqCszDFVosQwLz441i1bX77yRUWurBNA/+VhQU7KZNm04NPm5zpbLUlJe3j5deeoH167+tSjGx8Pv9dOnSlREjzub88y8kNjaWXbt2ct99d5Obu5O4uHiystpw221TOeOMIVH9/bKOeXZ2uI55376nRrXbtGkjjz32MG63h7Zt23LDDTcxc+YDeL1esrLa8MwzzzN37hN89tknFBUV0rlzF8aOvZnWrbP4z38eZvv2raSnZ9C//wAefPARIHzTpMcff4Rdu3aQldWWvn37kZCQwNKlOXTv3oPrrx8TqXByMP/6131s2LCevLx9tG6dRYcOnZg166nI8+vWrWXu3FmR8dPTM5gx48FaG2JXr/6UefPmYJom7dq149Zbb2fs2GtJT8+gS5dukT737NnNAw/cg8/nw2azceutt9O2bbuo97p791OYM2c+DzzwT9atW0tZWSndu/eo8/0XR6ahf6czM5MoLnY3WH+iZZLrfHJoyOtsN/1gaOiOZGzoqJoPQ7ETCOn4K0oJqbFU5G7FHhNDfKt2hCpLUNr0wNQ1bAr4NIUEh4nir8RfnEdsXAytOnUh1q6iWAZmYhtUM4QSdIEzAXwusDtQnIlYRgjdVv8FrxNZc/0uq6pCRsbB975JYF7lZA7MReOT69zySGAu6kOu88mhIa+zLVAOho6VnInm8+F3VxIMBNBNFc1dSkxiChX792F3OEjIaI2npJCE1FRwxGOaBkpsEqoRRHNX4KsMF2NISk/HGRdHXKyDhLZdsBlBLF8lOBMwPSWoMfHhIN0IoTtqF2QQLTcwl82fQgghhBCNQFEUrICXkBbCbSQQKN6LXVUIeNzEJibi93gJ+cNlhE3TBD2Ioeu4SkpQVRVnXDw2Uyfo82DoWqRfd1kZXpuKqtpIrHSR2r4z8VRtANU1LLuBYhmSY34cksBcCCGEEKKBKQpYpkZZSRkVFR6cyQG8ZSWRal3+qophelX5W9MwcJUeKGFsmiZ+nxfV76uzRK5pmJiGibvChRVXQcBmkJIRh6Vr2PQgmLFgGOF5SG7EcUMCcyGEEEKIhqSAy6cRrCimvKQCy7LQSosPe1qt7GLLwjxMVG3oOooexOvXCKk+Yl1u4g2DmNhETF0LTwaJzI8XEpgLIYQQQjQQ3bQorfSB34VWXlTnHbYbmuZ1YxoGalwihWV+MhU76Wnhu4TKivnxRQJzIYQQQohjpCjgDep4K13oIY1Q4W60YLBJxva6XDhiYnDqQQzTpKTSR0xakFhDo6qEujhOSGAuhBBCCHGUVFWJVHOzgKKKAIa3ErylKIq9yYLyalooRNAdvkmfZVq43X6CZoB000BuW3P8aBFXyuVy8de//pWePXuyb9++5p6OEEIIIcQh2SwNh13BtGBfsZeKCh+mqwRPWSm+sqJmmVP1hlLLsjANnQqXn0pPoOruoNHstnDVGAiv9tvtLSIkPOk1+4r56tWrmTFjBnFxcUd9bmFhITNnzuTHH39EURQ6duzInXfeSdeuXQ9/shBCCCFEPSgKWEEvOjYKPCp2zUui3cTvKscyzWbfamkaBiGvBy0YxFPpQsdOWqIzOt9dC2Cz2dGrQ0HLILxRVDSnZv949NRTTzFnzhxGjRp1VOd5PB6uu+46bDYb77zzDqtWraJbt25ce+21FBQUNNJshRBCCHGyU1WFoMdFaVEJaAEsVwGmqxjDMJp7ahF+rxfTNDG85RSX+/EGf1HT3DJAD1X9oEAdJRlF02v2wHzx4sX06dPnqM/LyckhLy+PO++8E7vdjqIo/PnPf0bTNObOndsIMxVCCCHEyU5RodIToLCwlICrApvmxVtRga+irLmnVifD7yHeAUXlXvSadzg3TTDDHyQUhfANiUSza/bA3G6vXzbNqlWrOOWUU2jVqlXkmNPpZNCgQbz77rtNUp5ICCGEECcPRYUyV4DyUhcBr4+Az4eq+Vp0zKEFQzgCJdi1APuKPZE0G8s0sIwDdxO1qlbMFQVstmYPD09ax+U7HwwG2blzJx07dqz1XMeOHXG5XOTl5TXDzMTRys/fz5gx13D++b9m8uQJzT0dIYQQok6KAqUVAbSyfByKhq5pGJpGeQtPnzV0ncqC/dgMPx5PiDJ3EFVVwNBB1w5sDLWqU1kUFEvSWppLs2/+rI+KigpM0yQxMbHWcwkJCQCUlZXRvn37I+4zI6N2XzUVFamNtmO5uXZCa5rGTTddT2lpKWVlpXTu3AWHw0EwGMQwDPr168+ECZNo165do82hQ4f2LF36EhMn3gxEvxezZz/OJ598xLJly4mNPfLNwW63m5deeoGzzz6XHj16Rj338ssvkpPzLDk5L5CV1aZhXsQRkh3vLYuqqmRmJjVonw3dn2iZ5DqfHGpe50BIp6S4HAUDNeTBpvpIiHc24+yOXowSJDsrE5sZwhaTSFy8HUxwtgq/Tt0dwp4UfmxqIVRHTHNOt0m0xN/l4zIwbwylpZ5IPdK6mKaJrjf8J0i7XW2Ufo+EothYtGgZCxfOZ9GiZ/j3v2fRtm04CN+/P4877riNm266jpycl6JShhpD9deANd+LlJRUWrfOwrKO7j2qqKhk4cIFZGW1pWvXU6KeS0xMIiurLapqb9L3vTmvs6ibaZoUF7sbrL/MzKQG7U+0THKdTw41r7NhWuSVeHEGywAFT7kLQ9cP3UELZNrcxFCEZXOimQqZtgCmFkC1h0ssqgEfesCBqiqoloZuNW0d9qbWXL/LqqoccjH4uFzCS01NRVVVPFX1Omvyer0ApKenN/W0Tijt2mVz9dXXUVFRwdtvv94sc7j22ht58skFOByOButz1KjRPPvsYtLS5P8PIYQQh6abFrkFbgIBDc1VihryHpdBOYDf48VTsA9LD6F7yvEFQqCHUJRwsEjNzZ9SoaXZHJcr5k6nky5durB3795az+3du5fk5GSys7ObYWYnlqystgB8//13jBlzDXl5e+nZszeXXno5r732Cvv27aW0tIRXXnmTtm3bsWdPLvPmzWHr1p+x2+1kZLTi5psnctppp0f1+/HHH/Dss0/j9/tp3TqLP/7xilpjz5z5AF9//RVFRYWR/qutXfslOTnPUFpaSlxcHPHxCZx77kguueRyPvnkQxYvfg6AZ599muXLXwRgxowH+fDD//H++++Sl7eP2bOfZtCgwZE+f/xxEwsWPMW+feH/p9q378CECZPo0+dUAD788H8sWZLD9u1bGTNmPIqisHbtlxQVFXL66YP561/vJD4+oQHffSGEEM1JM0xy810Yukma3YfX78fncjX3tOrPsjAMA1PXQDcoKCqjTWYSSZYJloVVM2vAMlFtCoqiYBgSpDel42LFPBQKUV5eHnXst7/9Ldu2baO0tDSq3XfffceoUaMid7MS9bd37x4AzjhjCDk5y+jZsze5ubvYuXMHc+c+w/Llr9O6dRYQTn259dZxxMXFs3z5G7z88uv85jfncccdt7Fp08ZIn+vXf8s//3kXF154Ea+99g7z5i1k27at5ObujBr7zjvvYfz4W2vN6dNPP+Zvf5vKxRdfyvLlb/D88y8xatRoZs9+jNLSEi644Lf8+9+zABg//lZycpaRk7OMLl26Mn78rUybdnetPjdv3sTkyRMYMOA0Vqx4ixUr3qJfvwFMnjyBzZs3ATBy5AXk5CwDYNWqtxk8eAgLFuQwf/4iVq/+jGXLljTAOy6EEKIlcHlDFJR6sakKSY4Qnvzd6Jp2+BOPA5YWQPf7CAVDlFf6sDQflqmDZUVtBLXZVFSl5VabOVEdFyvmEydOZN26daxcuTKyoXPs2LG88cYbPPzww/y///f/UFWVJ554ArvdzuTJk5tkXqGNqwh++wZogSYZ76AcsThP/wMx/X/bYF3+9NNmXnjhebKy2nDxxZdEjuu6xo03jgPA6Yxl3ryFZMRSbSYAACAASURBVGS0YubMB/B6Pdx++x3YbDYALrvsT7zyyks899wCHnvsSSC8it2qVSbXXnsDEL4d8Lhxt/DGG68ddk6WZTFnzmP07NmL0aMvjhy/5JLLePXVl1GU+n3OnDdvNklJydxww02RYzfeOI4333yNefNm8+STC6Lad+t2CgMGDAQgK6sN/foN4Ntvv67zg4QQQojjh6KAP2RQXlSOXfMQYwTRfcdv+kpddH/4xkMAPl8Qn8tFQnIKFgfKJWKZWJaFYprI3UCbVrMH5k8++STvv/8+JSUlAEyYMAGHw8EjjzxCz57hihqtWrUiNTWV2NjYyHmJiYksXbqUmTNnMnr0aBRFoUOHDixdupQ2bZqm2kZo43vNH5QDaAFCG9875sD8b3+bit3uIBQKkpCQwAUX/Jbrrx9DcnJKpE12dgdiYg7s1K6ubLJu3Vd06NApKndbURS6du3G2rVfRe6GtnnzDwwZMhRVPRBEx8bGHlHq0d69uykoyGfYsBG1nluyZPnRv2AgEAiwceP3nHnm8MgHCgjX1+/Roxdr135JMBjA6Tzw/16nTp2j+khJSWXPnt31Gl8IIUTLoCjgCxoUlnpI0YowPT4CAT96KHT4k48jocCBuMUwTNzlFcTGx6NYFqBgWVSVS7QAE7AdpCfRGJo9MJ88efJhV7gffvjhOo+3adOGJ554ojGmdURi+o9qMSvmMf1HHXM3NauyHExcXN1lCysqKvD5/IwZc03Uca/XQ2JiIh6PG8Mw0HWdpKTkWucnJBy6XGX1GEDUB4Vj5Xa7ME2T5OTac0pOTsE0TVwuF5mZBwLzmh8QIVx2z5SNMkIIcdxSFPAGDXL3u4i3GYS8bryVJ0f1nbKiYuLTMkiKqUplsazwf1hVm0AlMG9KzR6YH89i+v/2mFepT5QyeqmpqWRmZvHss4sP2sYwDOx2O646Ns94PJ4669L/cgwAl6vy2CZbQ1JSMqqq1tmny1WJzWarM2gXQghxYlAU8AQNdu93EdIMUuz6CZW6cjiWZeH1+olTbVUbD8ObQTEtLCTHvKkdF5s/Rcv3q18NZe/e3fj9/qjjP/ywgccffwQAm81G37792Lp1SyS1BcLpJPv3H/5OrR06dKJNm7b89NPmqOOWZXHHHbexZctPQDgNpfo4QG7uLrZt+7nOPmNjY+nffyA//7wFvcYfYl3X2bp1C/36DYhKYxFCCHHiUBTwBHRy91eiaUa4dKClR25Pf7IwdQ2vJ4CiUJXSYqFg1bgbqGgqEpiLBnHTTROw2+3MmfNYJMAtKSnm0Udn0qVL10i78eNvpbS0JFLFxLIsnnlmHtYR/PIrisLtt/+Fn3/ewsqVb0WOL1u2mPLyck45pQcAaWnpOJ1OiouLAFi06BlWr/7soP1OnDgFt9sVKbMIsGTJItxuNxMnTjmKd0EIIcTxQlEU3H6d3P0uNM2kVbKdpPgYTP/JkcJSk2KZeL0BgsFQrVQWu10N1zkXTUKxqpcVT3KHu/NnQcFu2rTp1ODjNmcqi6Zp3HzzjZSVlVJWVkrnzl2w2x2RsoDV3G43t99+C3l54Rrf2dkd+NOfruG3v70oqt3evXt4+uk5/PjjZlJTU7HbHVx66eVRFVQAPvnkQ5599ml8Ph9paemMHn0RH3/8IT///BPZ2R146KFHWLJkUaSOeefOXfjDHy7jiiuuAuCrr74gJ+dZSktLiY+Po2vX7tx221RatcqMjPHGG6+xZMkiEhISSEtL5957/8WKFS9F6phnZ7fnnHNGMnHi7UDtOubZ2eE65n37huuYr1u3lrlzZ7F9+1bS0zM4/fQzmDHjQaZOncTWrVvw+3107tyVGTMejPogUu1ESVk6kTT077TcEfLkINf5xKAoCi5/iD35bjTdRFUVspw+LGwES/OwY+D1ndh3vqzJGR+PaRikdulJZloi+F0o8SlYWhAlNgFFtaHrxgl136GWeudPCcyrnIyBuWg6cp1bHgnMRX3IdT7+KYpCpTfInkJP5O9yjMNGSqiAQEUphmGQEO88qQJzCL8vSR17kJmZilPzoCSkYIUCKM54sMVg6BqmdeIkWrTUwPzEeYeFEEIIIQ5BUaDCE2RPwYGg3GFXibNbGAFv1P6nk41lWZi+SrwBDTCr9mmZWKYBlllVQlE0NgnMhRBCCHHCUxQo84TYU+hGr7rNvDPGRobDT4yvkIDX28wzbH4hdwUBjx/NqMogMAFDB9kI2mQkMBdCCCHECU1RoMwdYl+BG8M4kLZqVyy0sny8pUXNOLuWQwsGIeTB6wuiYgHh1XLM6prmorFJYC6EEEKIE5cCJa4gewvdGFV7yaorjThUA03TmnmCLYdlWeheF15vAE03wDDBMsCsWjUXjU5uMCSEEEKIE5KiQnFFkP3F0QUe0uMBVNBCeCUwj+KprCRedeD1ayShodjsWJYhOeZNRFbMhRBCCHHCURSFovJAVFDeKtWJAqAFsQVd2MwAUpyuDqaJ1x9CDwWqUllMyTFvIrJiLoQQQogTiwKF5X7ySzxUx912m4o95CEhPgHF8OEvLQgnn4taTD2IoekEQkEccboE5U1IAnMhhBBCnDgUKCjzU1jqpeZiuM2mYNO8JKkhghUlhAKB5ptjC2caBoqpU+H2kpAUj2p3Sop5E5HAXAghhBAnBgXyS3wUlfsiQXlCnINYh0IgZOAvLyfg9TXvHI8DhmGgmCE8Lg+eeAfJsQe/IY5oWBKYCyGEEOKEkFfspaTcH7W4a1NM7EaIGEVF1/RmmxuALejCdMShaj4SSjaDYgMsfGndMWKSiPEWEUrIwrLFNOs8TV1HMTQsC9zeAMmWiWVYKLEgKfmNSwJzIYQQQhzXLCCv2ENJRe30FJuloxoBYuwxeJq4Akts5S5iPAU4AmXE+MK10vWYJCzVgSNQFmmXUPpT5LEek0xxjz82a/67ZVmUF4Xn6w8Y6JpeVZVFQXJaGpdUZRFCCCHEccuyYO9BgnIAu6KjuUuxhXxNWoHF6d5H2p5PSCjbEgnKAewhd1RQ/kv2kItW215v/qXpqvF1wyAQCIGhy17ZJiCBuTihTZ06id//fhQjRgxu7qkclQ0b1jNmzDWcc86ZPPTQvc09HSGEaJEMy2J3kZuyOoJyp8OGM8aGogXxudxUFu5v9Pl4gwY/Feps+Dmf9Nz3692PI1iB/RfBu2IEsQUrmz5gtyx8/iCGFmracU9SkspyjHTTIqAZ9T7fpimRO5HVV6zDhl2t/8fY/fvzePnlF/j++/WoqhLe9KGodOnSlTPPHMawYSNITk45pjk2l1mznmLhwvksWvTMIdtNnjyB4uIi4uLiAfD7feTl7aN166zIa9d1jUAgwIoVbzXoHFeuDPc3evTFkWMDBpxGTs4yLr/84oOd1uj8fj+/+915PPTQIwwdOvyozi0rK2XZsiV8/fWXqKoNCH812rNnL4YPP4uzzz6X7777hmnT7iA7u0PkvLy8vWiaRufOXSPHCgsLuP32O/B6vbzzzpts376VpKRksrLaYJoGwWCQ7Oz2nHPOSC688HfExDRvbqYQomkYpsXuQjcuz4GAUQGcTjuBoE5qTAjV1AiWlQBgNcYt5U0drSyfwnI/e9w2fmv7iu5qwwSwCaVb8KWfghbfGluwksyt/0XBIhTXCj02DYevGH9ad7yZ/RpkvIOxLAs9pBHEwi5pLI1OAvNjFNAMtu+pqPf5NpuCYRzb/+jdO6aS6Kzfpfz004958MEZXHfdjcyfv4jY2FggHFjNmzeHBx+cwaRJU7nmmuuPaY7Hg2nT7mbQoPDK+nfffcOUKbcyfvytkYA5P38/t99+S4OPW1dg3lhWrnyLNm3aRl7noXz99VfYbDZOP/2MoxojGAwwadJ4WrfOYvbs+aSmpgKQl7ePadPuYO/ePZx99rkA9OzZmyefXBA5d/LkCRQU5JOTsyxyrPobgyuuuIorrriKESMGM2LEWUyfHj6u6zrr1n3FE088yooVL/Hww4/Ttm27o5qzEOL4ohkmuws8eHzRQbDdrpIUY6DrKlbQRWXB3kYZ3zAtfioy6F/0Nh2VcjoCZzgO3r7YSCLT5sZtxrLCfyZXxn1BwmEC+PjyrcSXbyUYn4XTVxg5HuMvIcYf/rDhKPiGUHxrwMJwpmLaYxvg1UWzLIuKkhJISyQJAxNbg48hDpBUlpPYjh3bue++6fzxj1dw443jIkE5QHp6Bnfd9U969OjVjDNsOn369D3stwJOp5OBAwc10Ywax8qVb7F+/bdH1HbNms/41a+GHPUK9DffrGPfvr1cccVVkaAcIDu7PTffPDHyc3JyCn369D1sf127diczM/Ogz9vtdoYOHcHTTz9HZWUl//d/UwiF5CtXIU5UId1kV76rVlAOVTcR0rzEx9owfO4GHzuoW3y1x+Slr12cVvBfspTyw55T1PF89IGXk99vLJ7+VzFqSGeKe12OZqsdROd4fl3rWM2gvC6tdq6k1c5VZG59DVXzHvmLORqWRSCgEQo1b1Wbk4GsmJ/EFi16Bk3TuPLKq+t8XlVVpk79P5zOcGC2YcN6Hn/83+Tm7uT88y+kZ89efPjh++zenYvLVcnq1d8A8OOPm1iw4Cn27QuvVLRv34EJEybRp8+pADz//EJWrnyLvLx9zJ79NIMGDSYvbx/Tp/890nf1auh9993Nhg3rKSoqZM6c+Sxf/iJ79+4hFApyww1jueiiS6LmvH37Np544t/s2rWDdu2yGTx4CKp6+M+fkyZNPWyb9PQM7r77Pj788H8sWZLD9u1bGTNmPADr1q1l584dpKSkMHToCNas+YyiokJeeeVN2rZtx/fff8ejjz7C9u1bGTv2ZsaNuwWfz8ukSTeTlxd+n8aMuQaAkSMv4Prrx0SNvWLFS3z44fvk5++nR49eTJs2nYyMVoedc32ZpskXX6zmttsO/778kmGEU7vy8/NrPTd8+Fmcemp/ALp3P4Xu3Q/f/9VXX3dE46alpXPttTcye/Z/eOedN7n00suPYtZCiJZOUcAXNNhd4CIQjE4hdTpUUBTsioES8hKbmEgw6G+wsQ3T4vt8izW7LfyayX2pq0hR6+7fsMcRSO5EILkjoaTsWi8inG7jpKzHJWT99FLU05kdOkH55/Wao2oEydqyHG96T1zZw+rVx6FoIZ1AIEhCkrPZ96WeyGTF/CRlmiZr135J27btDhngDRgwkF69+lQ9Duc9t2qVyddff4nDEcO8eQt54YUVOJ1OADZv3sTkyRMYMOA0Vqx4ixUr3qJfvwFMnjyBzZs3AXDjjeOYNu3uqHGys9tH+q5pxowHGT/+VgCWL1/G3Xffy9Kly7niiqt55JF/sXfvnkhbl8vF1Km3kpCQwOuvv8szzyymT5++vPXW68f+htUwcuQFkVSLt99+g759+zF//iLmz38OgL/+dVpkztUGDhwUlZ4BEB+fQE7OMnr27E3Pnr3JyVlGTs6yWkH5119/RVpaOvPmLWTJkuXk5u7kqadmN+hr+qVNmzbidrsYOnTEUZ/bt++pxMXFsWDBXBYteoaCgoLIc3a7vVE/UJx5Zvgfoy++qN8/bEKIlklRFDwBnV37awflAMlxKimxFk5Vx1tZicNXht4A35xZlsWWYotn1pns3b2PSXFv80T60oMG5XpMEsU9LsWVPbR2UP4Lpj2OYHxW1LFB7R14M/oc05wTyn4Gs/573w7G0HX8Xp9kmTcyCcxPUpWVFfj9PtLTM+p1fmJiEn/4wx8BSEtLY/HilwGYN282SUnJ3HDDTZG2N944jqSkJObNO7ZgctSo0SQkhO8+dt55F2CaZlRaxvLly6isrOS226bicIST/X7963Po0qXbMY17KF27do9sjOzatTuzZs1r8DFSUlIYOfICAJKSkhgyZBjffruuwcepafXqz+jbt19UKsqRyshoxf33zyQhIZGFC+dz+eUXMXbsNSxcOJ/8/MatipCV1Qaoe7VeCHF8UhSo9IXI3e8i9ItiC7FV+6ssPYg9UAGuIrRgkPKCvGMujVjht3jlB5PXfzSpCFhcFf8lHey1yxz6U7pS0Ocaik/5A8WnXIplcx7xGK52v4o8Lu8Y3nvjanM6le3OpKzz+WzocDVfmv3YobVmr57OEs8IFnsOv2CSufU1Wm17A2fl7iOey5EI+X34Qw0f9IsDJJXlJFfXHy7DMBg3LrzZ0+WqJD09g2efXRzVpkuXrlE/Z2e3JxAIsHHj95x55nBstgObQ+x2Oz169GLt2i8JBgM4nfXbnNKxY+fI45SUcMBYVlYaOfbDDxuIj0+IagfQrVs3vv3263qNeThdu0YH/dnZ7Rt8jF++npSUlKjXfTBbtvzIzJkPRh3Ly9vLnj27+fzzT6OOP/rorKhvK9as+Yzf/e739Z7z0KHDefXVt1mz5nM+++xjvv76KxYteobFi59j0qQp/OlP19a770ML//8stXaFOEEoUOYOkVfkRq+jUEKSE3RdxWYFqdjfMBs9DdPi850aH20z0asKuQx1biPN5qvd1h5PRcezAdCPIiCvpse1oviUS1AMDS2+6m+waseX0RuA1oCVcjqbiyw+2G4R0EHBokOglLOcW7ApdX/4sGse0CB9z0fk970B1IbZsGkEfHh8IRLS4jGPsaKcqJsE5ieplJRU4uLi6wzwbDZbJO2iukrGL1WXFazJ7XZhmibJycm1nktOTsE0TVwuF5mZ9QvM4+LiIo+r88ar85kBSkqKSUpKqnVe9Sp7Y6g5p6YaQ1EUzCMo+9WrV59a6TOTJ0/gtNNOZ9y4g1eX2bdvL7t35zJixFmRY//3f1MoKSmJ/HznnXdHUpwOxm63c/bZ53L22ediGAZr137Jf/4zk7lzZzF06Ag6dux02NdwtKrTZqQqixDHP0VRKKr0k1/srTMItKkKNt2PMyYeK9Qw+eRlPou3tpjku8N/YxUszovdxEXx6+ts72k94JjH1GPTDvm8oiicmqXQOdVi1VaTHWUKr/vO4HXfGfwx8VvOjtl8yPMd/mK0hDbHPE8AM+TH5w+hJcViO4YyzeLgJDA/SamqyplnDuPjjz+gsLAgkgJwLJKSklFVFZerstZzLlclNpstErTXXFGvyeervSJxpFq1yuTHH2v/gfJ4PPXus76qPzjU/EbC52uk3fIN7PPPP6VDh45RK/WPPnrkaUglJcVs3rwpUhIRwtd72LAReDy3cf/997Bt29ZGCcy/+GI1AMOG1a5sIIQ4jiiQX+ajsNR70I2GNpuCqvtxxiVhuur/bweE/1Z/n2/x0Q4LzQQFkyviv2Z47NZabQNJ7bFUO1pcJr60U45p3KOR6FS4/FSVb/LC87SA1zynY8TDb2IPHpzHuvc1WGCuazp2PYQvqJMUd4j6kKLeJMf8JDZmzHhiYmJ44YXnG6S/2NhY+vcfyM8/b0HXD5RU0nWdrVu30K/fgEgaS1paOkBUEF9eXlZnUH+k+vUbgN/vY/fu3KjjO3dur3ef9VWdu+9yuSLHfjmvanb7gc/Hfr+f1as/rbNdU1mz5jNGjDi73ufv2bObxx6bWeeqfvUHlrS0Q68Q1UdJSTHLli2mU6fOTVITXgjROCxgX7GXwpK6g3K7TSEh1o5NVUEPYjcDGKHad/48Un7N4tXNJu9tCwflAGfH/lxnUF7W6TzKO59PRcdz8Wae2mApIkdKURTOaK9y9QCVhKq4+IfQoVMo40t+QjGCDTK+oWk4rBDlnqCkDDYSCcxPYt26dWfGjAdZufIt5s+fG7Wi6/V6ePnlF9ixYzvx8bXTVg5m4sQpuN0uFi9+LnJsyZJFuN1uJk6cEjmWnd2eNm3a8vHHH2JZFpZlsXRpDgkJCfV+PVdeeQ0pKSk89dQsNE0D4PPPP6lzFb2x9e7dl7i4eD76KHxLZl3XePPNuqvDtG2bTXFxEZZlsXHj98ya9VhTTjWKy1XJDz9siEpjqY/S0lKeemo2weCBfyz37NlNTs6z9OjRk/79Bx7rVCN0XWfNms+ZOHEcaWlpPProbLn7pxDHKcOy2FPooaTcf9DqH84YOwl2nRhVx+dyoRflEgrWL/AscFvkfGeyvUZW5ykJLi6Nr73BvqDPNQSTO9Q63hw6pircMEglIx4KjRRC1sE/IKiWjsN/+H1JR8KyLPTKIkzdIKRLjnljUKxj3bZ8gigt9RxyI0NBwW7atKn91btuWgS0+u9QtqkKxjFuoIh12LAfQ65XXt4+XnrpBdav/7YqxcTC7/fTpUtXRow4m/PPv5DY2Fh27drJfffdTW7uTuLi4snKasNtt03ljDOGRPX3yzrm2dnhOuZ9+54a1W7Tpo089tjDuN0e2rZtyw033MTMmQ/g9XrJymrDM888z9y5T/DZZ59QVFRI585dGDv2Zlq3zuI//3mY7du3kp6eQf/+A3jwwUeA8E2THn/8EXbt2kFWVlv69u1HQkICS5fm0L17D66/fkykwsnB/Otf97Fhw3ry8vbRunUWHTp0YtaspyLPr1u3lrlzZ0XGT0/PYMaMB2ttiF29+lPmzZuDaZq0a9eOW2+9nbFjryU9PYMuXbpF+tyzZzcPPHAPPp8Pm83GrbfeTtu27aLe6+7dT2HOnPk88MA/WbduLWVlpXTv3qPO9/9QDpdj/u677/Dkk4/zxhvvHTTd6HBcLherVr3FunVrKSjIx253EAj4sdlsDB06ghtuuKnWPoTNmzfx73//i7y8vWiaRufOXbnxxps499zzIm1eeeUl3nnnTbZv30pSUjJZWW2wLBO/3092dnvOOWckF174u0jpzsM52O90fWVmJlFc3PA3NBEti1znxqObFnsK3bg8dZc5TIqzEdQtnDaLRMuDZYvBvXdbvauvbMg3+d82i5p7Si9qk8/5ofdrtfVk9MHd7sj/1jYVv2bx6iaTBN9+JiV/cNB2lW2H4Gt1bGUYq9kdDhxtupOcnkZK/PGbztJcv8uqqpCRcfC9bxKYV6lvYH6s7HYVXT/8Rj5xfDtervPdd08jNjaWu+++r7mn0ugkMBf1Ide5cWiGye4CNx6fdtA2GXEGfisGp6Khlu1B13W0eqyUG6bF+9vDOeXVnDa4uKfJsJLXsOnR+ere9F642wzGsrXMIFQzwsH5KP1D+sTkRY672gwmuSB84z9vei9c2UMbbMyk7K4EnSl0bZvMEdQiaJFaamAuqSxCCAA0TePrr7865jQWIYQ4UooCQd1kx37XQYPyhNjwPhyHYuLAxIFBwOerV1Ae0MO1yWsG5ZkJFn/tsolfFyyNCso9mf0o7H01ruyhLTYoB3DYFC47VSXREf3t/Q7/gW8mE8q2hOu8NxDDU0acDYKSztLgpCqLEAIAh8PB//7XvBtPhRAnD0UBX9AgN99F8CA3rbHZFBIcBiFdBVPDaXeAz1Ov9JUKv8WKTSYlNRbEe2cqXNYml8y876LaBjoNw53c86jHaC4Om0J6vAE1qka+vjud02rcI67V9jcoPuVSDGftksZHK+Bx4YwrIJAUj8NpR1GUY76hkwiTFXMhhBBCNClFAU9AZ9f+uoPyWKedBKeK3aZi07wkxNoxvC7wFKN7jr56V57LYvH66KB8RCeF3/dWSCnfEtXWArSMpiuD2FC8rftHHn8S6E2FGU/AOrD+qlgmqXs/BfPY79xpGiZ+VyUeTwBFVbBZQezKcZrT0sIc8Yq5pmn88MMP5ObmUl5ejqZpxMbGkpWVRY8ePejWrfFuey6EEEKIE4OiKLj8Ifbku9EOsvfGoZjYMEGxoYa8xAFuVzmmcfTB345Si//+eOAunjYFRvdU6JulouhBHL7iqPbBxGwsRxxoDVNisKkEkzrgajMYPRhgzb5wsYUP/afyu/jvI21i/CUk53/dIPnmhmGghELohonN1MHSwB6urKYoHLT+vDi0wwbmO3bs4Omnn+aDDz4gEAjU+qpCqSpk2bp1a6644grGjh17TCXvhBBCCHFiUhSFSm+QPYWeQ26Itys6KhYKFp7KSrRgUb3G21xo8s7PFtW1HeIccG0PFx2VQqxyO6n7Po9q7259Gr70HjT+PZ0bgaLgzewHwO/TLJasN/kw0JdKM54/JnxDrBKudpNQtoWEsi1YikpZ5/MJJdbvTsmmYYChYegaYIBlVk8Dm01Bl/zzejlkYJ6Tk8Ojjz5Kx44dueyyy+jatSutW7cmLi4Om82Gpmm4XC4KCgr46aefWLJkCcuXL+epp57i1FNPPVTXQgghhDiJKApUeILsLfSgH2TlOzHOji9o4MAI38DG0nHXs0b5N/tMPthxIDhMiYVrTjXosXsVah033HG1/RXeVn3rNVZL0ypB4eLeKis2wdpQd74NdeE/6S9EtVEsk7TdH1HY+ypQj37LoWkY2FUTr8tFfJINSzeg+hYSpgnIHYjq46BXYtmyZSxbtow5c+Zw7rnnHqxZFL/fz5IlS7jllltYtmwZnTo1fHlBIYQQQhxfFAXKPCH2FboxjF9+8w4xDhvBkEG8TYNYJ0rIhWq3YfiOvpydZVl8nmvxxZ6alVfgyn4qGaGCOoNyLTYNb/rxs9nzSHTPUPh1Z4XPcy10bOzUMunqiE7bUU2NuMpc/Gndj7p/y7JA1zFMHd3pRLWqPkyhoFgG0LR3RT1R1BmYe71ePvvsM5YuXUrr1q2PuLO4uDgmTJhAt27dePHFF7nzzjsbbKJCCCGEOP4cKigHcNhVEmNA1xVsuo94h42QqwTT0NFDB69rXhfLCtco/27/gXGyk+GKU1XibDox5QW1zjHVGEq6/x6UE68exrCOCgVui22lsFlrXyswB0jd9zmxlbvwp3UnkNLlqPo3tSCmTUELmDhsoCpKOLfcMpHAvH7qDMwTEhJ4+umn693pyJEjGTly5P9n773jJKvK/P/3OTdUrs55BoYhDBmJA4ICwoLwlSiIgiAooLiA67IsIvvF/b30qwiuuoCShplRXPKCCigGcpIkCgwThcnTPZ27E44yLQAAIABJREFU8k3n90d1mJ6unqnuLph03n9133vuOaeqOnzuc5/n80z6eo1Go9FoNNs+w6K8PTVul2tDSqwgh2WG8fNZVLqXQjZbcuymCJTid4sV73aMrHNIbR+n1S6FviiJjr8NRnJH07PLCdulKIdiTv9n9pTMezPgufxeHGCvZCeze8y4cGo14dRqOnerwovUlj2/cvM4hYDOVJbW5rrBNUEFPoit1/t9a6aiP4mvvPJKJafTaDQajUazjTIqUl5ClNtm0Q7RFD7Cd4ja4GT6yfRNvBFOoBRPbCTK924UnBN+iWTPQpLtb44R5b4Zob/1CNxow8Rf3DZEyBScupfEFyY/Hfg0Px84nrfsQ3DDYwV4rOvdCc0dFPL4Tp58zsV1XETgDZ7Q1omTpaLC/Oqrr67kdBqNRqPRaLZBNpe+AhCxAsKWwJY+IvAIeSmcXH7CawVK8cQixYINRPn+zYJTd3Ow82OjwwDZ6l1Zv9fnydbtOeH1tkVak8V8cx+DxV4r89v35q36k8eMk74zoXnz2QyFXA7PDygUHBgS5koL88kybvHnBRdcMOHJ+vsnbvqv2bFZt24t1177b6xZs4pZs/bi1lvv3NJb0mg0Gs0UEELQmy6MK8rjEYt0zsXCI2QJSPWQHuhFTSLKGijFYwsVCztH1vlYi+DE3RThgbUlrxloPpRM3V4TXmtb5/DpguW9ihWDDySeWCLZp2Umsf73h8dIL4eZ78W3YijDHmem0qTTeWqUhw8VaWK0ozKuMH/jjTdobm4edSyTydDf349pmtTX1wPQ3d2N67pEo9EJFYpuL5jKAW/id/hDGL6AcfLuyt9EGE9M7BfIdV0uueRL9PR009PTzYwZu2CaFoVCHt/32W+//bn44stoaZmcv2m5tLS0Mn/+vVx++aVjzt1660957rlnuOeeBwiHw2XPmUqlePDBe/nkJ49h991HV9k/9ND9/OIXd3P33ffQ1NQ8zgxbJ2vWrOacc07nV796iBkzJlags2bNan71q/ksWPAOUhYLcqQU7LXXPhx33AkcdNAh/O53j3HLLT8Z9b4sX/4+lmXR1jZ9g7lW8cMf/oRXX32Fl156nuXLP6C2to7a2jqCwKdQKLDLLjP5p3/6NMceezxSbp+5mxqNZixCCPoy44tyKQRRyyfnCAx8jEKGVF83vudNeC0/UDy2SLFoA1F+ULPihJkeDUsfw3TGOrqkGg8g07Bj2jkP5ZvPeT2g4ENfHjrSMHODMXaui4alv8Y3QnTOOhtllJ8nniu4ePksRigBJXL5NeUxrjCvra3l6aefHv5+0aJF3HTTTXz961/n4IMPHjX2zTff5Mc//jH/+q//+uHtdGvFy5Nf+49JXy4NOalOZhsSbt0VrIkJc8uymD//Xu6++w7mzbuLm27672ERvnbtGr75zX/mkksuYP78+4dvwj5qampqaGpqwjAmVtmdTqeYN+8uWlpaxwjzZDJJc3MLtj2x9+vD4sEH7+XAAw8es89SvPjic0ybNn3Cory7u4tLL/0Shx56OLffPo9oNArA4sWLuPrqb5DL5TjooEMAOOqoT3Lddf85fO1ZZ51Cc3PLqCcZQzdRl112Baef/lnOPvtUTjvtTL7yla8C4DgOzz33ND/5yU08+ujDfP/7N5FMVk1ozxqNZttDCMFA1mFVexpvY0tEKLZuNwTSzREOxVG5DP3dHZOPlG8kyj/dvJ4T/GcwFo7ve15ITJvwWtsTiZDgU7sKfr+k+L492rMnV1W9P2ac4ReI9C0jO4EnC54fkOvrIlHViAoC3f1zkowbytrY6vAHP/gB3/3ud8eIcoCDDz6YG2+8kZtuuqnyO9R85LS2tvGFL3yRvr4+Hn/811tsH+ed9yVuvfVOLKtyld0nnngyc+b8kpqa8qvOP0wefPA+li5dUtbYF198niOP/OSE13j22afo7+/nvPO+NCzKAWbN2pNzzz1/+PuGhgZmzty8l+3ee++zSaFt2zb/9E+f5mc/u4vFixdy3XX/PuE9azSabQshIJ1zWdmeGtU8KGwbSCkIhUySYTClRHo5oqbCSU0+feWJjUT5wW2C48UrGCU8ygFy1TMHCz13vCf7G7N/s2BGTfHrlX49T7izS44bLuQsE6UgncohAneUIjcM/dR0IowbMf/MZz4z6vv333+f1tbx0xra2tpYvXp15Xam2aI0NbUAsH59B2vWrOa66/59OA/8jDPO4pFHHmL16lV0d3fx0EO/paWllZUrl3PbbbewZMliTNOkrq6eSy65jAMPHH0z98wzf2bOnNvJ5XI0NjZx5plnj1n/hhu+y2uv/YX16zuG5x/i1VdfYf78u+ju7iYSiRCNxjj22OM4/fSzePbZp/jlL+cCMGfO7Tz44H0AfOc73+Opp/7In/70JGvWrObmm28fjhIDvPfeu9x5589ZvXoVANOmTefSS7/O3nsXH3k+9dQfueee+SxbtoQLL7wYIQSvvvoK69d3cPDBh3DVVd8iGo1V8BMYzcDAAG+//Te+/OWxKT+bw/eLjxTb29ey++57jDp3xhlnkRsstjr00MM59NDDNzvf17/+jbLW3XnnGZxyyuk8+OB9vPLKSxxxxJET3LlGo9kWEAIyBZ/l7QO43mihHbMCFBJbBISEi2l4BPk8hpkhk594GqhSij8sUSxYv0H6Sqvg+F0F1rul69wydXsz0FpafO6ICCE4aQ/J3W8EOD78MTWLmc0Z9nJGO7LY6XVka/dAGaGy587lHQLPGxTmAilBCoVObCmfsm9jPM/jzTffHPf8G2+8MSwANNs+q1atBKCtbRptbdOYP/9eZs3ai+XLP+D99//Bz352Fw8++GsaG5uAYvrL1772FSKRKA8++BseeODXfOpTx/PNb/4z77779vC8b731Jtdffy2f/vRneOSRJ7jttrtZunQJy5ePfpT2rW/9Xy6++Gtj9vXcc89w9dXf4JRTzuDBB3/DL35xPyeeeDI33/xjuru7OOGEk7jppv8G4OKLv8b8+fcyf/697LLLTC6++Gtcc81/jJlzwYJ3ufzySznggAN5+OHHePjhx9hvvwO4/PJLWbCg+IfquONOYP78ewH4/e8f55BDZnPnnfO54455vPji89x77z0VeNfH5y9/eYlYLM7++39swtceeOAhSCm54Ybv8uCD99Lb2zt8LhQKU11dXcmtjuLww4ti/OWXX/zQ1tBoNFuWnOOzYu0Arjsiyg1ZbMduqQKGBEu4SK+ATHcVPcpzE7dEHGoe9Pf20YWeJ03rJ9Hx17HjhUHHrLMZaDlsEq9q+6YqLDh6FzH8/WOdY1Mkw+k11L3/+wnloziuj+sUUBu6smiHlglRtjD/9Kc/zaWXXsoPf/hDXnrpJRYvXszixYt56aWXuOGGG/jqV7/KySePtd7RbHssXLiA//mfX9DU1Mwpp5w+6pznuXzpS18BiqLuttvupqGhkblz7ySTSXPFFd8czgn/7GfPoampmblzR/KT58y5nfr6Bs47r+j6I4TgK1/5Kk4Z3d2UUtxyy4+ZNWtPTj75lOHjp5/+WXbZZSZikg0ibrvtZhKJJBdc8OXhY1/60ldIJBLcdtvNY8bvuuvuHHBAUSA3NTWz334H8Oabr01q7XJ58cXnOeKIj0843x5g99334Nprr8f3fW6++cecdtqJfO1rX+bee++hp6e0lVilGCokbW8v7Y6g0Wi2bRw/YPm6FAV3JDBnSEFN3MI0JMJ3idkSszBAtmsdmb4eCrkcA11dE1pHKcUz74/u6LlvU9F9pW7Fn0h0vj3mmnxyOoEdZ7BPvGYjDmwVNCeKX6/xa1jD2DQfK9+LnIDBReD75PMFBGrkbdfCfEKMm8qyMVdffTUrV65k3rx5zJ8/f9Q5pRRHHnkk//Zv/1bp/Wk+Iq6++huYpoXjFIjFYpxwwkmcf/6FY3KJ29qmjyqcHBJer7/+F6ZP33lU7rYQgpkzd+XVV/8y/DRlwYJ3mD37iFFOHeFwmLa2ts3ucdWqFbS3r+PjHz9qzLl77nlwYi94kHw+z9tv/43DDz9ylOg1TZM99tiTV199hUIhTyg04gqz884zRs1RVVXNypUrNrvWiy8+x5w5d4w61tXVOSrlZoih6DwUn1a9+urLXHPN/53ISxvFSSd9hmOPPZ7nnnuaF154jjfeeJV3332buXPv4Nprr+e4406Y9NybQg1GWoT+x6jRbHd4gWL5ugHyhWIu8tCvuWlIDD9DMhImSKUxpSTT24nnbj4AMx4vLFe8tnpElO/VIDh5lsAq9GO4mZLXZGs3X1S/IyOF4MTdJL94qyic7+z7BP9f9f+OGSdU+bnmSinyuQLx+Mj/UxX4wMSDSjsqZQvzaDTK3XffzbPPPsszzzzDmjVrAJg2bRrHHnssRx999KQ2UCgUuOWWW/jTn/6EbdvYts0VV1zBMcccs9lr//jHPzJ37lzS6TQA8Xiciy++mOOPP35Se9mR2dCVZVNEIpGSx/v6+shmc1x44bmjjmcyaeLxOOl0Ct/38TyPRCI55vpYLL7ZtfsGu8FV0uEjlRogCAKSybF7SiarCIKAgYEBGhpGhPnG1o1SSoIyCpiOOupojjpq9O/JWWedwpe/fOmoJwAb89Zbb+A4DocffsTwsY3f5x/96L+pr99097pwOMyJJ57MiSeejOu6PPPMU/zkJzdyww3fZfbsjxOPb/4zmCjt7esAaG5uqfjcGo1my+ErxcqOFNnciGirToQQSpF3fQw3h5QeAz3dRF13SqL8pRUBL68cEeW718FnZkGi611i3QvHjO/Z+VN44Tp8u/J/07Y3WpKCA1oEf1+n6Ati/KZwOKeF/jJqjAgm9tn5rovvi5GUDBUUbXk0ZVG2MB/imGOOKUs0l8vVV1/N+++/z/33309NTQ1PPfUUX//617nttts2Kfb//Oc/c8UVV/Af//EfnH9+0Vninnvu4fLLL+eOO+6Y9I2CZnJUV1fT0NDEnDm/HHeM7/uYpsnAwMCYc+l0erPCcCgXemCgco2sEokkUsqScw4M9GMYRknR/lHy4ovPc+CBB48qLt0wor45Vq5cTkdHB4ceOlL8ZFkWJ5zwadatW8Ndd93GypXLhwtdK8krrxRzyyfjJqPRaLZOAmD1+gwD6dFdIi08lBCEhI/wHfxCDqUUmSn8zX5tVcALy0dE+a61cNreklhqBcn2N8aMT9fvQyG586TX2xE5ehfB4k5F3oP1TgQ2qvUU/sSEufI9XFeOTBP4OmA+ASaclNvd3c0TTzwxnM7S29tLd/fk8lRfe+01/vCHP3DFFVdQU1P07jnuuOM44ogj+P73v7/Jax9//HHi8fiwKAc4//zzicVi/Pa3v53UfjST57DDjmDVqhXkcrlRx9955+/85Cc3AmAYBvvssx9LliwaVSicz+dZu3bNZteYPn1nmptbWLhwwajjSim++c1/ZtGiYuTENM3h4wDLl3/A0qWLS84ZDofZf/+PsXjxIrwNGlx4nseSJYvYb78DRqWxbAleeumFKQnbd999hzvv/HnJc0PNhqqrayY9/3j84x/LePzx33DQQYcwe/YRm79Ao9Fs/QhY25mmd2Bs3rHwXYTysJx+ejs6SHVPLI98Y/6+LuDp90dE+YxqOGMfiSkFiXWvjxk/0HwwqS1U6BmKRomWeBq8LRC1RgpBc2qsPXH9+7+j+d1fYKfLqxUqZNKk+gcQYjDPfBKWmDsyExLmt956K8cccwxXXXUVN99cLIpbsmQJRx99ND/96U8nvPjvf/97AI44YvQ/7SOOOILly5fz3nvvjXutYRgEQTAqhWDoe+0O89Hz5S9fimma3HLLj4cFbldXJz/60Q3ssstIX7GLL/4a3d1dwy4mSinuuuu20RXc4yCE4Ior/pXFixfxu989Nnz83nt/SW9v77AVYE1NLaFQiM7O9QDMm3cXL774/LjzXnbZlaRSA8M2iwD33DOPVCrFZZddOYF3ofIsXbqE9vZ1HHXU1CLOCxcu4IEH/mfUzcfChQt4+OH7OPLIT9Dauvkc/3JxHIc//OF3XHnlV9lrr3343vd+WLG5NRrNFkRAe3eW7r7RorwqZmFZEhkUEIGHn5m448rGLO5UPLlkRJRPq4Iz9y2Kchjrsa0QZGv3nPK6kyFeW0e4tgk7sek0y0giQSSR+Ih2NTEOaBHURyGvSjffEyqgeuWzZRVyZlMpstkCnusVDV108eeEKDuV5eGHH+bnP/85J5xwAvvuuy933303ALNnz+bee+/l2muvpbW1lc997nNlL75w4UKqq6vHpArstNNOQLHb6N57713y2i9/+cs8//zz3HzzzVx++eVA8cbB8zzOO++8svcwZcxwsfPmJDGkwA+m2BrLnHhE13VdLrnkS8OuHEPFn6VSJFKpFFdc8VXWrCl6fF944bmcc865nHTSiNd9S0srt98+j9tvv4Wzzz6V6upqTNPinHPOHZU/feCBB/Pd797AnDm38+ijD1FTU8vJJ3+GWbP2YvHihVx44bn8v/93I/fcM4/XXvvL8N5OO+2znH325zn66GO58cafMn/+HObNm0M0GmHmzN340Y/+e7h40zRNrrzyKu65Zx7PPPNnampqOeOMs5kz53b+9KcnAfjhD7/HMcccx2WXXcE+++zLrbfeyZ13/pyzziruta1tOrfccif77FNM73j99Vf52c+KNoy/+c0jrF69iu9853t84xtfZ8mSReRyWS688Fy+853vjboRmSovvfQ8u+++x3CR7WQ49NDZXHrp13nppRd47LFfYxgmuVyWSCTC6aefNarJ0BDPPfcM8+bdRVdXJ/39fVx44blcddU17LffAcNjbrvtFl56qXjD85vfPMILLzyHUgG5XI5ddpnJN7/57xx77PGTcpLRaDRbGQI6+/N09GRRow8TEg5W2CLfsY5wTQP+FPLJAZb3Kn67MBhepykOZ+0rsY2iKJdefkxBoheuQRkffUdnAchYNVkjSahQ/H9qWBZCCDzHwbSs4fz6UKIaFXg4uRyB7w8/1d0akEJw7EzJEwuieEpiirFi2vALGG4G3978zYXr+biuU/z7r0AO3lAFU9U7OwBClfmTceaZZ3LBBRdw+ulF+7yjjjqKF18c8SZetmwZV199NY8++mjZi5944om4rsvTTz896vjLL7/MRRddxNVXX83FF1887vVvv/0211133XAhaktLC9///vc54IADxr1msixY8B6trTpvTfPRctFFX+TjHz+SSy65bEtvZbtj7doV7LNP6Rt/jUYzmnVdGdasTxMMSgYpBSFT4PqKqqCXwAyTXbWMSFWSXP/ApEXn6j6fea8VcAYffNfFBBfPDhMPFYWd3f424RUvj7kuP+0wnLaDJvfiykBKkIYkZEpsy8A0JaZlYYWjyNo2fDOCyPaQXv0Bsm4arheQ71yNXd1IumMNKEV02q6oQhZUgJfux9ko9XNLo5Ri/usF6lJL+WL8pZJjMnuegl+1+SeshmlSs+vetLbU4qd7saobCTwXaVauk/f2StkR8zVr1nDqqaeOe3633XabdK75ZPjzn//MVVddxTe/+U2++MUvAvCrX/2Kr371q9xyyy0ceuihE5qvuzu9yTu5IAjwvMo/jjFN+aHMq9m6mMzn3NXVyaJFC7nqqmv1z8iHQBAEdHamKjZfQ0OiovNptk52tM9ZCEjlPJavHcDzR/4OhUMmhumS801sJ0VAmnQmTzoz8W6eQ3RlFL/6WzAsyhMh+Ny+AuE7sPxtQunVhDIdY65zQ9X0Ve2ByhYmvfbGxKIhCgWXcMgkFrGwTYllCIQQxZuOIEBKE2FH8ZVJ4HiYGMSSVcjqepTvEpMOXrgafIdcKk1vziCqJIG0CUSBvJPG98ZaEYaiEexwhFRPT8VeT7kcPQPmvbkrLbk+jossGHPeTfWSs+o3O480XER/hkjIRjo5VJAGr4AvPvqnGuOxpX6XpRTU1Y1vdjEhVxbXdQmFSrdmTaVSuBN8fFVTU8MHH3ww5viQ/WFtbe2Yc0N85zvfYffdd+fCCy8cPnbhhRfy2GOPcc0114yJwms02xr19Q288MLYAieNRqP5qMg5PivbU6NEOYAlfMzAJSFcUt1dU+5V0JdX3P92QH5Qp0ZMOGc/SVVYYOZ7SHaU7jze3zKbbO0eICdsMlcSISBkGdRVhyEIYZlGsQZq8AmAVd2A8jz8XAqEQMRqQUhQCmWFkYk6PF8hpYWVrMcybGItLbh1Lp15C1XwyTgGkUiChG2R6mzHd12ElJiWiWlaWKEQMhwDPnph3hQX7NskUGPN0wAw3PKEbBAo8F1830MqVXz/lNK2iWVQdvHn/vvvz4033ljSrzmfz/Pd736XAw88cEKL77nnnvT19ZFKjf6gV61aNXy+FF1dXXR1dTFjxowx52bMmMGaNWvo2QJ3mhqNRqPRbC+4QcDy9hTOBl09Q5bEMmXRElF5CCdD4PslI7/lknEUD7wdMOS+aBvwuf0l9TEBgUcotXrca7O1syoiyoWAaNikuS5GS32M2mQYU4JdVYcdryoOEAIZrcaIJbEbd8KwwyjDHn7a7itjOP86CBS+DBEg8WQYQkkaqiLUN9RTlYzimxG8cA22XQx2RuNx4jV12DWNyFAEJbZcbc4nZgii0il5LpRaM3yTskmUQmT7KaQHKJbmKl0EWiZl/zRfccUVfPGLX+S5557jsMMOI5vN8oMf/ICOjg5efvllHMfh/vvvn9DiJ510Evfddx+vvPIKJ5ww0nnwlVdeYcaMGcOFn47jkMlkhi0VE4kEtm2zdu1Y6561a9di2zaxWGzMOY1Go9FoNJvHV4qV7Wny+dGCO2b6CDuM6F2HCIUInMmnrgDkPcUD7wT0DqZbGwLO3EfSkhBY2S5qP/g9Migt+geaDwE5dQEbsg1qE2EiobFzCTuMCgJCda0oN4eSBtJOgGGjCpkxKbAbatZikHjowGAXZKA+GcKN2fSnC/jJGsKJJIVsisCw8O0Ehpue8muaClVhQSjiFA3rB1FCIlSAnevCzPfiRcbPaBiir3M9diREPJQcfGN04Wc5TChiftddd2EYBo888gjZbJZf/OIXPPnkk9TV1TFnzpxxI9zjMXv2bE488URuueWW4a6OzzzzDC+//DLf/va3h8dddtllHH300axeXbxrDoVCfP7zn+fNN9/k8ccfHx73+OOP89e//pVzzjln3JQbjUaj0Wg046MoNhBKZUaipvFosWjPwsX0criZFOmu9aT7J2+N6PqKh98JWD+oQwXF5kF7WOuoWf4n6v/x2LiivGvmyWQa9pv02gCGIamritBSFyspypESrAjSDiEiSWS8DgwbNzDxfYWQkytkVApMKaivCtPYWI1Z3ViMnEuTrCtwQ9VbNGIOYDfvNvz1AqeNHnukM7iV7y17noGuTlzHRRAgtiIXmq2ZCT3/mT17Nk8++STvvfceK1asAGCXXXZhr732mvQGbrrpJm6++WbOOeccbNvGsix+9rOfjercWV9fT3V19ahW6N/61rfYbbfdmDt3LrfddhtKKcLhMP/5n/85IctGjUaj0Wg0I6zrzoxqIGQYgqjh44dMhNuHlALXdVFTaBzjB4pfvxeweoNc5pNmCfaog9oFf0JsIu2hEGvBjTVNem0oFq/WVYUJW8a4DjKGaaHMMMJ3CcwwQihAgq+KAWDTKiurYzyUAjMcpaEqSsbpop8IXgG6+go0RAVCSoQoFpxOJVVoMsja6byzfj8KmTS/zR7MicZijjSKwVGzUP7NWD6bx3E9bB0xL5uy7RI3RWdn53DhZ2tr62ZGb51szpWlvX0FTU07TbnAZWO0K8uOgf6cty6UUnR0rKS5uXIWqDuaW8eOynb9OQtY35dnXWd6lOAMh0xqRArfDJNfuxQpDZz85FNYlFI8tkjx3vqRRT61q+CwaRIz30fD0vFtl30zSveu/wffHt/VYlMIIaiK2dQkQmzq33kyGSVHBFG3MwY+npKj3hMhwPAy+GZsSuJcCJBSQu8K/KppLF+Xpi9VoC7i43W8T7RpGoFTINWx+e7YlSbtKG5/NcAL4BD7fc6PFy2y88md6N35uLLnaZg+nabpxdfh24kpvV+VZJt3ZTn88MO56qqrOPvss8ecu+6661i2bBnt7e2b7Na5LWMYJq7rDBdqaDSabZdi44vKuDhoNNsDQgh6UgXauzLDwikWMQlZEuEVwC0ghMRzXGDyDYSUUvxp2WhR/vGdBIe1CRLtbxDvfKfkdZ4Vp2v3U1HSZpOKehOYhqS2Kkw8YpYM3gopUUphJ2qwapI4jsQLFErIMWJSKcCYWsR8aB6lFEIYGNJgelMc05S4+RzRZBUp4kRto+ihHo6Qy2QAkIaBgA+103ncFhzUKnhttaLdH+lqauYmZq5RKDj4no9QAVIU30/daGh8ys4x7+vr4/rrr+fKK69kYGC0j86dd97J008/jWluv//o4vFq+vo6cZzCVtWtS6PRlI9SCscp0NfXSTxevaW3o9FsFQgBAzmHNetTBIGivjqEZUoiRoAZFJDZbpzUAEFh6g1xXlyh+Ovakf+hB7YIPrGTTyi1elxRnq7fl+5dT0YZoUmLctsyaKqLEg9bpUW5aRJqnI4VjWPUtGLWNKNCxajmeP/yhVHBZjmymFJjCMG0+hjVtUmI1ZArBDjYxGtqsWpbEbIo2xLN04jWbN5PfKrMni4wJaz1ayioosYz3TSGU36Bquf5uJ4PKkAEH96NxPZC2Uq6qqqKT33qUzz66KO88847/PCHP+Swww4bNabSaR5bE5FI0eWlv78L369crpeUsqQFpWb7Qn/OWw+GYZJI1Az/Tms0OzrZgs+q9hSer4opGm6WSCiC4fQgDYNCLkXg+ZDtn9I6r68OeGnFiMrdq0FwStNqahc+i1DjC7ZU8yGTFuRQtEGsr45gSsGwO4phogJ/WHUb4RjEGjDMEJ6wkabT1WjGAAAgAElEQVSNrzbdsChAUom8aaVACmPUTLUxm6xZT9eaFI6SREJxPDOEYRgoKfGtOLgesaoq3EJhSqlFmyJmC/ZvFvx1rWS518Asax0A0Z5Fxc+lHAJFvuBiWwEEHoZVbDKko+alKVuYW5bFD37wA4455hiuv/56LrroIi6++GKuvPJKDGPLVg9/VEQisYr/M9+u8xU1w+jPWaPRbI24QcCK9hSOWwwc2KaB4Q4QDlsU2tuxIhHcglMsPixMvrPmO+0BT/1jRIjNrIHP7CmoW/DUJq/LJWdMSZQnYjZ1VeEx6QFmopYgl8IffApgRBL4vo8yYuX5dAO+X0FhKUfvUCmIhixmtFXR3pnGk3EK7mAKixAUfEnYDmOaEsPKfWjCHIpR87+tU/zd2WlYmMc730FJi3TjAZufQAXk8g4Jg6IziwowJOhYVWnKTmUZ4sQTT+S3v/0thx12GHfccQdf+MIXhhsCaTQajUaj2TbwlWLFujT5gochBTUJi4Tl4qT7MNwMruuS6eubsiPI0i7F7xaPiNi2JJyxj8QKxheTnhUjV7ULA62HjTtmUwgBVfEQDSVEOYCMxJGReLHjZjiGCG/JokQ1mMqy0VEF8ZBBW2OcAhaeMghHoxjhKIVAEkijaKtoRUjW1Q2nuVSaqrBg70bBK4XdWeaOuOEkOv6Kmeve7PX59AC5jlU4+UKxyZDvaVW+CSb1KTY1NTFv3jy+9a1vsWjRIs444wwefXT8KmqNRqPRaDRbD0Ne5els0avcNiUhP4vh5Shkc0g3OyU7xCFW9BVtEYc0Z0MMzt5H0LD6GZoWlm5KmEvuTOess+jb6RgCa+JPqYWA6kSYumRo9MGhLw0DYdoYyXpCLbtitOyBb4RLzPQRMk6jJKUgbBm0NcTBEDjJNqxYFY7r4ymjeJ0VRoTjyA8xe+Hw6YIAyR2pT7HMbRw+Hu1ZvNlrA98nm85QyOdB+Sjf011AN0HZwtz3fdatW0dv74ix/IUXXshDDz1ES0sL1157LY5TuoWrRqPRaDSarYchr/KqmEk8YhExfYSbw89n8D2P3vb2Ka/RnlL877sBQxkf1WH4/L4BCbeLyMCKktes3+NM+nb+FIjJRX+FgJpkhJq4PXzMiiawq+qK30iJGYoQGCE8EcYz4/hB2dkrHwpKMfh6x99EyJTMaErgYlIw4gSBwlcmShhkVQjfiiHlOCk/Faj/q48JZtWDg8WTuZH0FTvTUfYc2WwWAg98F6WF+biU/ZO/2267cc0113DfffeNOj5r1iz+93//lwsuuKDim9NoNBqNRlNBBKzvz9PZW8ytDgmXiC0wc104qT78XGXawXdnFQ+8E+AM1nTGbbhgzxQz//EA9e8/UfIaJSS+nZz0mkOivDpmjzpuRJNIK4S0w4RqGjFqWmELd9YcwzgR8w2xTcm0hjjZwcyiQBoE0iSdBycwkdIgFI0CRevHmqYmrFCIeFVlHKhmTy9KxrV+zfAxw02XfVfjOAG+60Dg6oj5Jii7+POee+4Z95xt23z729/mG9/4RkU2pdFoNBqNprIIAT0pZ9ir3DQlhpvCFwIvmyY/6JE9VQbyigfeDsgN2p2HTThnf8m0jpeRQWkP9EKsiXTDAZOO7hZFeXhQlBeFohmO4jsFsCMQ+Ng1jRCrww8EaitzBBElcsxLETIlM1qSvL9mgCBQBHaYQBVwAojZNlY0jlcoEI7FUNLCMM3i66d3s3NvjtakYFoSVg+EKCiTkPCQgUe0+z2ydXtt9imH4zh4hQKGlBAo2MrujbYWKlopcNVVV1VyOo1Go9FoNBVACOjPuqwe9CoXQMxUOP2dGF6+Yi3fs47i/rcDBgYNXCwJ5+zjs1NhKaFx0h76W2bTM/NknETbpNYcyikfEuXSLPqLm9WNmNE4yrDBtCAUx/PZOnuRlBExHyJiG+zcksD1AnpTxRTiQAlMO4Qyw0QScexoDGXYmKEIwrQ3M2P5HDpNAoLeYCT3v2rda2Xlmjv5Aj0dHcXiz03YY+7ojBsxd10X3/cJh4sFEa+//vpmJ3vrrbcqtzONRqPRaDRTRghI5z1Wtafw/aIor6+yEF6BbC5PKJ6tiDAveIoH3wnoGexDtIvZyanTu2nrWUkoUzpnPdV4INnaWVNaNxkLDeeUC9PEStbhDnQjQnEMIDBDKBUqxtG3Qk0ObDbHfEOUgnjYZHpTnBXtRRteFSiEaeEZYYSQKGkQGBZGKIIyK9cIafd6qApDjx+j2RjxtY/0Li1GzTdDJusQeA5ya7w52koYV5ifdtppDAwM8PTTT2PbNueff/4mGwgVW8puvw2GNBqNRqPZFsk5PivWpXC9Yl5vyDYw870E0iTwfVJdnQRTbO3uBcVCz/bBFPV6meKK5B8x+seft2/aUeRqdp/SuomYTe0G7itSmohIAqOQw5cWKlyzTaQz+xPco1JQFbNpa4yzuj2N6wf4kTjKsJBSooSJkjYYDkoYCCkr4rIjheCQNsGqtXXszdrh43auG+nlCcxNu9t4XkC6p5tkvGaT43ZkxhXm+++/P319fdh28S40Ho9z3XXXjTuRUoof/OAHld+hRqPRaDSaSeF4AcvXDeC4PqYhsUyIGQ5efx/CKoqoqUbLA1W0RFy5QWPQC5vexShsWuy74doprRuLWtQlw2wYEhR2CGGHMWJJfLVl3VYmwmS6YCoFdckQrhfQ3pmhx7EJBYqklMWbLsMCkcB3HaSU+BXyDt+/WTB3+T7ERIGjwkuGj9cv/TVdu51KYEXHvTZQirzrkxwM5m6VaUVbmHGF+Q033DDq+2QyyRlnnLHJyW699dbK7Eqj0Wg0Gs2UcPyiKM8PCuSqiMDAg2yKzMAAhpmd8hpKFZsHLRvsM5MQOf6t7imqCz3jXhMYNvnkDLxI3aTXDYdM6pMRhh0CpQSlMEIxfCyI1m8zonwqqACaaiI4rk93Xx5TSoRhoKRJthCQy/kkwiamZWGHIyggn55aF+qQKZjVEuKh1Yezl7WGOqNYNGx4OWJd75Jq2XRTqGzORagAU7i4qmwPkh2Gsos/n3766YqM0Wg0Go1G8+HiBYqV7Smy+WI03DQEptOP8Av4+QxKKTy3tENKuSileOofinc7RhTwRQ1vUR2UFuWBYdOz86fo2Otc+qcdOel1bcugoSaCsYGCMaMJrFgVIlL0+N7KTFc+XBS01sdIxOzBLHWBMmwCv9jYyQkk4domgrqZhGoaNz1XmRzcJhDAs/m9Rx2P9iyFYNNPShw3wM1mUG6hInvZ3qioK8vXvva1Sk6n0Wg0Go1mgviBYkV7inS2KLyFgJAl8Qa6UbkUQYUE0csrFW+sKUrBT4QW8t+1v2RXb1nJsanGj9Gx17kUkjtPqeGNaUgaayJYGzXTMcJxjHgNgRmZ9NzbMoYQTG+KY5sShCSQFv7gI4OC65M2qujNeDjYmNbUi0GrwoI96uH5wl58q/fzpCm6tMjAwR6n0HcIzw8Y6OyAKdY1bK+M+wyhHBeWjdGuLBqNRqPRbDl8pVjRkSKVKdroSSGojysQioLr4OTz+BUQRG+uCXhheVH4JUWWs2LjawYn0kCmft8pd6A0pKChJlIUnxsiBCIUJbDjVCiNepskZEpa62OkOyTZvIfrFt8MpSA12JXIVRJpGDDFpyUAB7dJFncF5JTNXws78cnQQgASHW/RG6oisOPjXpvNe9Spylh0bm+MK8w358KyMdqVRaPRaDSaLYevFCs7UgykHQwpiIYkkgCZ6kBEkniuVxFnjgUdAX9aNpIrcmb1u6X3Y0ZYv+fnNtt4phyEENRVRYjYJhvaCpqRGMr3UGZ4hxblUBTg0bCJiEfoHqeBq4+BZRqEohEK2dyU1pteBfVR6MrCAqd1WJjbuU4alzxC5x5n4o8jzvOOh+e6iPC2U6D7UTGuMN+cC8vGaFcWjUaj0Wi2DAGKVevT9A82nKkKB4RUFiVMUj09RBNuRUT5sm7FE4tHlNRRydUcKBaNGZepnUWuerfKiHKgJhkiHjVHqzghMOPVBJ6HkhY7VmJ5aZRSxCIhqrHJ5cdGpAME4Xg1XiiOu3rZlGwyhRAc1Cb441LFB24jgRJIMZjlrnyiPYtINR9S8lrXC3DyeeyEYOs1l98yjCvMm5ubN+vCsjFz586d8oY0Go1Go9GUT4BiZUeavsF2m4YhMN00nltAmBYqCMj0929mls2zvFfx6IJgWP9+LN7J2WZp04eBto9Peb0h4jGbqpg9Rr9Jw4RQAmnk8LQoH0EYNNZG6c84ZHOjxXkQKAIrjCdDmKaJ4/uEIhEKuclFz/dtEjz3vqLgW6zxa5hujhT+Snf8OZVS5LM5wlKnmm/MuLeyjz/++IQnm8w1Go1Go9FoJseGolxKQWMCklGLIDtAUMgSFKZuiQiwur/YQMgf1L/N4QIXhEuL8swUO3luSDhkUruRV/kQ0rIRoSgYoRJnd2CkLBaDNiawNsrHDwKFb4bJuwLDspFSEqmpRxqTe7JhG4J9m4ufzlK3efQ2/E0XGWczOe1jXoKKurKcdNJJlZxOo9FoNBrNOBRzytPk8x6NCYiHJfSuwQ4yeE6OQjZDITX1SPm6lOKhdwIGawk5LrqYa6MPYARjhVeuaibpxo9NeU0Ay5Q0VEcwxilfM+I1BEqg7B3TiaUUSgFSFvPNQyatDfFRNbdBoAiMMK4fIENRDMvCt+IY5uSdWg5qLS7wzEbWiYYzsMnrCgUXV1smjmFCzu5BEPDCCy+wdOlS8vn8mPMdHR0V25hGo9FoNJrS+KqYU943UKAmojAyPYSjteRSKZLhCFnXm1L+8BDr04oH3g4YauK5V3g9p4ZfLTm2c7fT8CJT6+Y5hCEF9dURrFKqXEgMO4QIxfD8gArHGLd9hAEU00VqEjaZXJiuvqJm8wNFV18eKQUyFMHMZ/CtKFYohFuYnEiuiwpmVMPyvijf7zuVb1f/FgDTSRU7II1TZ+C6Lm7ewYjpJx4bUrYw7+np4aKLLmLx4sUAY1qpakcWjUaj0Wg+fHylWNWRpi9VQAiwVYHcQB9hw8L3fXrb11Vkne6s4v63A4ZqCMMmnF/zJmyk37xQFdma3SsmyoWAmqowEdsYddywwwSei7RsrGQtyrArst52h5QMJ+QraKmLkc17w82moBg5V6aNEU0ykPOIV7VSZdvks1kK2YmnPx3UJlneF9ARVJMKwiRkHqECDDeDbydKXhP4Prl8nmgiscM76mxI2cL8v/7rv1BKcddddzFjxgw+97nP8fDDDwOwdu1afvSjH3H66ad/aBvVaDQajWZHxw8UK9en6E85WKbEtgyCfC9OoQA96yu2Tl9Ocf/fAwZ7FBEzPP619Q1i6c4xYzt3P70i7itDJGMhkpGxqRVWVT1+th8ZikEorp1YxkFIY1ShrCEFbY1xPljTj+ePnHCVRSgcpZD3CIckthUmnDAnJcx3q4NkCAYKsN5PkpDFCH3j4ofpb5lNtm6vMT72SincbBbluxhS4iuj1NQ7HGX/Jj3//PPceuutfOITn2D69OkYhkFbWxttbW0ceuih3HzzzTzxxBMf5l41Go1Go9lh8QabB2VzHoYhqDWzJEgjvBwoVRTnFWAgr7jv7YCUAwLF7lYH36u6n/r0kjFjc8mdKyrKo2GT2sTY1AYhDQgnkKEooroFZUV14eB4CDnGGzweNmmqi40qok27krwRQ6mijaKSEszwpJaUQnBAS3H2ATU6579q3atYua6S1zmZFG6qF6FzzYcp+7fJ93122mmncc83NTXR3r7pNqwajUaj0WgmjusHLF87QMHxqbPyJKIWXt96nO515NPjdJOZBP15xf/8PaA/D4KAr8Sf5fLEH5CMzjUIhElf25H0TzuyYmtblqS+OjKmQai0QxjRJFgRRCiGH4Af6MY04yLGRp6VgvqqMMnESPqP6wX0pYvpLT4GCAPfjhOOxSa17P7NAgEsdxvGnDMLpQtBvVyG/vXtCLRn4hBlC/NoNEpfX9/w97W1tbz77ki3r2XLlpGu4B8HjUaj0Wg04HgBH6wbIJ11CRs+Kt2JrRw81yGfzeBVoL06QF9ece+gKAc4LPQ++9mrSo4daJ1NrnYPVIWsCg1ZdGAxZVGVCykRhoG0Qtg1zZiJWvxAIUIxLcg3w4bpKhsigLb6OLZVSrgLEJLenMCs3wnDnJA3CACJkGD3enjV2ZVlbtPotcexTnTyeQZ6+/EcHTEfomxhfsABB/Av//IvLFiwAIAjjjiCK6+8krlz5zJ37lwuvfRSPvaxylgkaTQajUazoyME5Byf99f2DzeKsfHIpTOYzgC+VxlBDsWc8nv/VhTltTLFJfGnOTf28phxnp0g1fgxcjW7VWxtIQR1VWHC1ogYtKsbsJN12PVtiGgVygwVUy7k5G39dhQ2leITsiQtDTHkRo8lAiCQZrEjZyAQcnLpSR9rkeRUiFtSJ/JsYZ/h41XrXsXKlHbu8/xiF1BtIlKk7Hf+C1/4AslkkieffBKASy65BIAbb7yRG2+8Ec/zuPrqqz+cXWo0Go1GswMhhGAg5/LB2n7yBR/LENiWROLhuS6Z7vUEfmWsLPpyxUj5jGAl/1H1KN+pfpR97dUlx/bufDzppgMrmldeFbOJRyyGKhalaSGiNQjDIggl8JRJMBiZ98eJBmvKQymoidvUVoU2Oq4IBlNgAiXGCPdy2aUGqgbT1Pv80fnqtSv+DIE35hqlIJfOIKUW5jABV5ZDDjmEQw45ZPj7+vp6Hn/8cV5++WVs2+aggw4iHo9/KJvUaDQajWaHQUBPqsCa9enh6GdVyMMQIJxilHyyntMb05tT3Pf3gGzB55zqV4jL8efNx9vwQlUVWXeIWNSiJhliSJRbsSTCjhBYUaTngDBQ2nmlsihoro+RznnkC0WhHAQKHwvwUBQj5tFEgmwqNaGphRB8rEXw3AeKXDDazlL6DoabxQ8lx1yXSWeoCgIMQ+JX6IZzW2XiSUQbEI1GOf7444e/X7x4MbNmVa4Vr0aj0Wg0OxQC1vfmae/OoJSiPm6QdgUi34ub6UcalbOU6x1MX0k5cJC9clxRnkvuzEDbx4tR6wqmG4Qsg/pkZNgpRBgGRqIelI8XKKQd1vnkHxKmELQ1xvhgzUBRlAcKZ7D+MkBiWCahWHLCwhxgv2bBC8sVWTXWZ14EpdOvHKfYBTRsCoSwd+jPvaLtsr7yla9UcjqNRqPRaHYYhrp5rutKEwQKyzSwCr3EwgZBLkU+kyE7sOk25+XSk1X8z98CUo7iQPsDvhR/YcyYQJrkkjuTaj6YwAxXVJSbhqS+JoIhASEw7DChujZErArMQUFnhrUl4odIMmpRX11MNwkCRXqwjkEhMEyLQFrDueZWqPxmTnFbsHsdZNXYwmDpOyWv8VwPJ+9AITPRl7HdMaGI+csvv8yTTz5JR0cHuVxuzPn+/v6KbUyj0Wg0mh0BISDvBqzqSJPOjggX21A4A33YQLqQr9h6HWnFI28XOMZ8h4OrPqDWGCuGfDNC1+6nFwV5hZFCUFcdJmQOir5YEiNWhbIi+D4YdhyC8d1FNJVBBdBUEyWVdclt2BVUgWHZBIaNFIJASuLVNfR2lC7eLMWBrZIXektFzEsLc9/3yXWvI16bgLBgVIekHYyyhfmcOXP40Y9+hGma1NTUYFljK6MD3VNVo9FoNJqyEQL6sy6rO9I4bjGXIGRJbFNg4eLk8mQHSlsWToY1A4oH3wk43X6dj4eXlRyzfo/P4ltRkFPKdi2JEFCTDBMLFeeWpoVR3VxUidJEKfB2XE32kWNIQWvDSEoLUMzpl8ZwxFwEQbHL6gTYuRr+Yo8V5sm1r9ETrsO3N6pJVIqede0kIyahqh3bo77s37p7772Xa665hvPOOw+7xJsNcNRRR1VsYxqNRqPRbO+09+ZY353FDxRVUUneE1RZLlJ5gCBfIY9ygBW9ioffDQip3LiiPJ/cqWRxXqVIRG2qYkWRJ00LIxQmsGIIL1cUfzuwINtSJCPFlJb1PcVMiEAplGGipIkdCROKRAmkiZASVWYAVgjBjKY4q3tqmGb2Dh833TTVK5+le7fPjLlGKUU+VyAsBFIyfKOwo1F2jrnrulx00UXjinKAn/70pxXZlEaj0Wg02ytCFJsGvb9ugHWdmWLjHAERlSMRUuBkcPs6cfvWV2zNZd3FSLkfBBweKi3Ku2b+H3qnH1OxNTcmGjapSxZTY6Qdwq5tQUYGbwLMUEUtGDXlo1QxpSUcLsZqPT8gLavwfLBsu+iMI00s2yaSSJQ9734tkptTJ7F0o2ZDdq4To1A69TmTKyBFgBynU+iOQNm/BZ/85Cf54IMPNjnmb3/725Q3pNFoNBrN9ooQgr60w7LV/Qyki/m2Ugos08DPprCcAShkyA30kRvo28xs5bFwfcAjCwICFfDVxFOcEn1rzJhU44G4sUaQlXN92RDbMqivjiAEGHYIu7oJkahDhGMopfADscNGSLcGDCloq48hpUApSOV8HF8hhCAwTJQ0sGwbM1r+05SYLZhZb7HIbR1zLjywsuQ1juPiZFIE2b4d1te87FSW66+/nttuu43Gxkb2228/ampqkBt1hpo7dy4XX3xxxTep0Wg0Gs22jq8U7V0ZuvvyWKYgEpJ4PtREIeMK/IEM2d7Oiq751tqA55YV+Ebiz8wwu8ac90JV5JI7k6nfp8TVlcE0JA01EUwpMCMxzKomEOB6YIViqLE9ZzRbgGTMprYqTFfvUEqLKN6oiWJai2FZYEcnNOfHWgQrB8ZmWoT7V5Bp2G/McaUgl0kTcTIY1QGw44nzsoV5JpNh0aJF3Hnnndq+SKPRaDSaMhEC0nmPNZ1psoOWdGFTIVDYUmAUUkTtKhzXKTuHd3MopXj+A8UrqxT/FF5UUpQD9E37BG60oSJrlkIKQf2gA4swDMz66SAM8AcbJWlRvtWgAkVzbZRUxqHg+ChACINAGihhYthhXMNCGgaB75c1507V0F2ibtTOdSL8AsoYbakoBBTSGUKmjxl4ICZWdLo9ULYwv+6663jrrbc47bTTaGxsHOPKopRi3rx5Fd+gRqPRaDTbKgHQ1Zunoyczqp18SLgIIcF38TIDmMIgW6FCTz9Q/G6JYkFHcb1jw++VHJer2gU3Ul+RNUshBNRWh4nHIkg7DCiUtFDCQCjt4rY1YhmC1voYy9cNFIOwQhBIGyUNAmkSCAPDNMsW5kIIorUNkB57Lrn2Vfqnf3LUsXwmi1twCDfEsZWPkBamIXB3IKuesoX5q6++ym9+8xumT58+7phnnnmmIpvSaDQajWZbRghBtuCxtitDKjPau9k0JBSySFPiDPSRT6cQAwMVeRpd8BSPLghY0Rewk9HDGdHXicnR67uhKrr2OHPKa22O6kSY6mQcIxLDappJ0L+eYMh2zwwX71o0WxVKQVXcpjoZJpN1UEKgDAsQBGaIrCOJRmJYkSjSMEl3bz71aqdp9Tz45uHUywGajT72ttcCEO37B5n6ffEitaPG+55HOlsgqXwCBAQe8OHUPmyNlC3MGxsbNynKAR555JEpb0ij0Wg0mm2d9f05OrqzeF6AlIJY2CCd86iOWbi+wu9Pkc2kCPyiOq2EKE8VFA+943MSL3BQ7fKSY9xwLQMth015rc2RiNk0tDZj1bSisr24ro856MCiizy3chS01EVZXvBRykAJg0zOIwhCOJ5HPFEPpoVwMsDmhXnUEvRWzeKl9Yozoq+zN2uHz9mZ9jHCHCCTKRD4HsIC5bkfWlHy1kjZriznn38+DzzwwCbHnHnmh38HrtFoNBrN1ogQgrzr8/66AdZ0pPG8ouiO2JKocIiEDKKFTiKmwivkh0V5JWhPKX7514BkoYODQstLjsnU7U3X7qfhxFsqtm4pomGT2mQYM1aDL22wIsXGQcLWonwbIWRKGmsjIIo3lLmCR8ENUAp6nBA9WYGSdjFfqQwObC2OW+yO/tkznBI5LoDrB+TTqaIzi185L/9tgbIj5r29vfzhD3/goYceYv/996empgax0QeyfPnySu9Po9FoNJqtHgV0bhAl35CQcJF+nrgpyff1YymBWyhUbO2lHQ6/XSI5xFrK55N/GXdcPrlzxdYcj3DIormxGikpNqlRIMJx8CvzVEDz0aAUVMVs+rM2ZEd/bt7gDaWPxDAMfG/zFbzTktAQg4WZNvqCCNWy6PxiFkpbgioFvevXE6trIgh2rArhsoX5rbfeOvz1u+++W3LMxkJdo9FoNJrtGSEgU/BZ25khnXVKjrHwUIUshlcgnU5TyGUrsrZSip4lf+Oowt/4ZPWmx/a3HIYTb67IuuNhmZLmxirCrbuCkwfDRimFF2htsC0i/n/23jzIrqu8233W2tOZhx41WrIVDxhDcC7gmM/YJJgYSMBgcBKGEJIiUED46hJiAnEoqBThgxjIBYqkSOFUUs4lLsDAxcGASQKKzWQMBuPYBg+apZ7P6TPss6e11v1jd7fUUneru6WWW+r9VLns3mcP6/js4bff9b6/F6hWCowFGjix2DM2NrlCgTAISKKFz/25fQnB5VsEdz8Gf996EX9Z+yoA9iKNhgA6bZ+4O40lN1bjqWUL83q9zhe/+MVFPzfGcOONN56WQWVkZGRkZKxnhIBEG8anekw0eyTHOK7kXIsgUvSXHWIjMb1pOhNjcxHj05HCEivDNx6NebP+6aJWz4lTwu+/hO7AZctOOVgtuZzH1l07sRMfbXlYeRctJChDFig/e3Fdh74a+CMnppMoJG6hhJE2STR10n09fUjwnScNU6o0t8yKu2D0gl1fk0TRm25QqtURgg1zHi1bmF977bVs3bp1yXV+93d/95QHlJEiRHrjtTdo56uMjIyMdYuA6W7E4UmfIJg/zS6loJIzGCzceBrpVdFB97SmcbRDwx0PaarBKCzSiDEsbmLqgpectmMuhSUlm7ZvJte/BTM9hkYQa0gTfDLOZhditLIAACAASURBVIyQ9JddppoB/nHnujYCbblId3k6xbMFTx8WPHDYpqM9SjJEYJBJD+0UF9xm7PAYhXJpwc/OVZY9P5AkCe9973v53ve+t+g673znO0/LoDLADxWPH2gy0uhtxMZXGRkZGesOKQWtbsi+kTZ7DrdOEOWQtp63oi7VHETNcayoQxycntQVgINNzb/8RNHp9nhx/meLrter7Tptx1wKKQT9tRz5QoEkMZAvZbnk5xBCWEgBmweLaSHmMWht0LaL8PLL3t/lm9N9TOmjYnv40c9T3/efC4bEw1gRhQkbSQgtO2L+5S9/mZe//OUMDq5dh7CMGQQcGu8QRIrxhk+t5JJzNo5VUEZGRsZ6YjZtZXSqR6gMjdbihZueTJBxgJE23W6XXKlDfJL82+VgjGHPvlGeMf1t/qYULLlur7KDXv3CUz7myRBCMDxYpeBJhJ3mk+PkMRuoGcy5jrEsUFApuPRVPCaaR889pQ1aehgTL7sb6FBJsK0CDV3kPCbnluda+3H8MeLi8PzjG0PXD+iTYl6DrnOZZQvz/v5+/vZv//a0DyAMQz71qU/xrW99C9d1cV2Xd7zjHbzgBS846bZKKf7t3/6NO++8kyiKaDabDAwM8IY3vIGXvexlp32sZ4q2H9PtpflcShka7ZAt/cUsCpGRkZFxphHQ6ESMTKVpK9Xq/Oig51rEiaaUkwghyGmfJOwhLRtjDI2RkVMeQqwM33jM8LvhbipyYVHu1y+iPfxraMsBuexH+6oRQjBQL1DfcUEa6Zw5ZpKJ8nMKYyTGaDCGob4CrW5MFKcCXGlNqB2E1ivqBnr5FsHEvvIJy624Q8zwCcv9Xkgp0VgbJGi+7FSWiy66iLGxsSXX+Yu/+IsVD+Cmm27iO9/5Drfffjt33nknb3vb23jb297G7t27T7rte97zHu6//35uvfVWvvzlL/ONb3yDvr4+fvCDxe2i1j0CRib9eTM6050IlYnyjIyMjDOGEOBHiicPt9h3ZOG0FYCqq6iWXIpxA0/E6KBLr9shak4uuP5KafYMtz2gmRhvUJMLp8TEuT6mt/0vtJM/Y6J82/nbqdWKGOmivQrCWX46Q8bZgzqmUNmzJZsHinNJJcZAqxsTY2HbzrL3efGg4AF1IaNqfoGEFS98fidx6qO+UcxZlv01b775Zj7wgQ8sapUI8N3vfndFB7/vvvv45je/yTve8Q7q9ToAL3zhC7nyyiv50Ic+tOS2d999N7t37+ZDH/oQpVKaq+R5Hu9///t5zWtes6JxrCf8UNE77gEQRglBtLw30YyMjIyM1SOEIFKag+NdnjjQpNWJFnWDcGyJFbdxTEivMY5UMTrokEQxvW73lMfyxITmtp/ElMJR/qJ656LrdYZ+9ZSPtVyEgMGBCtVN27FzJYzloDXEmSXiOY8xUCu5lEvuvOVKs6I8c1sKtg5X+dD09Xyj98y55Va88DVjVELP9xG9xoaw5V72q/Vb3vIWfN/nxhtvxPM8+vr6Tvgf1Gg0VnTwr3/96wBceeWV85ZfeeWV3HLLLTz88MNceumlC257++238+u//usUCoV5y7ds2cKWLVtWNI71gpSCqVaAPu4pYAx0ezFFz9owdkEZGRkZZxptDFOtgLGp3tx0/WJIKXBtgeq2sY2hG4Z4QRsVn3o+udKGHz/Z4erOXVxVWbxwNMoPEJa2nJHGQZCK8nolT61aREkXUepLW6VvkNzfjLQEc3N/kW4vnsv5VsogHQ/H87Bdl1yhQLsxRRIt3rHzWZsFPzggOKKOGvAvJsyDbhdx+HG69TyFbfVz3utn2cJ8dHSUyy+/fMl1TpbqcjyPPPIItVqNSmX+dMZ5550HwKOPPrqgMNda88ADD/D7v//73Hrrrdx11110Oh2GhoZ4zWtew0tf+tIVjWO9EMSK6c7CRUWtbsRwPZ/lmWdkZGScbmbsD0cm/RNmLI+lVLDJe5LYsyl5GhVHBJ0OwvcxxuA3J1HxqbUPbwaGOx/RvEj9iLq3eOpKc9vzSfJ9p3SslSCEYNPWYSpFFywXrQ3YRUwmyjccBc9ioJpndCo9P5XWSM/FKVaQ5QFM3EZKC1j8WqjlBb/SD83po8HVXGs/VtRBuSfaI/babaalorQlRovlp82cjSxbmNdqNW677bYl17nqqqtWdPBGo0GxeKJ35WxqytTUwob1zWYT3/e5/fbbeclLXsJtt92G67p8/vOf553vfCejo6P80R/90YrG0t//1PlkDg6mRRD7RloUCt6C6zi2pFDMkc+tff5gxtow+ztnnLtkv/HZhdKGZivgyGSXXpjgeg6ut/hDv5bTIA2DbhtpuRhb0nXSjFB39t7tLHwPXw4PHUn4/x6KCBLD5X37Fl2v94xX4UmL1R9pZeSrNQqOZmjnDizbxsQhTv3c95bOrufFKZTzaCmIUsN6PFuQ82yUV8CODcZv4cilX9qev0tx+4+qKCOwRLru0C++QPvX3ohxciesr7RG6oDBTX0YY05LWst6/I2XrfJuueWWk67z2c9+9pQGs1yiGespIQR/9Vd/NZfO8trXvpa77rqLT37yk7zmNa8hlzvxh12MyclOGgE4wwwOlhkfbyOk4NCR1pwby/FIIRgvueTdzDbxbGT2d844d8l+47MHIdJ6nrGGz3QnWta9X0pBIfJxyxU642PIfBkhJF1/cevE5RIrw88fG8VMH+FVXovnVp5cdN0oP0A3SIDFI/unE2lZ9Pdvoih6dAONsSXSuCTn+LmeXc9LIwQUXYuJyS7GgFMwWELQDmNKliAKI6JeiLTkop1uh3KGUsHjf+JtPNM9cPSDgz+lO7xwhsbI3j3gljBGE+tT00NP1W8spVgyGLzs4s9j88CNMYyPjzM+Pj4vteKSSy5Z0eDq9TrdBQpkOp0OAH19C0/TzUbZd+7ceUKO+WWXXYbv+zz22GMrGstTTRTrJXMatTH0wjNzI87IyMg4F5FSECaaA2MdHj/YpNEKlx2QybuSuDWJjANUkqCjHjo89cZBox3Dlx9o86Lobn6n8FOe6y0uyrV0md76vFM+5nIQQlCqVBgeKFMqFxH5WprCYiQJ53YqQcbJmS0ErRTTQtDpyKIRecSJQTt53FyOUq1Oecv5WPbCMWAhBM/ZJvh3f74It4PmosdtNbtErUnQC4v9c4EV5UVMTk7y0Y9+lG9961tzgrpYLHLdddfxZ3/2Z/T396/o4JdccgkPPPAA7XabcvnodMKBAwfmPl+IcrnM0NAQeoEfxrLSN6izKRdbCEHbD0/oqnU83SBhoJp7SiL7GRkZGWcrQghipZmY8pmcDoiTkz/Uc55NOScJwoScrdA6IfJ9iLqoJEElnVMakzaGH+w33LvP8Fve47hi8cCMQTB+0StRbhnE2nrGFcplnEIRHYcM7TgfuzOGlg46E+MZx2NgU3+Rbi+Zd035gaLuFYiKQ0RJjFjC5/DSIcF3nqzx6da1vL3yHwBItXgDrSiKmB4fpT9fBHlunpPLvsLHxsZ41atexZe//GXy+TyXXnopl156Kfl8njvuuIMbb7yR8fHxFR38JS95CQDf//735y3//ve/z86dO+cKP6MoOsHx5YUvfCH79u3D9+dHLB599FHy+TwXXrj2Xc9OF1JCojQFZ2nB3QviExxbMjIyMjIWRghQxjDW7PHL/U1GJv1liXKAsh0jE5+CbiM6k9hCo5TCn5qcEebpP6uh0Q740U/38ND+Ntd6D/KSws8WXTcsbaG5/fkor7rmohzALlawcwWGNw+TKxSRuQIbqR16xsooeBYD9flWiQbQls20r0gUS+aC21Lwa1sEHXM09VgmS3e2nZpoEp/ENelsZtlX+Sc+8Qk2b97MF7/4Re655x7uuOMO7rjjDu655x6+8IUvsGnTJj75yU+u6OBXXHEF1113HZ/61KdoNtOpi29/+9t873vf4y//8i/n1nvrW9/KNddcw8GDB+cty+VyfPzjH5+LnH/rW9/iu9/9Lm9/+9vJ58+eZgfaGIyKsE2M6yz+k8SJJs66qmVkZGScFA1MtSMe29/k0FhnWfaHszi2RAQtSGJU0EUFXUhCjNbL7m64EMYY7j+ocR77Nq+wd/P+2pf47cJPl9xm6vzrCGq7Vn3MleIWy/T3V3ELJRQWIl/NZmkzFsUYGKzlyB1jTGEMIGy0Tmd7hBCLprNA2gk0OEaYO2GTwV/cQWHy0QXXTxJF2w85Vy3Nl53Kcu+99/KlL31pwXSVZzzjGXziE5/gVa961YoHcMstt/DJT36S3/u938N1XRzH4dOf/jTXXHPN3DoDAwPUarV5xZzDw8N87nOf46Mf/SjXXnstlmVRLpf58Ic/zCte8YoVj+OpJEk0hD0sDLW8YDxhQb9ypQyJ0jhWVgCakZGRsRCG1F52ZGpp68NjsaSgr2RjjEbqmMTyUGNNrHwZHfZQSYwdnFrqynRguOsXmqDV5GW1kSXGLxAzTs3toaUtik8Hs8V5rudSyDsM9FWwXBeFhdYGbRVOvpOMDY0tBZv7C+w93MKY9AXUWDbaGDQCW0qKA4M0Ro4suH3RFWwfzM1zV7SjFuUjP8Lvu+iEmSJjDEEvohgGOJ6HNueWQl+2MFdKLZlDPjg4uGDO98nwPI+bbrqJm266adF1PvKRjyy4/Pzzz+fTn/70io+53oiTBJIeQtpYcQfHLi0Y3dHGEMYqc2bJyMjIOB4B7V7M6KRPtxevqBlbyZPYUQtt2ejmCHb/TqIkQbUbx6StTK5qWMYYfnLYsHuPIVLw0vyeRdft9l9Ke/hyjJBYcRflVhZd93TgeC7FvkF6zSkGd11EMZpEOB6x3iC9zzNOC8ZAtehSK3s0WiHGgJZ2KtCRSNvCSBshJWYRnfjs7TYcV/csTYJMemjnRFttoxWd8SP0D/ah3epafK2njGUL82KxuGQnzoceemhBT/KMpYljxVSzh5OEaFsiVUDRzbNYw6xemFAveWdVcWtGRkbGWiEkdALF2GSXdnd5dThCQDFnUbAUrdjGiyZJVIz0CkRBQD7poZSa1yxoMcu3pZj0Dd/8RcyW4EmeJl02uw2uy/980fWD8jaMlbpcKG/txYZTKEO+wnDBotzXh26GGGEtPGWbkbEUM4WgHT/GYEBaGKMxCCzbwVguUkrUIsJ8sLhw1NuK/QWFOUbTbbcoFD1cr3pOnbLLFuYvf/nLefOb38yb3vQmrrnmGgYHB4G0KHT37t3ceuutvPa1r12zgZ6r9CJFnCSEzSaluiTyfRzhYckyaoG8Pj9IECK7b2ZkZGxshEjvn2ONHtPtcMH75WLbDZYlYGDqEOW+HWi/h1EJSBuVqDRKt8rCTkgbF9130HDvXsON+R9yZenxZW0X51fmbHaq5MoV+vqruMZDKYNV7CPO8skzVknetRjqKzDe6KGlDaTRc8u2UdI5aUOgVm4zlWB+usvAE//Okae/AeT8TAGpIqIkodloMTywDVvHp+xrvl5YtjB/85vfzM9+9jM+/OEPn5BaYozh6quv5s1vfvNpH+C5ThglGKXBGKbHx9BaU3QL1GsDtLsx4XEpLXGsUMqcs0UPGRkZGUshpSCIFOPNHo1WSLLCSHbRkzj+JIlXIej5eDpGxyFaa0Too5UibDVWPSs52klzyUc7MCSnuTK3tCg3CIy08fsvwdjLb4q3GmzHIYljirUqeUcwMFhDuB7goLXBCDeL+mSsGq0NA5UczU5AGKfnkTEGIW2M5czZJkrLWrCIOtr6bPY+fj87xXHi/PGvMnHh9fNyzbuNSeIoQkiJPz1N2RNgnxvdaJctzB3H4TOf+Qxf/epX+frXv87+/fsB2LFjBy95yUt42ctedlrao24kZiM+gjR/fPZENUmMTAKKnk14XEpLogyx0rh2lgOYkZGxcRBCECaKyamAqWV6kc9iSUHOlfihoihC/OkGubpNHEbkk4BYKaIggCC1aet1Tmx8dzISbfjuPsO+Q1N4KHbamv9d/uai6ys7T3fg6fh9l2CkzZmItpQGhgimJ+nfvpOi6YLtkr7XpM+TLEUy41QRIk1p2XOoBaTF2EZaGGEjZOrQUh0cpDFyYgF0XBjgyPbrGH3iHq7wnphb7oRNvPZBwsp5c8uimWvVaEV3chRvoA+xos4865cVfQ0hBNdffz3XX3/9Wo1nw6G1QQpNcswN0SQRUsc4JkIId14AQ2lNkgnzjIyMDYIQaUBish0w0fCJ4hVGyHMWrmOTD8colOoQRkRhiN2ZTlcIuyilUlG6SmH65JTh7sc0Q8lh/qL6H8vaxu9/Gt3BZ6zqeKvBdl1kocKWokWuWoNOgrZcyLR4xmmmUnDpq3hMNAOMMWjbI9EgpUwLQMXMi+gC19uOGtxr7eQKnpi33A6nCRc4Vq/TpdfpIm2HvlIf5hxoCLqouvvsZz97JsexoRHMj1RonWDiANWaIHecA4sxEK4gUpSRkZFxNiJEOpM40Qr55YEmh8c6KxblniOp0MEWiqA5hUx8dBJjtKbbSoW535wiiaJVjbEdGr78P4ov/DxhOjBznQtPhhEWfv3MNcETQLVWZmjTILliOY2S50ogzo2c3Iz1hdGGob4CrmOlDi2WR5SkKS1SytShZZEZIiEEm7Zt49+6VxKYo509ZdJb8ph+p0uzE50TmRuLRsz/+Z//mTe96U1nciwbFoGeL8yVxoRdVBRgiRPfKIMwQWTOLBkZGecwU52I0SmfYJle5MdSKVj4oaZgJUTNUew+m0gp7CjExPO7CibxIhZYS6CN4f5Dhvv3hrw6/z3+pH4QRyz90pB4VdqDv4qxXBKvgnbW1h+8WKkShQGFao1KOUepmEdIC9xc6jVt59Eqe4ZkrA2eLdnUX2Bkoou2PVSUYHseuUIeNSPMFzv7fmVA8P0DF/KlruC1pe8BUJx8hM7Q5RjLWXAbHYVMNAM8xyLnnN0vnIsK82azyRve8IYV7UwIwb/8y7+c8qA2GiINmc/9HYchSRxjOw45BzQWfnC0UKIXZs4sGRkZ5yACWn7M6GQXv5esKMtCCkEhJ5BCUlTTuF4eiaYXhLgqQCuFDn1Msrro+Cz7G4qv/FzT83v8VfUr5OXyhP34RTec0nFXil3pp6B9Klt24GkfpIMxBukWQacN6zIy1gpjoF72aLRDploRtiWxHA8jBOYkDi1CCK45X/LTR452cBdGM/jLLzF5wYsXtBI1OgGjOTLRZcemMvIsjpwvKsxzuRyvfOUrT7qDBx98kNtvv33OmSVj5UhOLLqZbf0sjaYgNf4xWUdRrJfl1ZuRkZFxViCg00sYnfLp+NHKmgMVbHqhwnMkJf8QujRE3Osi3QTp5tPmQN0WSinCTmvVEY1OZNj9pOHno2mm6xtL9y1blHf7n7aqY66IY6I1jutSrFapSAe8PIQRxnJRyqDPYsGScXYhgC0DRZ442JzpBioxloeRNq7nYVXrdCbHF9x2R13wRGm+S5GV+JRHfkxzx2+esL5WmpwjsMNppqYtBurFs7Z+4pSE+T/8wz/whS98Adu2ede73sUb3/jG0z2+DYFZpFpBa41UIRYGKfJzYlwpTZwYHCu7wWZkZJy9CCHoBDFjU/6ymgOVchYGUneVnIUtwSMgn7OIkQR+F68QYaIAMAg7bdbTaTYBUKvwJk+04f6Dhu/tN1g64vnek1Rkj8vdfSfdNvZqaKdAZ+hZKz7uSijValiuR2dqkvqmTeQJKfdVIbRQxmBm7OpYfX1rRsaqKHgWg/UCYw0fkCi7gMDglSok+T7sVnPRdLJnbi/CwfnLnN7CHXi11ogkJGqOYlkegScpFPOsoiH9U86iwvyLX/ziohsdPnyYd7/73fz4xz9mx44dfPzjH1+0I2jG0ghSe8RebBjrwEARim4quI3WSBVgDNh2gWjGF1RpQ6I0jnV251FlZGRsTIQUdIOEsYZPqxOhl9nUxrM0jg5QbpFyMgG2l/pvt6dwB84jUArT66BUjKUkQq0+bcUYw+OT8J9PaJoB7LJH+N/1u5e1bVDaSlg5D7//klUffyXYhTJCCAbPv4BatYII22gNllvEKMDOpYWemSrPOMMYAwO1HO1uBEh6icAyCsvJE2gLy7EXFeYDtTz+wRwFjtaF2HGH8uEf0h18xrw6DaMUlgoJwgjHKNoTYxiGKRTyZ1093qLCfNOmTQsuv/POO/nrv/5r2u02N9xwA+973/vI5/MLrptxcoQQ/GxPky/dp1EmFeq/daHg8i1p6krid9AIZLEfSF/9tE69zPNkwjwjI+PsQYg02j3W7NFaQbfOWkHgSgNGoTuTFPuqRONdpKtAWiRxSC4J0UoTtBqoJCGJYuxwIYO1kzPRNfzHE5q9jZnjyy5vKf/XsrZNnBKN839rVcddDUIIZK5INS/IV/owOkE4eZQ2zAYLlZGZKM94yrCEYHiggD/loxIDMm2qlWiBI5dw7RaS0S0v4MieJ3ie99jc4tLkw+Sn9zJ28avnOoJqrZFJiEoSTBKRxD2azRaJsKjknbPq9F+2j3mn0+H9738/d911F6VSiY9//OO89KUvXcuxbQiU1nz9p01m63AMcPdjhgv6DNWcoNNskisUsErzz6ooVojC2XWyZWRkbExmm6mNN3s02+GyCg9tS+I5Aj9U5HQP1ZrCKg/Q8328WkSiYnRkEJaNShQy6aUFnrON2oBoge6CSxHEhnv2GX5yyByTnmr449JuPLFwGkxQOQ+/72K81n5cf4zpLVeu6JgrRQiBl88T+D6WbVOvFekbGsBOOmhpIwCslX3vjIy1plJwIMwz1TQ40kILG5UYnEIRy3UR0qI9MXbCdrn+zextDvM8/7F5y63Ex46mSXJ9QDrD1W1Opr7pcYTu+dhOnpEJD2ugSDFvnzV6aVnC/P777+fd7343hw8f5lnPehYf+9jH2Lp161qPbUPwpW8/RiecnwRlgB8eMPzWhWlKi1IKRySk8fSUIFSp3dDZcqZlZGRsOISAINapIG+FJGr5CZ+eI6mYJjJfQ4chKo5wVA8Vx0dFeBwjpUwLPHudVY8z0akY/95+w6w7Y0X4/EHpXi5yTuxQeCzt4V8jydUJy9tWffyVkC+VcCr95Ms+fcPDuDrAWDYYDxBp8xaZCfOM9YXRUK0WGev4SClB2GhtEG4eXdmK7U8suu3zdgju/dnTuMp9ZN7yyuH76Aw9k6i4GYSY6wZK1EUlMTIOKeQMU60eiclRLbhnhWZaUpgrpfjEJz7BrbfeijGGt771rfzpn/4p1iK5zcaYc8Lc/UwRxYqf/vSXvK74ABfaIzwqdnF783IAHhwxPH+nIe8ItFK4aDgmdaUXrbyIKSMjI+NMcCqCvJiTFG2FsSTR6CSF/jwqjgh9PxXjWiOTHsaYeY2B2o3GisdpjOGRccPuPYbpmWe6S8wNhR9xZe7xZe1DuaUVH/dUcCp9lGtlSvk+hFNA+CotmnVyGJ0+g43twVlY9JZxbmPZLkP9MD7VI7FslI5QToHpTkTfEjVzni1ItjyD/z6ouTr3i6PLu0fw9hxB2Tkmdr0MPXMtdqfT5mF2Lu2iLojZfyRm61CJvvL67wGzqDDfs2cPf/7nf87DDz/M8PAwt9xyC895znOW3Nnzn/987r333tM+yHOVPUda/FHpO+yw0yrjK/k5/1Pewc/bfSQanpgyXDacCnPrOGGeJJpEGWT2HpSRkbFOWK0gl1IwUBI0fFILwuYRRHULcRTh6ghtFMaYuYiY35gkXmW3zln2NQzfflIzMhNov9p7hE3WNDudSbZaCzs/HI9f24WRCzc8OV3UhoboNBrYrkvOtegfHsSzBUZIjLQQtosxkMwEyde76MjYuCTKUMm7NJ2QyWZ6LSfCI05COElToIs2Ffh/R6+AgHniHMBKAvr3fIPxC18Bx+SsmyRE6AShYrQpcmCkTZQoBqo5rHUcRF5UmL/yla8kDEO2bt3Ke97zHgB+9KMfLbmzeBUd1DYy3/7xAV5rz38AXF6e5OftNGdqzxRcNjwzE3Fc+ENpQ6I1riXJyMjIeCpZrSAveBYlR9HVNnYwSdkpYxmN3+lSKqYRcpmEyOPuf3NT1qtgvGt49MlR+nr78aLzuNqb4FXFpZ9tsyg7RzJwEY2+yxA6Qdtr273TdhyMV6K6KU95cBO5YBxyBYyOU5cVBDi5k+4nI2M9kL4zGjb3F+n4MWGkmGqlBdoaie04c7NixyOE4KUXSfY/tLDZiB21ybX2E9QumFumlUIkIUm3Ra5UIVaaI+Nd/CBmU38RpTSWJVErmNE7EywqzKMoYvPmzWit+dCHPrSsnXU6q8/x22iMTvk89IuDUJ+//AJ5GLgQgCcb5mh6kEqQMjdnK6aUTguoMmOWjIyMp4i0qFMzOd2jsYggd22JNpAojWUJlDIMVmw6gcYWCpqHKNa3EjbbuGUbMZMzLuM0j7w1Prpor4eV0A4N9+w1PDKS8IHaf1HKh1yb/58V7aMz9CzE9mdh/BBjeac8poVwPQ87V6DXapKv91Oo1KjYEbKQh8SdiZTbICRaG6Sdg6yLZ8ZZhGMJtgwW2Xe4Pde7wEiL4qbtxO0GfnPhtLS+goC6hEXey+sHdtOKfbqDlwGgtcJEPknYo9AvMNow2YHpdkQUaWzXJrcO0w4WFeZ9fX38138tzx5qlquuuuqUB7RReHjvFCVx4tlV9/dxaW6Uh4NhejGMtGFzJfU6rxYdmu0IQ/rmmVkmZmRkPBVIKQgixcR0j6lWSJKcKJxtS5IoTTVvEFoxHTvUrR6B8LDiLhWh0HaR0PcplAMSFUPcm4sAN8ZSh4bVNAU6Fj82/GC/4SeHDf00+ZPyfZTkym0UjZAE5e2spTlwdXAIYVnExU30l8YoDw2TKxQQoUFpg+XmMIA2cq6rYZKJ8oyzDGOgVvRoVyMmZlJasFxiJ4fo+Utuu60qFhXmAJWRHxEVhoiLQ8RhhNZpczFXR0gVIYSHMdALE5rtkE3Vk0kLwAAAIABJREFUtXnBPhUWFebXXXfdine2mm02Ko8fml704fDbxQd5OHgRAPunDZsrAoHG1T6u4xHGaTJhGGWWiRkZGWcOKQVBrJicCpiaDogXEOSQRtL7czGxsZA6gaBFrrCJeHIMr9yPTiKU38Luy6FUgox9jNboKMCRpyfYECaG+w4afnTQYOmQzbLN/135BrZYfvQ98ap0Bi5DqpA43z9XXLYWWLaNLvTh6B7FYo6+Uh3heRgshLDS+7xz9jVLychYCGMMm/qLdHoJQZgw2Y4RQjDgLC2Ug/ouyqMPnJDediwDT36N6c3Pxe+/FBXHIARW0gOVYNt54jjddiVF6WeSRYX5+973vhXvbDXbbFTq5RzdBSLmAJvMOGk4RHC4ld6ELQki6uBZLuFMKn8QJZllYkZGxpozK8inpgImW8Hcg+14PNei4sF0YDDBNJaKEPkKYbuBWxwkSWJ00MFojYoj3KSHVprm6Mhccac8xZTIWKXR8R/sN/QSuMZ7mFcU7l9xoXxQOY/Gjhee0lhOhpASL5cj8H3ytT6cUpUBN4cseMigh5nNI7dnxIrlYbIIecY5gi0FW4eK7DnUSjv4GoPw3CW30U6RyV/5bUZHG/zzvvMoiIi/rp/Yqb565D4QEr//aWAMptdKraeLdWJACkEptz4zDhYU5mEYMjY2xvbt21e94yeffJILLrjg5CtuUK6/vEzwy90LfmaTMCjbjOsKh9vpsl6riWXbONUys4nlQZh51WZkZKwdyxXkliUwBmwJrj9GtbQJ3QghDpCuRxLFeEmPSClUrwsCkjhCJj3gqJOIMWbVqStKGx4cMXx3n6EzY9gyKFvcULx/RfsxQmKEpDX8f61qHMslXyrh5fOofJ2SCShXyrjVPLaxiLVBWjZG2mnDFGmDztJWMs49ynmHwXqe0ck0hUVhYVkWaonmYEl+gP6dAzwt1Dw4srjrd/XwD/D7LgEhaE1O4uZy2OV0v9ISCBUBS78IPBUs+I1s2+btb387b3/721eVnvLud7+bzZs38853vvOUB3iuon767/P+7vZdTL75JFKn4fAL3VHGgwrtMC1agvQBVq4qZoX5bAHoOnb9ycjIOAuZzSGfbC2dsjJLX15jqwDllOhONMl5FZIkotfpEEfhTDFnF6M1Qdid264xOnrKY9XG8PCY4d69htl0VYHhTaVvc5l7cHn7kDZTO68jzvchZl8SrLWzQZRS4tYGkSbGqVTpz5WwhCA2Bm25aTGnnebaG0OWrphx7mJguJ6n24vp+DEJAmFZR6/DBRxaZnnRrwhGO0tfHFbcQbllIK1XcVEIYeE5FiIJgTPbh2A5LOi1Z1kWH/vYx3j/+9/Pu971Lh555JGFVptHFEXcfffdXH/99YyMjPCOd7zjtA/2XELWN8/7O8nV6Qw9c+7vS3JHu2DNRs0BTBIhZ+ZklTbES5y0GRkZGStBCIgSzcHxDo8daDI66S8oygWpeB8uagqeRKqE3vhhZOzPdOb0ESKNgMdhGr72G5Mkp9FS1xjDLyYM/3S/5t8fTUV5VXR5e/lu/p++25YU5a1Nz2Zi1+8Q5QfQ0qa5/QXExSGQNsZy1kyUW3YaC8tV6lAaoF4v018rIqSNFunjWM1ExfVMZ8SMjHMdKQRbB0s4tkQbiZSS6uAQlXrfkts5luDVl0m+Hi4+u+Uc01FUJQk2in67Q//P/hlz/xfX5VvvonMAF154IZ/73Od4z3veww033EClUuH8889nYGCAfD6PbdvEcUyr1WJkZIS9e/eSJAk33ngjN998M7a9ZFPRDY9z2YvQE3uJ9/yEMD9Ir3YBbvdo9GjQPlqZfKRluHggFeMmDrCdCpE2KGUyy8SMjIxT5liXlUYrXDJCXsxZlGRATxQx3XHKxRpCxcRhSDHuopSiMz6K1vOnok/Fe/xYjDHsbcDuvZqRmaDFVmuKq7xf8LzcY8vaR7f/aSBtJnf9Ttor/DQVnC6FtCxKm7aju9NU+ioU+4vYkUYb0qYox1lCZqI8YyNR8Gw2DRSZmvYpFAoYO4cUMbUt25keObRo5LzsCbZe9DT++5c+/aLF091D8z6vHrqXsLzlqL2pTig88u84hx7AANHWi7AufsHafrkVsqR6vuCCC/j85z/P7t27+drXvsZPfvITHnzwwXnm757nsWvXLt7whjdwww03ZHnly0RIm8K1byWaOsyRR9KHiXKONquoyKPTvWPHTNXoKER6EtBoYzLLxIyMjFVzvMtKojTGQMGT2ALaocZ1LMJI0VeYaXOmE5Kpw+QGdpD0unieB6RF6I3REYDTGhmfxQqn6Ywd5psT29jTsjEIbiz8mD7Z4VL38LL3ExWGj3YHFGKmUc8akE4ZIC1JaXAz0pLYxRL1TQM4OiDRBuGk1m1IC2GyZnEZGxdjDP0Vj1hpBHWUXUCETZJcDccZIwoXtzgdrDh0d13BPz2k+R11P7+Rf3juM6kT+vZ+i8kLfhuEQMcRzoGjdSfBg9+keDYJ81muueYarrnmGoC5KHkcx+RyOWq12poOcCOhneLcf5eSJjvtcfYmg4wd1egYFWEdYxMUxZllYkZGxsoQQhAmM0WdMznkUggGi4ZeYuHJGEdHqFyFim4QFMs4SReTRJCr4Pd6VKIWsVYov43jzbh7r9GN6MBUzNMP3sWQCHirDSw9w30Cca5Oa/MVWHGXoLzttI9PWqmv+GzQSghBddMWom4bnSTEhSE2FRNyxVL6IhClLy5aOqnLirAwwsx5k2dkbEhm8s0nTJWJ6ZgB20VLB2FJvHyesNdbdNOd9TSt5YsPPZsfRefz7urX5j5z/XH6nvw67c3PxiSD8zeMV97TYK1Zcb6J4zj09/evxVg2PNqa31r5j0q7eX/zVXQigR8ZCq5AJwrrGB/eMFKZZWJGRsayEEIQKU2j1WOi2SOKNY4jGSpBO7Ew/hQFy0YIj7jdID9YIzwygVuz0GEP1WvjuR5aKbqTYyRRRBgECDG9JuMdnfKpHfxvniuOpIntq6Cx/Zp5bbrXgkK1jpUv0j5ykMqmrTPdOS2s/jp11aCwqYKj/NRdRViImRz2WeOJ7P6dkZEiDBRLBWhOoyyPWIEtbdxqP0yNLkucf+1/yid85vmjuE9+g3Z5/lu9WYfCPJs7W08cZ69Skz7n2+MAc1FzpRQ522DNFIAG0al1xcvIyDj3ESItFh9r9nh8f5PD412iGetDzwKrPULJBR36mF4LoUPiwMeKfVSSoHstdOgTBQHGT0V4HIapoDRmSeeE1XCkGfOTB59EP/kDdoojq96PQRAVhk7jyBY5jrBQXo1cqUjoVIitAo7n0jdQp1qvYAmBEU4qygUIdy37h2ZknN0U3TTf3FgeSaKxvBzKzuPUTn4t76wLbvjV3IKfCaPwnpjf0d7Ep6f25XSSVWiuM3qVHeRb++b+vtAeYU8yxFjHsLMu0ofk5AFK1R1MBxAnBqU1YrXhpIyMjHMWISDRhmY7YmzKJ4yOFmQOlASWjjG2iz/eppBvEemEbtfH6fVIoggrSlMxgulG2gpeKVqTk2s23pG24Z69mquj/+a33f2wim7ZUX4Qv+9inN4EUWnTmnbrLNb7sXJFhJS0Ykm1WCXAol4vUvUM2rbA5DDGYKQLxqA1Mw4sWZQ8I2MhjDH0lz1GEkXsh1iuR6AE2EUc18V2HXqdozm+0rLQKs0eEEKwubx4oMBrzy8ORay/+HQmzNcZrS1X4PqjWEn6FneenVr9HJtn7nc6lGsB03gopUkUOFn9Z0ZGxgyzgny6HTHW6BGER2fWcq4k0WCpgGjiIN7AVpI4IZgaJQlDjNZzhVadibElG32cLsY6hnv3abzp/fy6u59neftXtZ+p836TsLoDgB4Xns4hzmHZNuWBCu2JcaxyH10KFE2XONEUSgX6B6q4lgVJOqMgnPxM6v1RIZ6lrmRknJzBWp5ekEBsoxBgBPliAVEeJAr2zDUjqw4O4083sF0Ht1CmceQQrU3PpjJy8uZiE5v/Fycmvjy1ZMJ8naGdIlPnv5jBx74CwHl2Gp0a7x5zIzcGk0RYMofSacTcsTJlnpGx0ZlNWZnuxIxO+fMEOYAlBTXpk3gFTBASBQG5XhOjNaHvn7C/tXBXOZbxbtqpM26M8Cx3L1eXf7HsbY2w6A5cSlDejtsdxUibsHLeGo42xbJt4vIQ+TBAIYhiQ61aYUc+T831UNJCaxCWC5qjDYMyMjJWhCUEWwZLtKYidGxjtMH2CoROGdt1MFrjeB7KKWDZHQwS5RQRQtAduIywtBU7bFI/sHCXdYB/2HcRf3sGv9NyyIT5U4xYwD838aoYIRFGU5U9XGImfSeNvohZP/MIaQviRKdtmteuSV1GRsY6JxXkMN2JGG34BMF8QW7bkpJriI1N0m7gFBUYhTGG6YmJRfa6dkz6hu/u1TSmGjzL3ceLKw+edJvRS34PIy3qe/8TO2rROO83iIvDAHP/Xksc10XaNpbrMa0cyrUhlHQY6i8wUM1htAHLApNGxGeD4ioT5RkZqybnSOJqlbHDbSxLom2PIDYUC2W8/q0YKTFIrGIVnYRpLrrrEoUhSb6PJFdnOgmoHvnhgvtX67BfwLKF+d69e9m5c+caDmXjYQyYQo1cIUfOMkgpabR6c299dpR2z6jLLqO6xnQItZmaBpNESDcV6VGsIJe9Y2VkbEQM0OikOeRBmMxzLMw5EkuCIxVu6zBufStx2CMJ/BOKzc8EjZ7he3sT2pOTvDj/M55WXZ7/eFDejp7p8zC166XpzfMMjd+ybVSS4FX70dXN2GYaHRqKlRKlcgFheakoBxKdOWRlZJxOjIFyzmHTQJGxKR9tu8SRQnp5fONhCwtLaHpOHyVrmk4i8PJFivU6jZEREAJ/4FKUW6Jv33/O2/d/9J7Or14y8BR9s8VZtpp7/etfz7333ruWY9mQaMulXMxT9TRaG/xeTBAl84R5TfqM6hqT3fnC3BKzziyZZWJGxkbDAK1uxHizh9+LybkWg2XJRNtQzEl6saHihAijQdp02y2quQK9OJ7LzVxrChMPU5h6lASHx+Jh2n7MG73Hsasnd3HR0qVx3gsQRhGWtsz/8AyJciElpc3nETXHwXZo+wn9AxW295WouKn/eKyOzR0/I8PKyNhQGAP9FQ+VaKKQtDNxziLREmMkwrIJkpi8mycIFcVSFSUdHK9BPFMvk3jze+58rvM8fmYu5P88//yn4istybKFebvd5r3vfe+S60gpqdVqPPe5z51rSJSxNJZlUamVEEkMUUCl6M4J81lqM11AJ3zDrv70gaSVwhJpUVYQJbNN5jIyMs5hhABtYLob0emFNKcj+vMJVs7GIcIKA4r5GkX/IPnyJuh20WEHu1RDK01jZPkdMlc9Rh1THvkxXucwdphaKzrAM5mAhV3MTmDy/OtSm0N5ZmcC56LjhTy5+hC9yVFimceuDuE6FpvKRTZvrdBsBjPFaNlNNyPjjGBgqJ5nrAm63UVJD6Ul3W48p390sUDcDUgcD9wCtpefE+bqOHem//W8p/OKSy6nv+KRJOvrOl72XS8MQ77yla8sGpU9NmL7T//0TzznOc/hH//xH8nllnkn3pAISnmHuFCBUh9y6hBuPIYUYl4X0NeWvs99U7uY9I/mo2utsGaq/ONYo7MHREbGOYsQaf3gdCdirOEjtKbKNMXiAHp6hEKhDEIQtZvkByoE7TZ5r4BOenSmp7G63ZMf5FQxmsLUL6ge/sEp7UZbHnF+cM1FebHWh+15tMZHyZdK9DodSsNb0WGPJAoJvX4K9YRASKq1GtWcwbgujuMAAXod5qZmZJzr9FdzdHoxCRo10z9hVv5MNFM3O1+7EGqKXgFoph8KydR5v0lx6lH8voupFDwcx1qX79bLvvN99atf5eabb+YP/uAPuOKKKxgcTNuajo+P88Mf/pA777yTD3zgA+RyOR544AE++tGP8qlPfYqbbrppzQZ/tmOMoZBzaBVrJMZCenkcW2LbksStzFv3InuESf/odK5WGhcFWCitSZTGluvPjzMjI2P1HOuyMtbwMUpj22BLTdxskOvLEwU+ca+D4+UJu20qxSm6cYwaPTzX+OdMpK5UjtxHcfKRVW0blrbQGXwGVtQhKg5jrLWrZp/1PMZ2Ufk+vHwL2b+DUn4KY3moQh7HajEdKmqVGv31GjnXwkQB6yywlpGx4bCE4LzhMgdG28Txwvc1P1QIoah48wPDYXXHnJ1q0bYx67T/y7KF+Uc+8hE++MEPcvHFF89bPjw8zMtf/nIuvvhi/uZv/oa///u/59prr2Xbtm287W1vy4T5SZBSkDDjuGK7SCFwHUm3thMOHc3p/1V3H1/1t8w5s2itkVIDFkoZEgV2psszMs4JjvchV0qjtGHAi8BvIIt12p0OdrSXJIowxhD10mjR9MghjDFz0aS1Jt94HGfsEYrRyt1djLAYv/AVKK9y8pVPE+VN24n9NtLN0dM2pUKBVgyFQg1joJdYDBQLbK+VqBVtlJEkiUHaucz2MCNjHWBLwbahErFq4fcWFufGgJIOUkr0cfdCr1DALtXWbYuvZQvzxx577ARRfiwXX3wxDz/88Nzfl1xyCd0zMX16DjCbAmSkDULgOhYd6TB5/ovp3/MNAHbaEwQ+dGMouWC0Rs5sN+tlDpkyz8g4mxFCkChNoxMy0egRzHTqHCgoYpGDoEl3agIv6GGMmcufPJYzVQTutg+RO/xjitHinUANAnHM4y8obSXJ99Grno+xXIy00fbat6eXUuLkPOIwQlkeulzCJqIXaYpuEWUkkbEp5CRDOY/+fJEEmygBmJl1WPs+SxkZGcvEsSTnb6mw5/Di4jw2Fl6hQBLHJFFEvlIj7LZxKv0ot7Ju/cKXPa5er8cTTzzBrl27Fvz88ccfxz+mQUUURbiue+oj3EhIG+m4uHba1CPxqnMfzRaATnZTYQ6ATpDCQxtDFCuK3no9zTIyMpZCCEGsNI12wESzRzgjyG1LIgTYcQvb0cS9DsYYgm7nKR3vxPgUTz/yLaRY/CUgKG2lsfNaQFCYfAQr7tIZeibG8s7cQGdwcjmcTbtwpg4Qa4tOIqgWS8R+iMoXsbWkWHTpK3vYUmCkyKLjGRnrHEdKLthSZd9Im3Y3OuHzxFjkq31YdhG3eQgGdpCTBxBOjk4ssdbpNb5sJXf11Vfzx3/8x/zJn/wJV111FQMDqffjxMQE99xzD7feeuucE0uSJPzd3/0d27dvX5tRn6No6eL0b8P2H0vTVez8XKOhooxmGg257Kgf02RIltDKZJaJGRlnIUIIIqVptgMmGj3CWCGFwJICy5LU3YDEKhKNThGHYZob/RRhtGZ0ssPgkXt4hhhjsfTMzuAzUXaeoHYBiHQWzx+49AyO9CiO5yEAabv4iUW51EcgJHGimZgOsS1JvlRgsODiWtZcY6CsKVBGxtmBLQU7N5c5ONal0QrmfaaMRFt5WolLvVRjOjRUSnUSYdHtJZQq6/M6X7Ywv/nmm/nDP/xDPvjBD851n5zFGMOFF144Z6f4xje+kQcffJB3vetdp3e05zjGgHHyuJ6HJbskRpzQaGjCPzoLoXpt3Go/icosEzMyziZmU1am/ZA4Sjgy2UPK9L5a8gSW0IAimTiEU6rQCXpodWZyxjGawuSjaDtHULsAozXBwUe5YPqHbIFFBXm37xKiwtCMIH9qi6qElBhjyNUGMYU6VtSiFSkKhRJCSaRMqJY8hvvy5N3UmSELamRknJ1YQrBjuITnWmkTotmGX0oTOjlCP0FVakS+RudzpGm/6zc3bdnCvK+vjzvuuIMvfOELfOc73+HgwYMAbNu2jd/4jd/g1a9+9Vzqyr/+67+uzWg3AEY62F4+fUgrTmg0NNU7apIf9XyqtS7KLRJllokZGesey4IoMQTdLpPtBKUS8ibEkh79eUUnccmLLgIwOqHd7dI7Q7U6Mu6Sax0gN/0kXnc0XXhg97K2TZwSrc3POeO+4wshpKSydSdxuwm2y5RvqJfqJK2Yya6gVLDYta1KKefMRcgzMjLOfjb3py/ah8a7RLEiTjTTXY0BJrsapQyRcWbcWM4BYQ7gui6ve93reN3rXrfg55/5zGd4y1vecloGtlExBiw3h20Joph5jYbqssvDR9P4UUlCODWCXd9FrAxKGSy5Pu1/MjI2MkKkJYR+Y4KxIIfVGcFzPYzW6CShUixC4xCVcj9Ja5I46J6ZCLkxFCcewu0cweseSbuEroBu3yVExSHip6AZ0PG4uRxepU7YahCKPG7FxghBEhomWjGFnMOm/gLlggNZhDwj45zDaKiVXPI5myMTPs12MPfiPZue1uzET+EIl8eid9LXv/71K45833bbbZkwP0WMMQjHw7EtIJnnWFCWAa0AImVwrZkOoEmChSLQgkRnwjwjYz0x26mz5wc0Wz40D2OXBgmaE6gkQQiBm8vjSWhPNxHtFkbrMyYai5MPUxm5f8XbtYeeReJVCSrnnVFBXqjW/3/27jxK8qq+///z3vvZau2u3mcBhgGUGaORqOAkKggGA1+NYn6GJL8vGjAbnpCTk6PRGD3nl0SNhkQTUDEiwiHJNxi3XzQ/1KgMGBYjIBoVEFkGZ++19qrPdu/vj+qpnp7umo2pnp6Z+ziHc7o/S9Wlaqrr9bmfe98XNz9Abc92/EyWsNkgMzSClIo0ikgLa8hJKKcS3AyOSMhkHCZKWYo5rzMKx+ZxyzppGQOekpwxnqdU8Ng906TV7v86DsdSz7+oP/3pT4njeH6Vs0P75Cc/ycxM77JZ1hFwXDy3M2lKu9nu5oJsATDbhIlCZ1uapnhCk6aSRGt8WzLRslYHAXONiKhWJS7vxWhNs1JGVqqLJnG26jVa9c5wNbNCkztVWCU/9QOyc08e0Xmt4hmkXoH62ItXdBz5wJp1tCtzyEye2M2TzecxwxvINybRfoEEhee2mG7ElLIjpKEkMpKh0gATgYuy828s65RTzHrks153xeQTJaD3DOaVSoU/+qM/4mMf+xhKqV6HAZ3Fh2699VbWrl170OOsw2OkM39B1CLdr8e8KOaDecswUZjvMU9TFCkgiSJNzlaotKzjrtyIiJtV2tpHVqZozC4svnM8K6vIpI1X38XAru8g06U10AFqOsAVKZ5IkBhqY+cBhtbAmaTB4LLn9Ivrd4b7pP4A3kgeMDRiyVA2z1xoyOdGSI2gHsKAJ4hDTS1yGB/KMpBzUVLMT+xc0WZblrVKSGCo4DGQ96g1Y6bmmrTDFATI4zxJvZeewbxUKiGl5B3veAcf+chHllRiAdBa8+d//ud86UtfYsOGDdx66619bezB/MZv/AaPPPII3/rWt1i/fv1xa8cxIRTKUfMlExeWlN3XYz6z3zhzYwzCpAjhEsa2ZKJlHS9xklJtRkxXWjRbCSNuE0+0aVTnjnfTOnTC8JNfwYmXr4E+lRb4SusluCOn8/LTJQOijtAJSVBa4YZ2eEGAN7ER1ZimkQraiYfvCNpRSpwfJGkbpusGITrza1quz+kTPoN5zwZyy7K6jOkE9IGsSzFbJEoMzXaCF7ir8m9Ez2B+zTXX8Na3vpXf//3f573vfS8f+MAHFu2Poog//uM/5q677mLTpk3ccsstDA0NHXEDwjDkxhtv5Bvf+Aae5+F5Htdddx0XXXTRYT/Gl7/8ZR555JEjfu5VS3aCuZIC7ew/lKVTo3P/YA5g4hAlM7RDWzLRslbCos+ZgHozZrI2x9RMg5KfEngp4fRetE5J4tUx2civPLtsKJ9KC3yidilnrsnzik2Cgt/phEkprHQTAXBcF+koHD9LPXE7veKJQxQnRPMv5VxbEsediaqB77BuNMNA3ussDmQDuWVZPQgEgSvIBQFaSXS4+oa39ByQ/Lu/+7t4nsdNN93EM888w/vf//7uvnq9ztve9jbuuusuzjvvPG6//fajCuUA73znO7n77ru54447+MpXvsLb3/523v72t3PPPYdXpqvZbPLRj36UV7/61Uf1/KuRkQ6OVAgB6X495iXZwCVhtrn4W0fHEVJKwji1X0iWdYztqy+uRIoUoJTAEQkIaLQTnt5VZe9MDZKEwJGIym7ae58hbDWJw+WHi6wUoWNUVGPgsS8xtOPbi/ZpA39R/jXuzF7Bb55f4JfPlt1Qfjwox0EqRWZ4Amf8HJziMO0oZbopaR4wNjSONYHvcPpEgXNOG2C46KOEsH//LMs6JGMgSTSDuZVfhfhwHHKmYBAEfOpTn+KRRx7hIx/5CLOzs1x11VU8+OCDvOIVr+DWW2+lUDi6npXvfve7fP3rX+e6666jVOrcLr3kkkvYsmULH/zgBw/rMf7xH/+RLVu28IIXvOCo2rA6CZTnIYXAKJ/EzQPgiZQL/CeZbS0u9WXiNkoJ0tSQrNQiJJZ1ChACZDK/+E+7hmzPIaMa7XqdbburVOYqCCAjEzxCMq091CsVkmjp8tB9b2vSBr0QYPOT32f8x//M2E8+TzYpLzn+e+5L+I2XFbj0eeq4BvJ9sqNryJZGMW6G6bpmNg7Q2qC1WVRIxQZyy7KOBaVWZ7GMnq16/etf3/05n89zyy23sHXrVi6//HIee+wxfuVXfoVPfvKTBEGw7DmH46tf/SoAW7ZsWbR9y5YtbNu2jUcfffSg5+/YsYPPfe5z/Mmf/MkRPe9qZwxIP4PjCBCCxsjCRcerg0dJtaayX0ecTmIcUlKtSbT9hrKs50qITk+50m1MfQYZVomnd1DbtY2djz/G9HQZV2hEYxq3/CzJ9HZMdZLm3DRGr/zFcVDZxsRj/8r44//G4M/uZuDxL1HY+8iyi3Q+Ic/h8XN+m3WbXkQxOL6BXEpJtjiAlwmInAIUhjFOpxcrihe/jkHgcMaaIs+zgdyyrJNYzzHmk5OTPPTQQ4t6Zv/gD/6A9773vWzevLk72fLAc47EY489xuDgIMVicdH2008/HYBbbQtsAAAgAElEQVTHH3+czZs39zz/wx/+MFdffTUjIyNH9LyrnTEGkS3h+wGNVp3W0DkUJr+PTENGVJ21qsxsc4TB+WuiJEnwREotlSSpxndW51WgZZ0oJClUp4hrc+g0RVfLzJXr1JoxaarJyiqiVqZer3WDuDLxilVccRt7kWkbr76X/MyPF9qdhmQqz/Q8b+/IyyiOPx+zgusdSCnJDw1RnZ7uTk73s1ncbAGdxOjS6WSiOcqpQyXU7N+JJegE8rGhLANZDyWxY8gtyzqpHbRc4lVXXbVkuzGGRx99lKuvvvo5P/nc3By5XG7J9ny+M3Rjdna257nf+c53+MlPfsLf/d3fPed2AAwP54/J4xyN0dGlQ4GMyROVRwmjGPBJCxPI8rMAjKkK9WScXHahxnwuA4mXwQ88RkeWvqbW8bfc+2ytLiZNQCriub3EcQXpGCrtmGojIk0h8Oc/c1FnBnY2WLzOQy7b/zGLqrKT7NN3Ltsb3kviZGj93JvI+Cv3b1DNr4HhZvOowQmUjnGHJ0hrs8hcCePlUGmbOe0Q58fxE83+1V4DTzE2lKVU9PHd47uq6IHsZ/nUYN/nk99qfI97/rXLZDK87W1vO+wHMsasWLnENE35wAc+wJ/+6Z/iecemcPfMTB19HIaBjI4WmJqqLbtPezkazc6YFekU2BcBxmWV3eWERnPhVq+ZnITCBFMzAhdjSyauMgd7n63Vw4lr4LhEu56h1mgxVw2J4sPrBc9l/e7ntR9k0iY/+X1yM48d1vEGaAbjJKUNtAc3olMP+ti+fYJcjjgMyY2cjtApsXSYq2kG/DyTrYBcME4cC2q1GNdxaUet7rlCQDZwGJtfqVMaTbXcOsizrTz7WT412Pf55He83mMpxUE7g3sG81wuxx/+4R8e0ZN99rOfPaLjS6USzzyz9LZrvd4p6dWr0ssdd9zByMgIr3nNa47o+U400nGRSqJTTeIPdLePqQqPHlCZpVWdI1sctiUTLesoCCFQaQviFu3qLHsmK7TCZHV8joxBJi1Kz96F15pa9pDESBzRuVCfZJjKaReSHxxY9th+Uo6DO3IaQbtCVQfEKQgNYZyisiO0mynNtoH56Zxp1Lno6QRyl7GhDMWs170bsCpef8uyrBXUM5j/0z/90xE/2JGec+655/LII49Qq9UWVXbZvn17d/9yvvOd77Br1y7e8IY3dLdNT3dW1vu93/s9XNfl93//97n88suP9H9hVRHKQTlqPpgvjMMfUTVmD+hEMlqjMESxRttvM8s6bEJ0esrD8l6mJ2epVlur5zNkNMNP3dkzkJd1hlvrF7ItGeOsIbhwg2Gs4LDSA/OElLieh5PJUU99cEdpRXrRXchKY2m9YCEgl3EZK2UpZN0jGp5jWZZ1MuoZzG+88UYuvvhiXvWqVy2ZnNnLmWeeeURPftlll/Gv//qvPPDAA1x66aXd7Q888AAbNmzoTvyMoohGo9EtqXjjjTcu296PfexjfOpTnzrxV/6cJ6TCUQ4xMam78FU7JBvUI2gnhsBZ+CozSYxGk6QaR9oJoJZ1MEKYTklSYGbns0zvnV115Ua9+u5lQ/lt9VeyLRmlorOsG5D87zMl6wdWPtYKKTHGkB0axeRGMMbQbCYc6rKmG8iHshQyNpBblmXt0zOYT09P8+53vxuA8847j4svvpiLLrroiMP3wVxwwQW89rWv5cYbb+T8889ncHCQrVu3cv/993PTTTd1j7v22mt58MEHufPOO0+a0H04hOPhzE960m4Wg0BgKMj2/EJDHmv3u2YySYxRgiQ12MIsltUJ30JIhDCgNamRSCmQaQhG02xF1GamKE+WSVdTKNcpxd3fJTf7+KLNbePy3rk3E+OwpgCXbpBsKHWG4hwP2cEhpBCkmRLTDeAQEVsIyGc9xkoZClmXQyZ4y7KsU0zPYH777bdTq9X49re/zdatW/nkJz/J3/zN33D66adz8cUXc/HFF/OSl7wE+Rx7Zq+//npuuOEGrrzySjzPw3VdPv7xj3PhhRd2jxkZGWFwcHBRzfR9vvzlL3PLLbcsGcryL//yL93qLicqpRTK9YEaCEnq5rpLag/KJrMtl7XFhS9CncSgIEkNuD0e1LJOAUpCqkElTUyzgvBzIATSH0CGFRpTu2kbl1qlQqtWPy61x5fjtOfIzv4EFVYI6ruW7L+1/ioGsg6vOlNyzvDxC+RA5/XMDmLcDLX9xo0vRwpBLtsZQ17IzAdyG8oty7KWEOYwy3ekacrDDz/M1q1b2bp1K9u2bWNgYIBXvOIVXHzxxbzyla887CEvq9FqrMqilGRm2xNM7tgJwNDTd+I39gLwieprGFq7jgvPXLgwyhQHaRTOYGQwYHQgsBOnVhE7w39ldBYFijCtKngBSXmKpD6H9Hy8gRESHMpTk0ztnjzmlYuOtCqLCisM7HyAxB+gVTqboPoz8lP/s+yxU2mBO6JXc+4ZJTaPCeTxDORAfrCEBqL8eqrt3hc1UgoKOZexwSy5jHNShHH7WT412Pf55HfCVWU5kFKK888/n/PPP593vetdbNu2jbvuuoutW7fyrne9C+gMebnkkku47LLLGB8ff+6tP8UZY3D8hbsEqVeA+WC+RpXZ1Vy7+Pg0RgpDO0y7C3lY1qlCCJBxg6QyiY5amDTt3EUC4jCkvH07lVqbOF46CXFF2pe0EUaTKT9Jcc/D3e1+Y/eSISv7+3J4Ps7aTVwxIVAruDCQcl3SOO7+7mcyuNk8adhED65Ha0OtuXwod5SgkPcZHcyQ853O3yL758iyLOuQjnrVhg0bNnDNNddwzTXXUKvVuOeee9i6dSuf+MQnqNfrR1xq0VpKa4O73/CdODMKc08CsMGd4kfNA49PUWha4fEJHpZ1PEmTEE89Sxq2F21vhgmzR1CP/JgzhuzMYwzs/u8jPvWx4MW8ePNmXLWyPeSO55EbmaCyezuZQpE0bOMVBiEzgCiMM9s0y95hdBzJYMFnZCAg46n5VTptIrcsyzpcPYO5Meawxy8WCgVe97rX8brXvY40TalUKsesgac66TidygdaE2XHutvPdvZSrmi0kd3b2ibVKDRRqklSwwp2rlnWcaPkfPCrl0mjhaEkYaIp10Oarfj4DevSKUPbvoHf2H3Ep+5dfzGDpTP60KilHM8liWKCfAHpuCjHIQ0GGBxPiINBHGNoa0U7lsTLlGT1XEmpGDBcDPBdOR/IV6TplmVZJ5WewfyVr3wl99577xE/oFKq58JA1pETjofjKOJIkwSDpCpApW0Kss0GNUmlvZZSpnOs1hpPaNLUEKca35ZmsU5yrtSYVhXdrhNVpsEYUg2VRkitEZMe50mdA7seOOxQ/lQyxsOFS3jJ6S5ZT/W5ZQuEEORG12LCJmRLpNKlljjotsZxRmg0UowRdMaiLL7r4HuKkcEMg3kP31VobWwgtyzLeg56BvM4jnnooYeO+Dbky172sufcKGuBUB7KdYmjGISkPbChOx51s7uDmebiYK4wpKmeDyQ2mFsnLykFurybNGqTNKoA1NsJ5WqbKDm+gTwz91MGd/Tu2Li7vYkvNV/KgGix0Z3CH5rgJc/L8Ep/5YesZAdKxP4g0isy14I40cDBh8MFgcPYYIZizsNVnVrmx2PyvGVZ1smmZzCv1Wq8613vOmQwF0Kwe/dujDEopfjRj350zBt5KpOOg+t6tOkMKI9y491gPqaqPNU0nD08P5RFawQpBocw0mS949Zsy+orIQSiPtntJY9Tw2y1RbOdHPce28LuB8lPL/07eEfzF3k2HmIyHSCh0yO+ZiTHyzbkGcqu/LizIJfDHV5HwwQ0aimGg/d2CyDIOIyVsgxkPZTEjiG3LMs6xnoG86GhIb71rW8d9OS5uTne+973snPnTtauXcv1119/zBt4qhMYnEIJKhUwhsRfKEk5qmo8eMAEUBOHKBnQChOGCr790rROGlJ0Kq9gUoSQxNUZUq2pNWIq9XBVrNqZmfvpsqH8xuqlPJlMdH8/fQBevVGyprjygTw/NIySAp0bZS5yDjkpVgjIBPOBPOchBXYMuWVZVp/0DOaXXXbZQU+87777ePe738309DSvfe1ref/730+hUDjmDTzVSSFJ/AKu5xGHIak30N03ImvMNlP2H7Ki4xDlSVphghD2y9M6MXVW7OyEVq1BCoOo7kF4AUllCmMMjUaD2Uqb9iqpQqRquykuM3zlU7WLu6F8NAcXnSnZOHT8FgdS2QG08plpOwe9mBECchmXsVKWQtbtrulp/6ZYlmX1T89g/ud//ufLbo+iiL/927/ln//5n/F9n7/8y7/kzW9+c98aaIFSTneFVaNcYpXBTVs4QqPbDWBw4eC4jZMRxIkm1eYQC2Rb1uojhUbUpxCOj4nbqOIENMuEc3uRyiGOY6r1kEo9WlIdpK+MwWnPYpRH6mRBKrLTjx60DOKnahfz43g9RR9edaY47osDDa5ZR+IEVEKHJF2+p3zZVToty7KsFXFEdcyfeOIJ3vGOd/DEE09w7rnn8nd/93ecddZZ/WqbRad3SjkuUi1UaTBeHlotAALdoBUPkHE7X/ZJHFFQIeXYI041nrITQK3VTwhQaQhSYloNwpndCKkwWuMLQdqogjE0mm1mKu0Vr0nu13YwtO0b3d9TJ0ucGSaobV/2+Mm0wKdql1CTRV69UfCSdQLnONUvFbIzGLy49jQSr8hcSxAnS18/KQX5bKeHPL9vlU4byi3LslbUYQfz2267jY9+9KOEYchb3vIW3vGOd+B5dnZh/xkcRyKchdc69XLQmgJgUDaZacL6+REuUauNU51CZNeRJDaYW6ufIkU050jqZTAaHUdgDCbtDFEJp3eRaijXQ6qNaEXmTYikzcCu/yZTeXr5NidNVK257L7JtMCHa1fw0nWCl58uCJyVC+T7r/irHAc/m8XJ5FBC0nYHqDTNkuErUgoKuU4gzwU2kFuWZR1Phwzmk5OT/Nmf/Rn3338/pVKJG264gQsvvHAl2mbR6TF31AHB3M11fy7JBrMtw/qBhS9/HbVROQhjTdZf0eZa1pFrVWhP7+wMJj+QgFaYrngv+cDO+8lUnz2qcx/yt/B7myTFFSx9KITA9X0yxUEaczNkB0sINwDlkEqf1Amot9JFoXz/QJ4P3E6gt4HcsizruDpoMP/GN77B+973PsrlMr/0S7/Ehz/8YUZGRnoe/+lPf5rf+Z3fOeaNPNVJKRCO2/1d7xfMB2STHQd03KVJgiNSW5nFWrWE6IwlNwaS6tSyoXyle8n3cRt7jiiUb0tGeCoe5+n8eVxwhsN5uZUdsuJlMmSLgxjHJ3Uy5LwMWnnUtY+rJM1QEzWj7qTNfYF8vJQlNx/I7d8Iy7Ks1eGgkz+/+MUv4jgOf/qnf8o111xzyAe77bbbbDDvA4FAub17zH/QXPylqpMEj5RWGNvKLNaqJE2KaMwhXJ8obC/eeZx6yQFk3GDk6a8uu28mOJ3bKi8nF83wxuxDTKgKd7c38R31Mi5+nuTKtQGNZtj3NgohyBaKOEEWncbIIEvsD5EaQaoNRvkkSUozTNl/pU4pBYWsy9jQQg+5DeSWZVmrS89g/oUvfAGAUqnE3Xffzd13333QBzLGUKlUjmnjrA5HgVALb1XiLZSlXKPKzB7YY56mOGjqsSZJDcdpzpllLSEEKB1iwgbR3B4MBsxCb/nx6iWXcZPs7BMUJh9ZtL0x9Dx2FF7Mf/4s4Mld+7au47HKOgo+XHSm4C1jYuVKHwpBtlhADk6QSge0JlUOU9W45ylSCPL7esjnJ3XaQG5ZlrU69Qzm+Xy+Z8nE5RhjePzxx49Jo6zFlBQgJEJKjNYk/iBGSITRjKg6cdgm1dnOcfukEVr7xKnGd+wEUGt1kDommXyWNA67kzuB49pL7rRmGH3yy0u214sb+VJrC997yrD/avOuhAtOE1xwmsBVK3fVK5UiyGURpdNopA5JbEi1JNXLh2whIJ/1GB/Kkg/m/9TbPG5ZlrWq9QzmQRBwxRVXHNGDfeQjH3nODbKWUlJipEJKSao1SEXil3DbMwBclfs25favMJxdOEfHEUZBnNhgbh1/rtQkRkGrQtJuLNp3vHrJ98lP/mDZ7f+4++f4Wbi4PT83LrjwTEFhBSd2AmRyeYJCkdTLMxsqorj3okr7FgYaH+osDGTDuGVZ1omjZzD/7Gc/e8QPdjTnWIdHOQqpJPs6GcP8mm4wP8uZ5P5GynB24e00cYhyBe0ooZBx7Dhza8XsP69BiPkSfvUZnOIYUXVm0bGt6Pj0ku+jwsqSiZ73pS/k3yovhv2W51o/AK85SzJRWPlxYcXRMYSfI1YB9dQ7aCjPZhzGh7IUs16n9fZzb1mWdULpGczXrVt3xA92NOdYh7ZvkSEhF3q+a+O/QH76RwA4QhPXyzC6UDFHxyFKQqMdMzaYsWNKrRWjTIKWncmFsjmNcHyS+hwibpOGnYWxUgOVWkhlpXvJjSEz9wR+bSdxdpTinocW7f5JvIbP136efaG86MPFZ0meP8LKjSPfj1IKkx1GG8Nc5BD3COVB4DBeyjKY9+xqv5ZlWSewI1r50zpeDI7rIOXC6p9IxW7vDNZEnd4+pzkD7BfM007JxHZbrOyy5dYpqxNcDbRrEJSQaUg8uwejU4zW0OoMYWnHnV7yMFqBXnJjUFGV1CuAkPi1HQzuvB9gSU/5TbVLeDxeCwiUgJefLnj5cRhHbrTGGIMQgsL4WurGJdEQL3NXwfcUY0OdQO5IYe+MWZZlneBsMD8BGANKOYtqmQOkmSGYD+Z+NLd4X5rioWmlmsiuAGr1iSLFCIlBoprT4LjoZhXl59HtGjpZqBaiDVQaEZVauDIXizph6Nlv4dc75VRqY+eRP6Dqyj5lne2G8rOHO8NWBjP9D+T7r9QJkC2NINIYYwzKD2g7BarNpYHcdSQjgxmGBgI81QnkNpRblmWd+GwwP0FIKZDu4mU8vXwR5itU5nWNVJtuZRadJLjECKOIUxvMrT5JIwQglEtU3ouOIzAGF4HuTvIUhEmnl7wd9h4ffSyINMIIidecJlN+shvKgSWlEPfRBr7a/HlKGcFrzpKcNbwyPeRBLocT5IhbdUyaIh0HkRtCpiFaeaRC0U4U+9ciV1JQKvqMlbIEnkJrYwO5ZVnWScQG8xOEFGbRIkMAIlPs/jyiaky2YHR+7SFjDOH0TrLDG2m1E/K+nQBqHVtSCkQUg0kxYR0dLSyuE+83ybPaipirhqTp0tU9j6Vg7ilKO7592Mc/Ep7BHc0tKCF46Rkeb1svcPpY9N8LAqJ2ZzElx/PwBkcRUiGzRYRJMUIxF7sUAp9ae35V1PnXTAgo5j0mhnJkfYUxoHuUSbQsy7JOXDaYnyCkkEhvcY956u0XzGWNH9dTRnMLb2kUhuREbCeAWseco1uk5SnisI2O2ixX/iPVMFNt02hF/b8o1AmDO+87vEMN/EXl1yjrHOeOCi7eKCgG/e0ld32PzNAYqjqHE2SRjktVDuIrTWokUgJJRBilRHG66PXKZhzWDOcoZDpD2ezH2LIs6+TVM5h/7GMf6/78h3/4hyvSGKs3RwmE8lCOQ5p0hgMY5dEmIKCNIzTNWgPGB7rnGK0hCQnJkmpjqzVYz4kQAiU0qRGYVp24MtPz2FaUMlNuESX97SXfJ1PZhjDLTyb9YfASHpgepCRqXBQ8xr3h89Fujjefs3LDVhwvIPULOMM5tHQwOqHZiGkKMCZFSTE/uVt3g7fvKcaHsgwWfOxANMuyrFNDz2B+8803c/nll69kW6yDkELQEhn8Yonm7FR3e9MrEUS7O8e0ZoCBReeZOMRIQ5Sk+I7Cso6WEEDUQHh50nat53HlRkS5ujITPJ12mYGd9+M19y7aHuYmqIoCD84N8fXZ53e33xeey0vXC35ng8BboWorjuciBydoJg5JqokSg+u4GNLujYZUm+4Kno4jGRkIGBnM4CpbacWyLOtU0jOYFwoF/vqv/7r7+1VXXbWkju/tt9/ev5ZZSygpEH5m0bYkMwzzwTwfzQAbF+03SYz2zPwKoDaYW0dP6ASiFsrxSdrNJftjbZgtt2i0+zvBcx+3OcXQM19D6sXPt2fD5Xxj7ygP7VycaMfzcNnzVmaRoH3VVrIDJdxslrIOaO038XW5UpFSCgYLPuNDWTJ2YqdlWdYp6bDHmL/pTW/CGMNf//Vf8573vKefbbKWYYzBdSSps3icuSoMdyuznC730E4MgbMQPEwSoaSg0UooZDw7ztw6ejohqZdRabKoDCJAM0yYKbeJ+zzBcx+/up2hZ7+5ZHvFX8NNj45Qbi/8O3ckvHKD4GXrBXKFFgkqjk2QtJuogVESJyBq9H5dhIB81mNiKEs+01mYyU7stCzLOjUddjC/4oorAPjIRz7S/dlaOcaA6ygiFFJJ9HwAigtrSY1ACcOZzjQ/KFcJRhaGs2idINHUmhETw1m7RLd12KQUaG1QJIiwiolCknaDpFXvHqMNlOshlfrKruBZ2Pu9Zbd/dvpcyvtdM5xZgteeszI1yfdRjoMOBnC8LLNJQNzSPYf1ZAKH8aEsA7nOip32wtmyLOvUdkyrsvzwhz/khS984bF8SGs/niupIJHKQacRAMYJ2CHXcYbZAYBT2Q77B/NU44iUZgxxkuJIO43MOjyKBI1CJG3ae3+2pBxInBqmKy1aKzR0RaQxg9vvwW1No5LWon13xy/ie811PJuOAuAruORswQvHxZIheP3mBRlilcHgEraWn5DquYqxoQxDBR9lV+y0LMuy5h3TYH7ttddy7733HsuHtPajpMCITo/5/sqZ0zij2QnmA60dwM919+n5FUDjRBDGGse3wdw6NCEEplVFZYfRtfoBoVxQb8fMVtrdOtt9YQyYFBAU9j5MfvrHyx52S+1C/ic+o/v7WUPwK8+TFPyVCeQDYxO061UgxXFdnNIEU/V42WMdJRkeCBgtZXCVxBg7jtyyLMta0DOYa63ZvXv3klurvbZ3xkWuzPjSU5WSEiMUjlw8iVMPrIP5uXjDeobyfvvSJEGhMUbRaMfkA7vQkHVoQoAJGwg3Q1Kf7W43BmZrbaqNPg9dMZrhp7+G25oCRM9SiB+v/jJPJGuAlekll0ohpCSNO8FbCIEJing6JZMNcFJJ03hLxohLKRjI+4wPZch4nQWC7LAVy7Is60A9g/ns7CwXX3zxku3GmGW3W/3nyE7gEI4PNLrbSwM5ol0KT6RkRMR01Mbxgu5+kUZI6VFrxIyXsnaFEmsJ15EkqUYIgWzNgpchbtQwtXJ3omecGqbKLdphn4auGA0Iguo2CnsfwQkrBz18Ki3w02QCgI1DnYor/e4lL45NgBCUd+1AOQ6u7xOrLG5G08oNkzQbhIkCOq+RAHJZl4n5BYJsD7llWZZ1MD2Due/7R1TH3BjD1772tWPSKGt5SgkQAuF6i7b7rqRi8oyKTpCpVeuURhaCuY7auF6RdpQQxRp3heo3WycOEzWRbgaSkGhmF0ZrTLoQwJthwnS5f0NX8nseJj/9IzAGcRgzlCs6wxea56Ok4JKzBC9e04deciHwfJ+o3QbAcV0SfwCVhhTXn4UQnbuEM82UfLaAiVJqrc6Ea+gsEDQxnKOU73xebQ+5ZVmWdSiHXcf8cNjx5f3nOkuDOUBT5cF0gnmrVqM0MtLdF9VmGRgrMtV2CeMEV7kr1l7rBBG3QLqIJOz0kO8XIsuNiLlq2LdgKZMWhan/OeRxTyXj/HvjF3g2HQEE43m4epNkONufC03P9wmGxhBz0wRDY+i4TSVyKGQ8qi1NIFMcmZKkmnItYmAgg9amM458sLNAkGcXCLIsy7KOQM9gfv311x/xg1199dXPqTHWwRljcJVCO0uDdermoVOoZVE5O4A4jHBr0wTZ9VQbsa1nbnUpCdoITByBqJM2a91QnmqYrrZoNJefyHgs5KZ+SHHPQ4d17L/WtzCliwC8/DTBKzcIlOzf3R8nyKL9Av5oQM1kyWZCwkZKe35xIOMqpJBA5y6CEDBY9JmYXyCoM468b82zLMuyTkI9S3Rs2bLliB/s1ltvfU6NsQ7OmE6ZNYNactteZQoLP0dLl0tP2k1cqak1IrSxk3RPZY7qDIuSUiBqk6iwTBo2iad3zE/0FLTjlN0zjb6EcqFjMrM/JTvzeM9QrpXHtuFX8Gh6OtoIvtl6AVO6SNGH3/x5yUUbZV9DOYDKl2gmDpXUp9GKKUfOokE2UZzSjjqfpWzGYcOaIhsmCgSusoHcsizLOirHpFzivvHljUbj0Adbz4nnSupIpFKkycIY4CBf6K4AmtV1Em1w9gsuOk1RaOoxtCNNxlMHPrR1ihA6AaMxwiWuzpBG7UX7a62Y2UpI2qcqS/k9j5CfWb70IYBBcE/wy3z5yWG0ObOz8A6CTaOC154jCNw+B3LHwXFdQgLqrYXPWBQvfT08VzI2lGWoEDBayjI1tfSi2LIsy7IO10GD+cMPP8xnPvMZnnnmGUqlEpdffjm/9Vu/1e2tbbfbfOELX+C2225jx44dKGXDXr/tK5kolWS/uXnI/XrMh2SdHTVYt7DOEGma4osUrQW1VkTWz9hevVOUiSNIQ6TjEScLPeKdUogh1UbY138bvUJ5lB2l4Y+zdXaCe7YPz28VuAouPUfwgrH+LhYkpEQpRWbtRlTcYNY47KuuciApBUNFn7GhLL6j7NAwy7Is65joGcz/53/+h7e85S2k6UL94O9973tUKhWuvfZabr75Zj7zmc9QqVSQUvLGN76Ra6+9dkUafSpTErR0kAdcBKVevvvzhKrw+OwkDIx3txmtkaSAQ7kWMjaYWakmW6uEK9LOaGgTE83sQigXozuf71gbpssrsIpnjwC79/lv5ulGjq88rmlEC9vXFuBXN0kGM/3tJRdCUBxfQ9pu0tABge8TNZbWTt9X/nDNSI58xsFoW23FsizLOnZ6BvMbbq4H2LMAACAASURBVLiBiYkJrrvuOjZt2oQxhh//+Mf8y7/8C+Vymdtvvx3HcXjjG9/I29/+dk477bSVbPcpy1ESjcILCtBcWJbcKJ9I+HgmBGBT/b/R/Oqic03cxlEB7TClFaVkXHuH41RiWhWSyiQqyKPjCOJOAm5FKdPlFnHSp7kHOgEEXnOSgR1LKzfFXpG7d2a5/2eLn3/LaYJX9HmC5z6u75NmhlCOTytMabTMkqKN+8ofDua9zvAaO1XDsizLOsZ6BvPHH3+cm2++mU2bNnW3nXvuuZxxxhlcddVVbN68meuvv56zzjprRRpqdSgpMEajckWYnVy0b2bwBayZ+x4A48ww2ZojzZS6+3W7iZMbph0mlGsh2eGc7e07yUkpUEKTGknaqpO2W6TthQu6SjNmrtpeslLlsaLCCiNP/X+INKRXvL6vdTb371l4/qwLrz9XcubQytXbd7N5GonCmDypXtxTrqRgaCBgfCiLa8sfWpZlWX3UsypLu91eFMr3Oe+881BK8fGPf9yG8uMk8BwiXBx3cdlEs+5FPBGv6f4e12YW7U/DFp7odPOVayFJnyb3WauLac4hhEaHze62VMNUpc1spdW3UA5Q2PM95EFC+b3hJv69srn7+xmDcM1L+x/KleOAEPjZbGe100yRViuh2V4I5QLIZz02njbIutEcjrSh3LIsy+qvnsE8CIJltyulKBaLrFmzZsk+Wy6x/4wx+J4iMkuDuRCCqrPQQ96uL64QEUcRgYxwHUkYpdT7WJ/aOr72zZEUOiatlxH12W71lSjR7JltUGtExzxo+tXt5Pd+H5m0kUmbTHXbssc9o87k/ym/ic81XoZGIoBXbRBc+SJJ3utzT7kQ5MfXky3k8UfWkx8aJpE+er8Xw3MVp00UOGtdkZynOIzFSC3LsizrOTuqcolSLp/nb7nlFrvIUJ/tq2WeaMh4GWg2F+0XQRHC+WNbi4O5TlOcpEkpUEzWBVPlFgN5z4aOk5Bj4u7EzqTVIJlfOKjRTpiptEnSY3+3REU1Ss9+C4GhMPlIz+O+FL+Ku2c3dH8v+J0JnqcN9DeQO55HmiS4nkfiD+LphAY+fn6YVuIC6QHVVqTtIbcsy7JWVM9gXqvVeM973rPsGOR6vc6f/dmfLbvd6j9XdQKD8Jbe1cgUFoK5lyytqVzds5P82g0IkaHZSqi3EvLBMSlnb60CrjLEqUBHTeKp7ahMHjNfV3OuHlGuhX2bVxBUnkEc4iqvZVweqK3r/n72MPyv50syfaxN7mUyYAyZ4TF0u0maahqRwXGHaISapvBI0pRsxmHtSJ5Cdl+1lb41ybIsy7KW1TORhWHIF7/4xZ4nfulLX1qyrZ81hq0FSgkEBul6S/YVBwow3fl5mDJ7Ik3WW7jDobVGxxFK5khSzeRck/zaou01PwkIAaY+g5MbIW3V0UmMrs2Rapiuto75Kp5efReZuSdplc5GRQ386o5DnvP11osIcZECXr1R8NJ1/alNLoSYf0EMweh6RBJiVIDIOMg0JYxSmvNj64UjWDOaZ6QYIIWttmJZlmUdPz2DealU4vOf//xhP5Axhl//9V8/Jo2yDs6VshM85NK3T2UKtIxHRkTkZMTkdIUNa0uLjtFxhPQFpFBrRFQbEcXs0pBvnRikFAiTIpRD2qxB2CSpVwBBmKRMzbWI4qU1uZ8Toxncfg8qaZMtP3XIw+9rP4/7wnPYmQ4xGMAbN0smCv25kBdCkCsO4GZz1GenaRsPz3OJUwHCQbqGNDYIoJD3WDuSI+s7fZ0Ea1mWZVmHo2cwf8ELXsC6det67V7W5s2bD32Q9ZwpJXBdSaoVUin0fotAIQRzziiZdCcAcWUaDgjmJglR8wu2GAO7Z5rkMi7K3vE4IQmdYOZ2IPJD6CQkrbdAQL2dMNun8eQybqKS9iGP+4/2S7m3dTYt07nwO3sYXneuJHD682/NCwKCYok0TUndHG6mSUMraqFGG4OjOhNNXUcwMZJjqOAjwIZyy7Isa1XoGcw//elPH/GDHc051tEJPIe41VlCfFEwB3R+DCqdYD7U3g6cs3h/EqPEQhBptROm5lpMDGftkJYTiJSgNYg0JKqXUUmIjiMMUK6GlOtRX8aTO+0yQeWZQx4XGod7W2fRMp0FeS48U3DBaf0ZurKPWxxCZwZx0jYNAjKFEcJQd4N3khiKBY+1I3kCV9k6/pZlWdaqctxn/YVhyI033sg3vvENPM/D8zyuu+46LrroooOe98ADD/Bv//ZvPProo3ieR5qm/OIv/iLXXnstw8PDK9P442RfycRWS+E6CqLF+/3BUah0fv4551m2Tz+LM3JGd3+aJLiisxrjPlNzLXJZl2LGtZPeThAyaqAcDx21MFqTNBt9G08OgE5wwgojT36l5yTPRAWQRjwdj3FX+wW0jE/WhTdskpxR6tPQFSnJ5HM0qzWkn6UcKQI3T62V0nYy6PnqNK4jF/WS21BuWZZlrTY9g3kURXzuc5/rHOQ4XHnlld19r3/962keUKZv48aN3HzzzUfcgHe+8508/fTT3HHHHZRKJb71rW/x9re/nZtuuokLL7xw2XPCMOS3f/u3ueqqq/iP//gPXNdlx44dXH311WzdupV///d/J5/PH3FbThTGdHrMUyPxlQu0Fu1Pc6OLfvennyA9IJgHpBSzAc1Qk6SaVBt27Kmzcf0AvtOzvL21SggBJm6TVCa7CweFie7PeHIgKD/F4I77EKb3Y08WX8DH9/wC5fZCAF9fhDdslhT8/vWSO66LWxjGaYckuISxJpy/Ltn3WhRyHutGc2Q8xwZyy7Isa9XqmcD+67/+i7/6q7/iQx/6EPfdd9+ifbt27cIYs+i/e++9l+985ztH9OTf/e53+frXv851111HqdQZB33JJZewZcsWPvjBDx70XN/3eec734k7v8jO+vXr+d3f/V127NjBN7/5zSNqx4nIUZ0+S+EuLZlolEddFru/+3Fl8X6tUTokp6t4+4XwME55dk+VuA9jkq1jQwiBQ4TSbUwak9TL6DiiESbsnWn2JZQDDOy8/6ChvOIM8/c/e9GiUP6y9YLf/Pn+hnIAqSSplyeTzxEe0NeglGDtaJ4z1xbt0BXLsixr1evZY37PPfewdu1aPv3pT7Nx48ZF+zKZDHfdddeibddeey133nknL3/5yw/7yb/61a8CsGXLlkXbt2zZwvXXX8+jjz667IRS3/d56KGH8LzFlUTGx8cBqFQqS8452TiqE3ZUtoiqzpImyaL9PzvjdWx+5v8AkDd1KknaGfYyr7b7Z3hBgDOcXXRes5XwzO4qZ64p4irbc77aCGFIZ3Z0bpuYzr+BciNmrtrua+iUOll2e+IW+Hcu4b69OVI6/748BZc/X3LuaH8DuRCC7MAAxhhi4RPkhoj2a2YmcFg/licfOHZ4lmVZlnVC6Jm8fvjDH/Ke97xnSSjv5a1vfSs/+MEPjujJH3vsMQYHBykWi4u2n3766QA8/vjjPc89MJQDPPNMZ0LaBRdccETtOBE5SiIEVHWWoDC4ZH8p71MxndCthGHP9AGrgGpNHEV4LO0FbbYSntpZpRWltjb9KuGIFIcIkYakrSZxvUq7XmG60mau2upfKDcG2aP6yuMb/2+ub7yBb+8tdkP5SBbe+gv9DeVSdZ4ryOdh6HS8XIFmO6ZGnijWCAEjgwFnrxsg59tQblmWZZ04evaY79ixg1e96lXL7nv/+9+/ZNtLX/pSJicnj+jJ5+bmyOVyS7bvGx8+Ozt72I8VxzGf+9zneOMb38i55557RO0AGB4+fmPSR0cLR3XebCOmUo/I5ccgXLrKZ9sdgKQz/rhaqZLbOL7kmJwTI0cHkBha0eIhLNO1kAnPYXwoh7K958/Z0b7Pxhh0u048vRsjFH7WoRVCea5JagTZjH+MWwokIQhJ9omv4VR3Ltnd8Ia59fuK9n491C9aq3jDCzy8PpVC3KcwNk59eoqgNETLCyBYQyYyGAPDGZ81IznGSpnj8m/2aN9j68Ri3+dTg32fT36r8T3uGcz3VUhZznIVUxzHQSm19OAV8g//8A94nsf73ve+ozp/ZqZ+XGoZj44WmJpaGqoPRQhI4oRKpYWT0URRumQ4i5MpQm03AO1qmWq9jZKLQ5Oe3IuYGEBHLSqtpUFmdq7J3oGAieEcrhK29/EoHe377MQ1TNRCeBnae6cQQtAKE6bKLeKkP3MB3OYUQ09/DWmWH75igNtnXtwN5VLAa84WnLfGEEcRcbTsaceEVBIn7UzwFNpheqbR/TdZyHmMDWVw0czONvrXiB6O9j22Tiz2fT412Pf55He83mMpxUE7g3t2KTmOQxwffsm1KDryb+NSqUSjsfQLtF6vAzA0NHRYj3Pbbbdxzz33cMstt5zU1Vj2t68yC0BiVPf2/v683MIQoRJVti8z9D6JI2TaxqFHCDMwXW7z0+1lyvVo/wqLVp9JKUjrc8TVaUy788ej2ozYO9u/UA6Qm/5Rz1CujeDj1V/m8biz+FjRh//9YskvrJV9GfZUmpjAz2SQUlIcW4ufyZI6GfxMhlR0SntK2Vks6My1RXxH2otHy7Is64TVM5hv3ryZu++++7Af6D//8z/ZtGnTET35ueeeS7lcplZbfMWyffv27v5DueWWW/jyl7/M7bfffthB/mThORIpBQkK5Sy9+ZH6A92fT3dm+NHepYklSRJU3EDoqDuhdDlhlLJtd5Vn99aJ0s44Xqt/lBJIk6DbdXQU0p6bYqYaMl1uker+hHKRhgTlp8lUtvU85jP1C/lpsgaAM0vw2y+RrC327x+DVgGO5+IGAUlhHL9YIjYKWRyjaTw8V7FhTYGJoUzvP2aWZVmWdYLo+V12xRVX8MEPfrAbkg/mySef5EMf+hBvetObjujJL7vsMqCzWND+HnjgATZs2NCtyBJFEXNzc0vO/8QnPsHXv/51brvttm65xa1bt/Kxj33siNpxonKUxFGSVIN0l44zjoNS9+fTnFmGK48RJovDudGapF4maTUOOSbXGJirtHlqe4VyPbITQ/tESoGZ/RmmOkkahSTasHemTqUe9qU3WCRtnNYsg9u/TWn7Pcse85n6Rbxv7v/ih3FnYvYvnSF48wslWbe//waMdMDxUV5Ava1JghJxKpiJfBxHcfb6AQZynl2x1rIsyzop9Bxjfumll/LFL36RN7zhDVx55ZW8+tWv5uyzz2ZgvjxZpVLhiSee4Jvf/Caf//znueCCC7pB+3BdcMEFvPa1r+XGG2/k/PPPZ3BwkK1bt3L//fdz0003dY+79tprefDBB7nzzjtZv349AH//93/P7bffzjve8Q62bt3aPfbhhx8+qmE1JyLXkTiOIIo00s8CiyfLai9Pc/AssuWnAPhfme9x9+Q5vGjt4rkDjUqFIJfDyUJhMGCm3D5ozgnjlGd316gNxqwZyi4Zt24dHSFAkSKAsFFDxyGtKGW6j+PJRdJm7CdfQOren5ntyRA/jNajkQQO/OomycahY/+eZwp5pFQ0KhUy+TxxFGKkRLg+woEk0UyFGiE1o6UM40NZJNihK5ZlWdZJo2cwB/joRz/Ku9/9bm699VZuu+22ZY8xxnDppZfyoQ996KgacP3113PDDTdw5ZVX4nkeruvy8Y9/fNGqnyMjIwwODhIEncV0du3a1Q3uf/EXf7HkMa+44oqjasuJRgAZ36XVaiEySxcaAqiu3YJX3oZDii8Sdu0p86K1Y0uOS5OEjGNQcQOlXJJDLDKkjWF6rkWzFbPO1oo+JlTS6owlVwqjE6rNmNlqu6+TkoPa9oOG8gfDjXyleR4ayUQBrtgsGQj6cyHmBVmE66EaDdTIGXhxg8QohJcDY0haGteVrBvNUyp4GLsOlmVZlnWSEeYwCiA/8MADfP7zn+f73/8+09PTQCcsn3feefzar/3akgWCTkQnWlUW6PSwzlRDfranxkgmJdz1U9J0aV3y4jPfJFfvDEn6p/oreOELz2Y8vzhcCSkprtuASEJmxTCp7pSfO1RAh85iR+PDOUYGAjs3tIeDvc9y/o6DmdtBXJ5EBjn27p2h3oz6d7FjDH59J9nZnxBUf7bsIduSET5avRyA89YKLjlL4PTx7khhdA3Sz9Ca2YUePhvlKBwJzUiDAaEU68fyZDy1Ki8CbRWHU4N9n08N9n0++a3WqiwH7THfZ8uWLSdF+D7ZGAO+rxACIuPg+B5ps7X0uEwJ5oP5WjXLf283/OqmxQHLaE1t5zb8bBZncJhCRmEQTFcOPSwoSQ27Jus02wnrRnN9DW8nGykFoj6Fjlsk1TJhopneMUkYLb3AOmZ0Sm76xxT3Przs7seitdRMwN3tzTgSfuV5gp8b79/UStf3UY6DkQotHVzPp24U1XrSGW9vDMODGdYMZ1HCluy0LMuyTl6HFcyt1cudnwCaGEF+YBjlVGhWF18BxpmFajUv9Z/hq5PnUd7gMpg5oKa51oStFrmBGkbnEPMJSElBeoi7CQaYq7ZpRQlnjBfI+k5fl4g/0QkBjjCYNCRp1UjqZeqthNlq+7DuUhytgR33kp37ac/9O5MSN9dfTYqilIG3bJaM5Y/9hZbjeRRKJeb27iUYWUuqApQJaQmfoDBEHHdeAykEa8fyDBV9O8HTsizLOunZCmMnOM9ZqMySZEqgli4K1S6cRupkABiQLc5wpvjujuVTjk5TktosQifINMRzJYO5w184qt1OeHpnhdl6aEsqHoRKQ+K9T2LadZKozXSlzVS52ddQLpPWQUP5ve3ncUPttaQozh0V/PYv9CeUQ2ehIO34CCEw0qWWuKROllrLUDF5klQTBA5nrh9guBjYUG5ZlmWdEmwwP8FJIcgEDmGcUo8Uwl1mEqh0CPNru7/+eu47/GC3ptzqEc7jCJmGpGET33NwkwZSioPWOd9fnGi276mxZ65l89QyhAAT1kiadZoze9m9t0K10b/x5CJpM/bYZxl/7I6ex9R0wP/bfCkRHr98tuANmwS+078rK4HAqADX94lRhFFKuS1JtaEVagaLPmevGyDnKXvnxbIsyzpl2KEsJzhjzP/P3p2HyVXV+R9/3/3WXr2ku7OStQmBgOyEaEC2DIZxAH+gOIIafEZAURAVxG1EHYFBRxN4UBQYEBURHEEkYV8UwiIgIISwhOzpJL3XXnc5vz8qXaTpzka6053O9/U8eeDeOvfWuX068KlT33susYhJexcUSh4Jt//VWQLn3aeANhrdzDRX8sjyiZy+f9/ZcN/z0P0CYbGAk9TQs3kcK0rc1ckUFcVS/0+F3FIYKtZvylEqB4ytj8mSipsZBGilLOXOTXTmPLoyme2WCe2qaPsyDD/f72svlvdhudfAMm80Ecfk1BmD+8CgHppemTG3XYeCMoGQsheg6xoNtVGaaiKD3gchhBBiuJFgvodTCiK2WZmFVaAw0XQd9Z6nQ/p2stf2HPd1FrROZEWHYmJN7yAW+D4dLS1Yjk202EG+s53kqBj4GqbuoOsauqbtUNlFe1eRshcwvjGBa+3dj0s3DNAKOTJrl9PWmSdf3P4HnF1lFtuJb3ql39eeL03k97mjKGEzpRY+MV0nMsgPDOqhGRa+ZuFGE3hh5T0tU6/Uk8tSiEIIIfZSUsoyAtiWgWVWhjJARzf6mQV3a3tt1xsZQHH/myHloP+07JXKdK15h3KxiCoX0MMAFw/X1onZleS0IzPh2bzH8jVdZAv+Xv20UK3YRba7i/WbsrsllMc2vcyoN+9GD70+rz1cmMGtuTmUsTlmksb/O2BwQ3lNUxMA0XQtqaZxWG6EYimg7NZR8gJc12Ty2BS1cUdCuRBCiL2WBPMRwDJ1LLMSxj2lY+h9h9V30+Tq9qtup/QCKa1ARwEefmv709hhuUBQLmGEZWw9xNE8dE2jLq5hmjqpmIVjb/0m0ZIX8M66btozJbS97LdO1zWym1poa2mhdWPboN7gCaB7OeIbXiDZ0v9yiIHSWFJqJmbBJw7SmTVBH9QPTJquExqVJRE1O0LWGQVuAs8Pacv4pBKVevKI1JMLIYTYy0kpy0igIBGzyBU8Agwsy8IMQ3yv90xp95ijMIvtOLkNAHwn/Ueu6Z7HSy017FMTMqNh64lZlUuEmo7ml7ETLnoxSyo6CqPUSdRMVoK6ZVLaxrLnflC5KbTkRWmsiYzohxFVgq5C1xT5rnbWr15OV1du8N83KDHqjT+ih31n5BcVDuQtr4nuMEI8leQT03Vi9uCPgr5FMNcth2y+jJ2Mo0pepZ68NjqifxeEEEKIHbWXzV2OTEop4hELAD8Eu6aRWMOYftt6kfrqv5tayMejTwPwl9cVa7q2PltZLhXxct0Uuzuwip1kNrZgUyQsl7B1Dz0sY6nKEon6NspbQqVoac2xemN20G96HEqmKhH6ZTYse5l1b7yO7w1+6YrTvYqG1+/sN5Tflz+IBwoHsjxoYsY+ac6cOXihPFlbi2lZoGnohlG558F0sGwHpVsoBV1Zj7ENccbUxSSUCyGEEJtJMB8hbNPANHVK5YCNpQih3nc9c4ByvHdgn2RtYh9jE4GCO/8Z0pLpPywHvo9XKhF4Hh0t6yo3l/pl/GIeKyyBVyTsaqEm4VAfDTF0jUTU3Opa5u1dRd5e10XRC0bUeueaBnro0b1pA5s2ttPR1oVX7lvjPZCMUhcNS39P7cqH0cP+v7K4v3gQCVfnUx/QOWqQS1d0N4btujiRCPExk3CjUQKlYSbSlDBxbINJY5LUJhwpXRFCCCG2IMF8hLAtHdvafANoqAjR0Y2+w1uKj6Vz7Oxe+76SWoROSNGH370Usqpzx8KSKmQIfY9sy2ryrS14xQImPlaxg4hjENXK2GbvunPbenc7X/B5e20XndnyiKg7D4FcvsjaN15lw5p1hLnOQQ+eyXXP0PDGH7e6HCLAPflD2G+UxmcP3T1LISrDQjNMdNMmqyUwEnWUPCgYKTTTYcrYFPGIPBlWCCGEeC+pMR8hNDSSMZt8oVLG4G9enSV8742Gmkahthk0jfSav1V3fz99J1d3nUJXEOX2l0OOnaxx+FhtmzOrha4OwjCsBizDNNGCEl4hTzQSYpSyRJxavEAjYuuUA0XKVWzaYgLZ80JWtmTIFyM01kbQ98DpcwV058uUi3nCXDeZzm5QCr+8jYL7AaB7eWJtr/X72v2FmbQGCZJGCXf8dD46ettjOZCUZoAVQVchpbJPNBrFCA0MU2dMfQxD0/bqZTOFEEKIrRkB85QCKnXmiahdLQsphyZ2JL7V9oWaab2243qRsxKVevNQwSNvK37/ckhbfusJKgiCXrOege+j+0VKxQJGoR0vn8HNb6AmbpNUXUQdE7OcqS7t2CMMFRva8yxf102uvGeUtmjau4G8ZVM3XZkS3qZ1ZNavoid1huHgrb7idK+i8fXf9/vadzs/xn2Fg3nHmsrE/Wcyc4w9aKE8nkoTT6dB04jE45V6cgx0J4LuRAhCRWdRIxG1Gd8Qx9gTBlcIIYQYIhLMRxDHMqqlI/lSiJGs32b7cnRUr+39zDWcnXoWh8qU9opOuPHvIY8tDyl4O1jeUsoReD5dLWvJdXZSzmUxVJkw341DibDQTdLpP7BW1jvvZH1bniBUwzKga5pGoBTt2TJvrunknXXdBKUCZlCgnM8QBsHgvn/gUbPiIWpXPrzVNp1hjEPGaJxzsE59bHB/iIYbwYzEMU0Tu6aRRE0NgQJlRVBWFEPXGFMfoz7tVj7JCCGEEGKrpJRlBLFNnWjEpOQFKCBEx3ZdysViv+21oO9NiYcZrzO6Pst/t34YhUao4OnVihfXKQ4bp3H4OA3X3HrY625t7bUd+h6GXyDb3Y2ez+OVSsSdOK6dxg9UnzW9g0DR0panI1OiqS5GKmZj6Ax56YOmaxRKAV3ZEu3dRUrlAEPXSEY0rLCEViqRG+TSFYBY6yu4mdW99m0Iktj41Bh5Hi/N4LT9dfat3z2fasIQdMtGNw3KZgIjFUUHuotg2xaTx8aIOlJPLoQQQuwICeYjSBgq0gmXju4SAAEasZo6wtaNfdY0ByjUTMVq+Xuf/WPDNfy09tc8GhzKW/kkbWGc9UENT65U/H2t4shxGoeN07CN7Ye/wPehnCcIAoLNs8nKK5OMB2jKp61kY2hQ8noH9FI5YOX6biKuSUNNlGTMwtT13RrwdF2j7Ifkij7tXQVyBR9NA8vQSLgaEd3Db1tDtlwmDILKSjWDyM6sJbHxpV77isrip93/gqdMZqa6OXL/OlKRwQ/lTjRCuVAk1HQwbCzbIVcO0XUdx9JxHBjfkMDUNQnlQgghxA6SYD7CRF0Ty9LxvJBQMwmsKIZl9hvM83XTsbPrcbNr+z3Xh43n+XACfKVzVde/sjFMUfLhiRWK59YqZo3XOHiMhrWNgB4EAZnWjb32heUiZlAizHdTm2qEwKcLs084BygUfVau78a1DWpTLqm4g2sbqEFYA13TKn9KnqJQ9unKlMjkPcreu+UpcVcnavhopSyFtvX9/lwHjFIk1z1FpGsFetD/bPyt2Q9RxGXOJI0jxtfvtptn3brROOUSmuVQwCYaTeCH4JcDYhGbcXXy0CAhhBBiZ0kwH2FsQyMesejwSmi6SahpGHYE8oU+bZVu0THpJAD0cpbGZX/o95ymFvLN9N38qPAJWgqV9dELHjyyXPHsGsXREzQOGq1hbOXBQu+tu1ZBGVXMke9oxSlmcZNpUnaKttDA0DT8QBG+Z5a1WA5YtynHxvYC0YhJbdIl4pi4m5dffD+zspUgruGHirIXkC/6dGVLFEo+5fd8SNA0iDoGEcNDDz28XOfghnIg2raUWPsb22yTter49IE6jfHBj8GW4xCvqaFr00Z8zabkJomGOQoemG4dYT5kXFOcuoQrs+RCCCHE+yDBfIRRCmqTLp3dJQI0DMNCd6JA+zaPC+04G6afSePrd2y1zVfqHuFp54M8tCbO5moZi4q0qgAAIABJREFUsmV44K1KictxU3Sm1LLdFUC8UhkVdqLCkGIuTzGXJzE+SsrRsbRKiUvZiJMv+RTLvQOyH4R0Z8t0Z8uYpo5jGSRiFlHXwjZ1DF3H0LXKg340AA2FIgwVSkGgFEGg8ENFseSTL3oUij5lPyTsZxZe0ypLUSaiJpH8OkrZDLgRvGLfDzoDye1cTmr9M/2+FiiNvHJ4y57O/zskts1vLAaSpmuEVgzTsvGVTskLcSMupUIAGEwamyTuWhLKhRBCiPdJgvkIFHMtHMekO1tG1zRGue4OHRdasW2+7uQ3ckz+jxwyfj9WFJMs3jQW/BJtYYJcAe78p83ENBw3RadhGzO4ge9Xas+3oIc+jhZS6thA4HnE6hrAriEI9coDk0KFRu+FPXw/xPdDcoXKzLWuV2btdb2yZvfmXA6qMqOuFIRKVc+3PbquURs3KXsBbmYVmc7Kh4lSYXBCue4XsHMbiLS/sdXyou91noZnJTi5WWdy7e4tFtE1Hd9K4Mai5JRO2QvIGibJuMn4hji2uXvvARBCCCFGGgnmI5Cha9SlXNZuzFaCKCaGYVRvvtyWbN3+xNteBaCQnIiTW48elHq1SbQvZSYw8z05/vnSRG7r/CA3Pw+HjNWYM1HD2cYKLlvSQh8VBhRzOQAym1pwRieosUMwLEqehu1YZIshhZJfDelbhvVwBwP31uia1quExjJ1TC+HoUIy7dv+xmGXqZDa5YuxSp1bbfJSeQLjG5N8ePK2V8YZcJoGSqEZJj4GppvEC3XQAqIRi7F1sZ4mQgghhNgFEsxHIKUUqbjNxo7KTaBlDEzbJtiBmd5sw4GgaQR2gnzddIxSFw1v/HGH3vdQZwXTrBZuzx3Ni2vH8MYmnROn6TTvwNJ9ha62XrXoYRBg6CFhtgM/n8F2oxhaDboWoyZuYekBRV/DsQzaur0+NenvR03cpDPvo2uVG1pNQ0MvZykXS9s/eBeZpe5+Q/lqv5bxZuVDgTbuQE5u3P2PHkg1NKG8MhgmXWUfI1KLlykztiFBfcqR9cmFEEKIASLBfIRyLYPapMuGtjwFX6e+bjS0t1DK57d5nDJdMqMPr24H2ylvea+kXuQ/Eo8A8ERxX15atg+vrG/kxGkGSXfrAb30nptTlVLofgmvmKNUKOCVSsQIcdIxnHInYblIzHZBGRhGlNDfuXRoGnplhl0pLFNH0zQsP0c8EicadBEUcuiWQ7a9lcDzt3/CnWSUM0RblxI4SezsOiLdK/u0uTN3BM+XJ/Hx+tcZ25RiVO2ofs40eNINDXRt2gSmQxipwQiK+H5IJqeYOCZJMmLJLLkQQggxgCSYj1BhqKhPVdY0L3sBGStJNFbYbjDvQ+/9K+K5dVjFth06dI67jDnuMgCWvjaWUixGuq6WQv1+O3S8l2mr1nOHYUghm8VNlil1bqKYy2GYJtF0DU40QdzRyHuV607FLVo7K7PcmkblxlADOrLvrqISs0PKoYFlQEQrAwqjlMPVTbLrV/WpgR9oybVLtlpH3uM1Y19OPdCgMX0Ig9ubvnRDJ3TTuLE8oW7RUTSoicaIWRZj6mI4li6hXAghhBhgEsxHMNs0aKyLsqYlQ67gEXdj2JEI5Z28ebFj/BwSLS9QSE8h23QIdmYtdSseqL6+sfl06pYvwvC3ft79rLVQBtZDZNOrdDR/FGXY23zffHem13YQBFilLnKbP1wEvo9XyBNNBuiZjbhOjIKTxihnsUyHiGNgaz7oCl35RB0d09Dx/BCHEoZt4JS7KbZvxCuXsGybaEoNbigPfeIbX95uKH/b2pf5+xtbXYJysBhm5X6EUIWEhoWdqsdDxw9CnEicpqSDjiahXAghhBgEEsxHMKUUtQmHrmyJ7myZohHHrRlFubBqp85TTE+hmJ5S3fYi9SjNQFMB5egoAidFOdZEpOudHTqf42dY98ZSaqcduMM3hwKoMKSjZV3vfUphFtrp3LShEqwbNQgDUhEXp7AR3w/QIgmUbpDUQ8JMK7FYDSiFHkCxYyOlQiXolwqFQVtxRStl0f2A9OrHcbLr+m3zdGkKRzjLycYmEJtwKGo3h3IANx7HSDagezlyHjhOgrKvMa4xSl1S6smFEEKIwSTBfITTgHGj4rxd6qI77+OYu37zoDIdOvY5Diezhlzd5rKU96xd3t10GMmWv2/1HIf4L3DjC/VMnTxmh24O3ZpSPo9XLALglctk1r5DvL4BC6oh3rTaiTWMxc91kevsQO/uxjAMLNfd+dKe98HOriP+zgMktpFqf1X8F6ZObKKl/kPbXQd+MDjRaOVpqoZNlhhxUxGg013SGN+YIBGxdmnFGyGEEEJsnwTzvYBj6YxrirNyXTcYFpqug1K7tOZ0KTGOUmJcdTtXN4NI53IAupsOJTdq5jaDOcC5kQd4beUYuta71NamCcYc2Cfg74gwDHv9e76jrde1+Z5Hoa0Fr1x5rH0YBIRBUN0ebKm1T6FtZ6p57iGNmLvpQUH9cWoaQTdQQeWhS9GoTTISoSbhYG2+UVYIIYQQg0uC+V5AKUhGbMY2JChnfJJj9sHLdpHvHLi1ub3oKNomnoTh5ymkJgNQjtRjF1oBKKQmkm04mFFv/l+v42bYm8s62uGBXAMzG0KMSILATryvkA6VIP5e5c2z6oNOhaRXPYZZ6qJz/Bw2ZhWjy5k+zf6n+yOckX6JJr2D7rGzMI3dvwxiD13XUbpBYEQq9/r6inRtikTUIQjC7R4vhBBCiIEhwXwvUak3tykZMdozDnoQwAAGc4ByYmyv7UzTodSueIhQt+gefcR2b/Y8qbQYVr+7nR01k0zTYQPax8EW6XiruvThqLfuob8FDpdpU5l7SANG5CQ27d7u9WIYBslRDaDp+LpBe15hmzoTxySIupaEciGEEGI3k2C+F1EK4rEIgQndhCRHTyAsdOOVSoNy02M5PoYN+30cpZmgGzt9fHzTK7Sn98dyIwPetwGjFIaXJbDi2OteIN3+8rabo1E79QD8bazpvjvYjgOahuekN38zoRGPWoxriOOYshSiEEIIMRQkmO9lPGUStcFPJNhQ1ommYpjdGwjDEN/zUOHAzpIqw+m1XUyMw82sIdRN2ieeSP3yRds8fsKbt1fOg0ZoRekafSSl1D4D2sf3TSlqli/Gzbfs8CGd4+fguzWD2Klt03QdNxbDrB2Llt1ENqwsK1kT04lGowASyoUQQoghIsF8L9NzU2QyamEaCTZ2FDA0g0htE+XuVoq5HJFEnFK+gArDXbpBtD9dY2dT7niLcnw0XnQU7ROOo3bVI9s9TkNheDnSqx5l3eR/I+a1U4qPA61v+B9UKqQUaLzTAes2dvHv4fZDeajbhPFRtI35IKEV3Q2d7F969NjK7LiCnBYhEq1B0zXq0lHiEQN/J5+eKoQQQoiBJcF8L6UURGyDMaNi5FWcXBjBTIKjFHayFhW24iZrybVvQqOyfLVXKu3y+4ZWlFzDgdVtL1LX6/ViYjxuZvV7D6vSUYxb/qd3z6cZtE88ES8+epf7tjV+qGjJgLZhGQcWn+M1bywP547g47HnYNtl83SNPoJ83QxiMZcwv+s/v/fLME18J40yLMxiJ/mij51Ms09NFFPXJJQLIYQQw4AE872cpWvUpuMEZZdNnRaJphRB4GFaXfhuCrcpjhYG6EGRrvVrBnwGPbTjFFKTcLtWkK+bTvfoI0mue5pY++s7dLyuAvy3nuZu+99ojENjXKMuCnEH9J1c1UUpRc6D9jy0FxQbs7AuU/lnUsvxvfQS0OAD9ko+YK/s/3oMh1JsNKAoJcdTSE9936vLDJR47Sh0AnK+hucp6iyHMbVxahK2PDBICCGEGEYkmAs0O8qoWBTdKLKhLY9tucTdKF2eTrYICdckYpnomx/XXi4WMUyTaCJBtrMT3TBQSqEBaNpOP9K+c8KxaIGHMiwAyrGGHQ7mAE1GJ0s3KVa0ljjefZU1YYynSvuScCDhgG1U/liGhqVXsmigIAyhHCjyHuTKinxZUQ4ryxbW6FnOjD5Dt3JZZ9dwemzba7IDKM1gY/PpKNPdqesfLIZpouk6WryWUDcpFjUSMZNR9bVohjPgH7KEEEIIsWskmAs0O4Lvh9TEbaKuSXtXEYIInl+Z6c0UFUZEw41GMJKNaK1r0E0DPZrE9XzMunHgFTDDMmEY0rXx3bprTdN2KAD2hHKAUmICvpNC93Jkmg5HaTrptU9u8/if1d7aa7slSNFSTpMM25jjLOWl3D48XZ4GwERzE9PNdTxTnkJHGMfB40vJxaTcAr/MfJiVwSiOd19lhr22crIdLGHvmHDssAnlumGQrG8EQjKhRaGk0VAXpT7louuaPDBICCGEGIYkmAu8zfXFSoFt6IwZFSXwbPKbigShwg9CQqVhR+J0aXHsUZNxvU4Cw0azXTrLFoZukXDKqHwnqcYmwiAg39lBsn4U+e5OSvkdX45RGRabpp0GKqDyxBswS13EW/+5w+f4UvKBXtsz7HW82jGOsjL4fPwhorrHFGsDDxZm8oXkg9V28xOP86fwWD5kLtvh9/LtBK1T/hVl7sabULdCNww0IFrXSGi5EHrYrsPoxghRx0ApJJQLIYQQw5QEc9FHGIBumEwcHadYDunMliiVfELNJV/0yKNBrB4zLGNG4/jlkJKnQHOI6ybKsim6SaKmS2hZaFo3qYYmlG5S7GzFtCzymb5Pw+xF00B799czX7cfdn4DVr6VXP0MvGgDNase3anr+s+a/8Pk3TKbZquFZqv3qippPc9n9Pu2/zPSTbrGfhBN+RST+2z34UmDzbQt3FgczTDRnRi+EQE3RtoqE0knIFSyDKIQQggxzEkwF/1SClDgmDpNNVE0XcMru3gdZTK5Mh1Zn7qogWbYBJtnYAsln5Rr42smXTkPI16DqSkMwySMpMiFLok6A1DouRzhTqyZHthx2qac0mtfKdaEk9vxNcS3DOXvl9J0Mo2HUkhPHtKlD7ek6TrRRBLNTRCi00kC1zRpSLikolHKsuKKEEIIsUeQYC62SymFChSGYTGhwcYPQgrlgHLZozvnoWlllKqEeaWZlJUNKDpyAVHXIhmJUAx1sgWfaCKBFgZohlG5+3IXdI8+kuT6Z9CDMlaxfWAudmvv1XQovpOiHG0cNnXkUAnlsVGjCd0keuihDJu6mEtd0kUDvGCoeyiEEEKIHSXBXOyUMFTomkbMMYm7FqmEor7k05EpkcmW8S2bXM4HKqUTZS8A08ZXJhDSnoeUa2CYBrbjEoYBfrlMoqaGzk2bdqovfqSW9sknA5Ba8zeiHW8O/AUD7fscRykxYciXPdySaVmgQaxxAhkVpVBUjE8ZJFJJTMuu3nAr5StCCCHEnkOCuXjflFLoQMwxSUQsyrUhuYJHOSiSK3gEgar8iSQpFSuh1g8UpUAn6rqQGoupfIyOdSg7AlQCpxuLke3s3PyUyh1Lll1jZlVqvXWD5Prn0EIf3S+ih+WduybNABWioVCaTtukuXixpp06x2DTdJ1E3SiK2QzdKkaAxoSmKHWuIjRtublTCCGE2ENJMBcDIgwVpq6RitmkEw6FzbPonZkSGzMB8G5NRah0LMelwzewgKjtoAwX3dCJ1tShOVHigGnbdG7cuGMd0A1KyfEAtE77NwC00MMstBFaMSLtbxDYCWJtSzHK3ZTjYwjMCMXUJHwnhdJ0lOEACi0McLpX4UXrCZzUgP6c3pf3fEAxTIPQiuAmIJaOkorbmLpW+QnLFLkQQgixx5JgLgacChWuZTCmLsaodIRM3mNTZ55C0a8s16cUoRWtPAZeN9HsCIFm4MZiFKJNuGEey42iNH3X+qFb1dnubNOhABRqpgEKtnpuDWXoFGum7NJ7D6SaxiayHW2EQaUm37QsIskkSTOKHncJZcUVIYQQYkSQYC4GjVKVWfTahE06bpMteGzqLJAreLSpGGUvxNcVnXYNrvKx3RiZcohp6OiGBWhomoYbi6NUCAq8cmmnnyzai6YBw6dWfFu0zTXtoelimBbRhiasSISUExKpTRGWCvhStiKEEEKMGBLMxaBTqhKFExGLRNQiXwpo6yrQnS3j+SH5YoDpajiWQ+ArPMPEMmwIA2K1oyBRj5Zrp7KCY7hrwXyYM20LFSqCICBW34AWhoSmSyweITGqDtuNoAFBAJjulhVCQgghhNjDSTAXu5eCqG0Qa4hTqg3pzJRo7y5RDkJcJ05QDsmWNEpmhBqzCHaEbs8iqZtopo1eyAEQiScoFwuEYYjaxWUXhxM7mkD5HkYkhh8bRcyGVDqJnfMgEiGsfHFAEMhMuRBCCDHSDHkwL5VKLFy4kAcffBDbtrFtmwsvvJBjjz12u8du2LCBK6+8ktdeew1N05gwYQKXXXYZkydPHvyOi12iFNiGTmNNlPpUhGzBoyNTRNfKBKGiVA4ITQ3dcigVfAzXIdwczg3TxE7Xozo24sTidLdu2iPDuWGaWI5NMZev7tMsF80wcBNJzGSCRMTC0DX0SBLJ4kIIIcTItmt31w2Ar33tazz22GPcfvvt/PnPf+aCCy7gggsu4PHHH9/mcdlslk996lMYhsFf/vIXFi1axJQpU/j3f/93Wlp2/GmQYmgppdA1SEYtJjYlmTYhTWNdFMcyKGkuoeFUbhjVLZRuo5kW8cZx+HYK3TDAimAYxlBfxk7TNA3TsjCjycqTO5NJEnX1RBMRGhtqaahLkoxYaFR+RlokKTd4CiGEECPckAbzZ599lvvvv58LL7yQmpoaAI4//nhmzZrFf/3Xf23z2P/93/9l7dq1XHbZZZimiaZpXHTRRXiex3XXXbc7ui8GmFKV1VzG1seYNiFNOhWlpEx0XaOjbNNV1DAcF9+w6S6GmG6U0HTQ9d6/xtowehDQlpxoZa12y3FINY1FNy00O0I8nSY1cTpN40YzqjaFG3FQhlU9TinwfEnlQgghxEg3pMF80aJFAMyaNavX/lmzZrFixQpee+21bR47bdo06uvrq/scx+GQQw5h8eLF1Scfij1Pz5roNTGbSWMqs+g1SQdN18B0CDWLMFRolk1oVEo/oLKMoG4YJOpHDfEVgGXbvbY1XSeaqkXXdZzaJkI3QTSRoLY2TtPoetLJKFY0gWY5aNEUoe4MUc+FEEIIMVSGNJgvXbqUdDpNMpnstX/ChAkAvP766/0eVyqVWL58ebXde4/t7u5m7dq1A99hsfspiGxeE715QppRTfWk0wlsWwcnga8MTNsmPXossYaxWLaNZkd7nUI3Km0GS38z9NF0Lab17qy3bhiEpkM8lSBaO4r62hSNDWnisSiWGyUIFD4WHjZeoMsHSyGEEGIvNKQ3f3Z0dBCLxfrsj8fjALS3t/d7XGdnJ2EYVtttqed87e3tjBs3bgB7K4aSUqryKVI3ScYsEjGHYjkkEgTopMlZNYReGdO2Nq/7bWI5Dm4yTbGro7LKSdum6vncWBTf8/HL5Z3ui67rhFvcbJpqaMAvlch2dhJP11Aq5FF2BN0wMIKAeCpJMmZiN9RixHyMmnilbjzUUZqO5piVpVaEEEIIsVcb8lVZhou6ur4hf3cZNSoxZO89EngphXJTlAoFSm6OcryGvF+Dlh4LXgk9LGHEklDorh6TaGwkLBXIdXS8u6++jmx7R68VXkzbwS+XqttuLI4VjZLZtHGLfTGUa2Ma4DRNJN61FqeuBtMpEaupx0mmCdrX4DbVU24r4dQO3e+aGFzyd3nvIOO8d5BxHvmG4xgPaTCvqanhnXfe6bM/m80CUFtb2+9x6XQaXder7baUy+W2eezWtLVlCYfgKYqjRiXYtCmz2993JDENm6C7iIZCNyxiERc3laCcSKIVuyn7UXzbRTd0rHQDpc52ip4i9HXyRY9oMkUxl8XyDUpeiFeqBHHDNEmkEuQ6u3GiUUw3SrFUxrcN8oUyjuugaRAqDdt2iDbG0BJJbJXBjLn4ZRfPiFAuKmKmTWd3CU1P0i3jPSLJ3+W9g4zz3kHGeeQbqjHWdW2bk8FDGsynT5/Oiy++SCaTIZF491PL6tWrq6/3x3EcJk2aVG23pdWrV5NMJhk7duzgdFoMO34AoNANHc2OEYQKzY1j6jq6bWElYuiROGlzHKpmHP5GA82NEYQB6aRLGG+gvPYNtHgCVeikpFWe4mNYJlYsQbomgdu4D6amUNk23NoUab0ed/RkyLRiRmMoTQc0Ah10yyHUDHTbQekWaAa6EyEMFaGSL6mEEEII0b8hvfnz5JNPBmDJkiW99i9ZsoSJEycyY8YMAMrlMh1blBz0HPvmm2/S1tZW3Vcul3nhhReYO3fusF0yTwyeMASiKYKgZ91vBZqBZpiEugWWiwoVpuNgWiaOGyGeTJGIudTUxBlVl6SpoZbJ06cxZf/pjGtI0tRYx+jGWmrrakklXOKJGI7rEkml0e0IuuWgMMB0wbQra41HEgRKR4vVgmGhFJiphiH5RkYIIYQQe44hDeZHHnkkc+fOZeHChXR2dgLw6KOP8tRTT3H55ZdX251//vkcc8wxrFmzprrvs5/9LKNHj+aqq64iCAKUUvz0pz/FNE2++MUv7vZrEUNPKbV59hz8zTPTmmmCHUVpJlhuZZlF0wHNAN1CbQ7OlYCtYzhRlJMgtKKgG9VAH6ITYmBYNpg2es+5bBv0SjDXjMoSh75WeSiSp0yCIKzcuGpaW+u2EEIIIQQwDG7+/O///m8WLFjAxz/+cWzbxrIsrrvuOo455phqm/r6etLpNK7rVvfF43Fuu+02rrzySj7ykY+gaRrjx4/ntttuo6mpaSguRQwjPcsNBspAaQaEqrKMYgAYBhgWoW6ihZUPdbphEaKj2Q7oFqDQTBuFhubGCRVougm6idJMNCdamR3XbTAs/EDJtzRCCCGE2CWakgWTAbn5c29iGhCEGpU1CjWUUpiqSKBHMMISoeGAUmjd61CpsRhaiBdoGLpCD308LAxDq5TMbM7i2/tbJOM88skY7x1knPcOMs4jn9z8KcQw0XOzaEXln77mglIEuoMKFbquodlRQqXwVSV9B6FGQKUkJQgqx8nHWiGEEEIMlCGtMRdiuOn5Akkp0GJplJLwLYQQQojdQ4K5EP1QSuH5Q90LIYQQQuxNJJgLIYQQQggxDEgwF0IIIYQQYhiQYC6EEEIIIcQwIMFcCCGEEEKIYUCCuRBCCCGEEMOABHMhhBBCCCGGAQnmQgghhBBCDAMSzIUQQgghhBgGJJgLIYQQQggxDEgwF0IIIYQQYhiQYC6EEEIIIcQwIMFcCCGEEEKIYUCCuRBCCCGEEMOABHMhhBBCCCGGAXOoOzBc6Lq2V7632H1knEc+GeO9g4zz3kHGeeQbijHe3ntqSim1m/oihBBCCCGE2AopZRFCCCGEEGIYkGAuhBBCCCHEMCDBXAghhBBCiGFAgrkQQgghhBDDgARzIYQQQgghhgEJ5kIIIYQQQgwDEsyFEEIIIYQYBiSYCyGEEEIIMQxIMBdCCCGEEGIYkGAuhBA76Sc/+Qn77rsvf/zjH4e6K0IIIYDu7m4uueQS9t13X9asWTPU3XnfzKHuwN6oVCqxcOFCHnzwQWzbxrZtLrzwQo499tih7prYSW+//Ta33347Tz31FJqmEQQB06ZN44ILLmD69Om92r7wwgtcc801dHd343keRx11FF/96ldJJBJD1HvxfqxevZqbb755q6/LOO/5nnjiCW6++Wa6u7vp7u7GsixOOOEEvvKVr1TbyDjvuZYuXcqCBQtYsWIFplmJQaeeeiqf/vSnq9sgY7wn+dvf/sZ3v/tdIpHINtu9/fbb/OhHP2Lt2rWEYcgBBxzA17/+dRobG/u0vfPOO7nlllsACIKAj33sY8yfPx9N0wblGqqU2O0uvPBCNW/ePNXe3q6UUuqhhx5S++23n3rssceGuGdiZ51xxhnqrLPOUm1tbUoppXK5nDrvvPPUzJkz1SuvvFJt98orr6iZM2eq3/72t9V2n/zkJ9UnPvEJFQTBkPRdvD8XXHCBOu+881Rzc7O66667er0m47znu+OOO9Qpp5yiVq1apZRSKgxDdd1116kTTjih2kbGec+1du1addhhh6mvfOUrqlQqKaWUeumll9TMmTPV1VdfXW0nY7xnOeuss9Srr76qFixYoJqbm9Xq1av7tFm7dq064ogj1DXXXKPCMFSe56mLLrpInXTSSSqbzfZqe9ttt6mDDjpI/fOf/1RKKbVy5Up11FFHqR//+MeDfi0SzHezZ555RjU3N6vFixf32j9//nx10kknDVGvxPt1xhlnqGeeeabXvnfeeUc1Nzeryy67rLrvU5/6lDrllFN6tXvxxRdVc3Ozuueee3ZLX8Wue+qpp9TcuXPV3/72t36DuYzznm39+vXqwAMPVC+88EKv/YVCQT3++OPVbRnnPddvfvMb1dzcXA1cPT7/+c+r2bNnV7dljPcsnucppdQ2g/lll12mjjjiiOoHMqWUamlpUdOnT1fXX399dV8mk1GHHHKIuuKKK3od/4tf/ELNmDFDrVmzZpCuokJqzHezRYsWATBr1qxe+2fNmsWKFSt47bXXhqJb4n267bbbOOKII3rta2pqAir1bgCtra0899xzHHXUUb3aHXjggcRiMe67777d01mxS3zf54c//CHf+MY3en3d3UPGec/3pz/9iWg0ysEHH9xrv+u6zJkzB5Bx3tMZhgFUShO25Ps+YRgCMsZ7ov7+m7ylIAi4//77OfTQQ7Ftu7q/sbGRyZMn9xrTv/71r2Sz2T457aijjsL3fR588MGB7fx7SDDfzZYuXUo6nSaZTPbaP2HCBABef/31oeiWeJ+2/Ave45133gHgyCOPBGDZsmUopapj3EPXdcaOHcuyZcsGv6Nil/3ud79j9OjRHHPMMf2+LuO853v++ecZN24cDz/8MGeffTYf+chHOP3007n++uspl8uAjPOebt68eUybNo2sX19iAAATKElEQVSFCxdWJ08ee+wxlixZwmc+8xlAxngkWr16Nblcrs+YAowfP563334bz/OAd3PY+PHje7XbXTlNbv7czTo6OojFYn32x+NxANrb23d3l8QA++1vf8ukSZM488wzgXfHtGeMtxSPx1m9evVu7Z/YeR0dHfz85z/n17/+9VbbyDjv+davX8/atWu5/vrrWbhwIaNHj+a5557jggsu4KWXXuLnP/+5jPMeLh6Pc8stt/DNb36TWbNmkU6n8X2f73//+5x++umA/F0eibY3pr7vk8lkqK2t3Wrb3ZXTJJgLMYAef/xxHnjgAX7961/juu5Qd0cMkJ/97GfMmzePyZMnD3VXxCAqlUrk83kuvfRSRo8eDcDhhx/O2WefzXXXXcff//73Ie6h2FUrVqzgs5/9LAcffDDPPPMM8XicJUuWcNFFF5HNZjnnnHOGuotiLyelLLtZTU0NuVyuz/5sNgtAbW3t7u6SGCDPP/883/rWt7jhhhtobm6u7q+pqQHeHeMtZbNZGfNhbtmyZTz00EN88Ytf3GY7Gec9X8+3mfvtt1+v/QcccAAAL730kozzHu6nP/0pGzZs4IorrqjOgM6aNYvTTz+dK6+8klWrVskYj0DbG1PTNKvLYG6t7e7KaTJjvptNnz6dF198kUwm02st1J6vxt679rXYMzz77LNceumlXH/99dX/ifeYPn06mqb1+fozDEPWrl1brUUXw9OTTz6JbducffbZ1X35fB6ABQsWcMstt3D00Udz7rnnyjjv4aZMmcLSpUtRSvXa33PDYBiG8vd5D7ds2TLq6+v7lClMmjSJIAh49dVXOfzww2WMR5gJEyYQi8X6LUNas2YNU6ZMwbIs4N0ctnr1avbdd99qu92V02TGfDc7+eSTAViyZEmv/UuWLGHixInMmDFjKLoldsGTTz7JZZdd1iuUb9y4kfPOOw+A+vp6Dj/88D5j/sorr5DL5aq/E2J4mj9/Po888gh333139c8PfvADAL70pS9x9913c+mll8o4jwDHH3880Pfmrp6b/Q488EAZ5z1cfX097e3tFIvFXvvXrl0LVGZLZYxHHsMwOOmkk/j73/9evckTKv+vfvvtt3uN6Zw5c4jFYjz99NO9zrFkyRJM0+TEE08c1L5KMN/NjjzySObOncvChQvp7OwE4NFHH+Wpp57i8ssvH+LeiZ312GOPcf7553PKKaewbNmyanC77777eOONN6rtvv71r7Ny5Up+//vfA1AoFLjmmms45JBDOOWUU4aq+2KAyTjv2f7lX/6FQw89lJ/85CfVFTvefvttbr31Vo455pjqTKmM857rnHPOwfM8rr766uqSiW+88Qa///3vmTFjBocddhggYzwSXXjhhQBce+21QGWJzKuuuooJEyb0urcgHo9zySWXcOedd1aXsF69ejW33HIL8+fPZ+zYsYPaT0299zs7MehKpRILFizgoYcewrZtLMviwgsv5MMf/vBQd03spOOPP541a9b0+9rYsWN55JFHqtvPP/88P/7xj+nq6qo+3vlrX/uaPN55D7J8+XIuvvhi8vk8q1atYvTo0aRSKb7xjW9U1zyWcd6zZbNZfvrTn/LII49g2zZhGDJv3jzOP//8XsujyjjvuZYsWcINN9xAS0sLpmkSBAHHH388n/vc50ilUtV2MsZ7jmuvvZYHH3yQ1tZWWltbq6UpV199da9ylLfeeosrr7ySNWvWoJRi//3359JLL6WxsbHPOf/whz9w6623ApV10E8//XTmz5+Prg/unLYEcyGEEEIIIYYBKWURQgghhBBiGJBgLoQQQgghxDAgwVwIIYQQQohhQIK5EEIIIYQQw4AEcyGEEEIIIYYBCeZCCCGEEEIMAxLMhRBCiM2UUuRyuQE9ZzabHdDzCSFGLgnmQoi9XqFQ4MYbb+S0007jgx/8ILNnz+bEE0/k/PPP58Ybb2Tjxo1D3cXd5q677mL27NkccMAB7LvvvsyePZvZs2dz6KGHcsQRR3Deeefx6quvVtt/61vf4qijjmLfffdl4cKFQ9jzXbdhwwY++clP9rq+FStW8OlPf5qjjz6af/3Xf+Xxxx/v99ivfvWrfP7zn+/3tXPOOYcHHnhgUPoshBhhlBBC7MU8z1NnnHGGOuaYY9Tzzz+vwjBUSim1atUqdf7556vm5mZ177339jrm0ksvVc3NzUPR3d3mU5/6VJ9rXLp0qTrppJPUAQccoP7xj39U969evVo1NzerBQsW7O5uDpjW1lZ13HHHqV/96lfVfZ7nqZNOOkl973vfU0EQqPvuu0/tv//+atWqVb2Ofe6559TMmTPVypUr+z33m2++qY466ii1aNGiQb0GIcSeT2bMhRB7tUceeYSXXnqJL37xixxyyCFomgbA+PHj+fGPf0w6nR7iHg4f06dP55vf/Cblcpn/+Z//GeruDKgrrriCuro6zj333Oq+f/zjH6xYsYJzzz0XXdc5+eSTaWho4N577622CcOQH/zgB3z2s59lwoQJ/Z576tSpXHDBBXzrW9+ira1t0K9FCLHnkmAuhNirvfPOOwDU1tb2eS0SifD5z3+ecePG7e5uDVsf+MAHgEpoHSneeustFi9e3CuUA9USplGjRlX3NTQ0sGHDhur27bffTmdnJ+edd9423+PjH/84ALfeeutAdVsIMQKZQ90BIYQYSj2h609/+hMf/vCHqzPmPebPn1/990KhwAknnEAmkwFg9uzZ1dceffRRbNvm4Ycf5q677uL1118nl8sRjUaZO3cuX/rSl4hGo9X28+bNo6WlhWw2yy9/+UueeOIJHn74YfL5PEceeST/+Z//2efDQjab5brrruP++++nWCziui5Tpkxh3rx5fPSjH0XXK3Mt5XKZG264gbvvvpuuri4sy+Loo4/moosuYuzYsbv08wrDEKDPz6nHb37zG37zm9+wYcMGpk2bxne+8x1mzJhRfT0IAm677TYeeOABVq9eTT6fZ/To0Zx11ll88pOf7HWuIAi48cYb+b//+z8ymQymaTJ16lTmzp3LGWecUW23q9f7l7/8BYBZs2b12t/Y2AhAS0tLdTa8paWFY445BoCOjg5+9rOf8d3vfpdIJLLN97Btm8MOO4x7772Xiy++eLt9EkLspYa6lkYIIYZSa2urOvjgg1Vzc7M67bTT1B/+8AfV3t6+zWO2VWN+2mmnqYsvvlhlMhmllFIvv/yymjNnjvrc5z7Xp+1dd92lmpub1RlnnKGefPJJpZRSy5YtU4cddpg677zzerUtFArq1FNPVSeeeKJ66623qn0/99xzVXNzs+rq6lJKKRUEgfrc5z6njj76aPXiiy8qpZRqaWlRZ511ljr66KPVhg0bdujn0l+NuVJKPfLII6q5uVnNnz+/uq+nxnzevHnqxhtvVJ7nqa6uLnXGGWeoWbNmqVKpVG2bzWZVc3Ozuvnmm1UQBCoIAnXvvfeq/fbbT91www293mvBggXqQx/6kFq+fLlSSqlcLqcuv/zyXv0aiOs9++yz1dFHH91nf0+N+aWXXqry+by64447etWYf+c731Fnn332ds/f46qrrlLNzc1q/fr1O3yMEGLvIqUsQoi9Wl1dHQsXLqShoYFXX32Vb37zm8yePZuzzz6bO++8k3K5vFPnmzp1KpdeeinxeByAmTNnct555/HEE0/wxhtv9HvMQQcdxNFHHw1Ac3Mzxx57LH/96197vfdNN93Ea6+9xuWXX86UKVOqff/hD3/Y61x//vOfeeKJJ/jyl79cLTtpbGzkiiuuoLW1lRtuuGGnrqdHEAS8+OKL/PCHPyQajXLJJZf0aWMYBvPnz8c0TZLJJGeeeSZtbW288MILvdoce+yxfOYzn0HXdXRdZ968ecydO5ebbroJpVS17cMPP8xBBx3EpEmTAIhGo3z961+vbg/U9a5YsYL6+vo++03T5IYbbqC1tZXjjjuO3/3ud1x//fWMHz+e1157jT/+8Y98+9vfJpPJcMkll3DEEUcwd+7cra7A0tDQALxbPiWEEO8lpSxCiL3e7Nmzeeihh1i0aBGLFy/mqaee4tlnn+XZZ5/l5ptv5qabbqqWNWzP1Vdf3WffxIkTgUotc3Nzc5/XDz744F7bo0ePxvM8Ojo6qu+7aNEiDMOoBvgejY2N3HLLLdUymUWLFlWvaUtTp04lEonw17/+dYeuo0fPeXzfx3EcDj30UL7whS8wderU7V5HU1MTQK/lJl3X5Re/+EWfYydOnMh9991HW1tbNSTX1dXx+OOPc8cdd/DRj34U13VJpVIsXry4etxAXG9nZydjxozp97V99tmHX/3qV732KaX4/ve/zyc+8QmmTZvGN77xDVauXMkDDzzA3/72Ny6++GIWLVrU52bQnnKXjo6O7fZJCLF3kmAuhBCA4ziceuqpnHrqqeRyOR588EGuv/563nrrLa666ip+8pOf7NB5Vq5cyS9/+UteeOEFOjs70TQNz/MAKBaL/R5TU1PTa9uyLIDqcT3nTafT2Lbd5/ijjjqqVzuAM888s087Xdfp7Ozcoevo8eSTT+5w261dh+/7vfY/88wz3Hzzzbz55psUCgU0TSOfzwO9f0aXX345X/rSl/j2t7/Nj370I+bMmcPpp5/OnDlzqjXuA3G9nudhmjv+v8O7776blStXcsMNN5DNZrnnnnuqK/iccsopXH/99dxxxx189atf7XWcYRjV9xNCiP5IMBdCiPeIxWKceuqpHHPMMRx//PE89dRTO3RcS0sLH/vYxxg3bhwLFixgypQpaJrGM888wznnnLPV43pu2tyenQl0ixYtIplM7nD7gbAj1/Hwww/zhS98gVNOOYU77riDuro6ABYuXMi1117bq+2UKVO49957efLJJ/nLX/7C4sWLWbx4MSeccALXXnttrxtQd+V6Xdft8+Fha7LZLNdccw2XXHIJiUSCpUuX4vt+tbyop9/9lav0jN/2bhQVQuy9pMZcCLFXu+eee/jyl7/c72s1NTVMnjx5qzPd77V48WIymQz/8R//wdSpU7e6csn7sc8++9Dd3d1vXzZs2FDd31M2s+WSfj3WrFnDSy+9NGB9ej/uuusulFJ84xvfqIbyrQmCAE3T+OAHP8iPfvQjnnjiCY477jgeeughnn76aWBgrrehoYGurq4d6v91113HmDFjOP300wGqNfFbjrWmab1q5Xt0d3cD7HBZlBBi7yPBXAixV/N9nxdeeKHfwFsul1m9ejUHHHBAr/09M549s6w33XQTS5YsqZaZvDeQr1+/fpf7efLJJwPw2GOP9dq/YsUK5syZQ0tLCwAf+chHAPq9AfF73/se99xzzy73ZVds7We0bt26Pm1PPPFEXn755ep2IpGolqz0hNyBuN7m5uZ+g/17LV++nNtuu41vf/vb1f6PGzcOwzBYsWJFtd3Klf+/vbsHSS2MwwD+2MfQMYtqkyJoqsCgaMgliSIKTBpaGlqihgIhoaUQI6itdCiiXDoNLZY1BYIRSFNBRINKUMPtg8IhO6UHjg73DpcOee3rklwO1+c3yvu+nnd7OOf/vv8fqK2tzZp/f3+PwsJC1NXVffpfRJSfGMyJKO/FYjFMTk6q9coAcHV1BYfDAVmWMTExkTH+pWzh4uICT09PWFtbQzKZRHt7OwRBgNfrVYPm5eUlVlZWvv2Mw8PDaGxshMfjUcskYrEYnE4nbDab+ubYarWio6MD6+vraglOKpXC6uoqIpFIVhOdf62npwfA70OyL3XloVAoo5vma0tLS3h4eAAAPD8/Y2dnB5WVlWpdfS72azabkUwmcX19/eG4+fl59Pf3w2Qyqb+VlZWht7cXGxsbUBQFh4eHOD8/x8DAQNb8aDSKpqYmGAyGT5+JiPKT7udb39uIiPLE4+MjgsGgep1hIpGAoijQ6/VobW3F6Ogo6uvrM+bIsoypqSkcHx+jqKgInZ2dmJmZgU6nw8nJCTweD6LRKKqqqlBdXY22tjYsLi6itLQUJpMJoihiaGgIkUgEiUQC5eXl6O7uxtzcHPr6+nBzcwNZllFRUYHBwUG11OZ1gyFFUSAIAqxWK8bGxjIOhabTabUxjyRJEAQBzc3NsNvtaoB/j9/vh9vthiRJSKfT6g0pLw2U/uR2u+Hz+RCPxyEIAoxGI/b29uByuRAIBCBJUsa+AWB7exuiKOL29hZGoxENDQ0oKSmBz+fL2PPBwQF2d3cRDoeRSqVQXFyMlpaWrH18Z78AIEkSLBYL7Hb7u0F+f38f09PTCAQCWY2f4vE4nE4njo6OYDAY4HA4YLPZMsbc3d2hq6sLs7Ozb4Z2IiKAwZyIiAherxeiKCIYDEKv1+d8fZfLhbOzM/j9/r+6AYaI8gtLWYiIKO+NjIzAbDZjfHz8y4d9v2pzcxOhUAjLy8sM5UT0IQZzIiLKewUFBVhYWIDFYsHp6WlO1w6Hw9ja2kJNTU1O1yWi/w9LWYiIiIiINIBvzImIiIiINIDBnIiIiIhIAxjMiYiIiIg0gMGciIiIiEgDGMyJiIiIiDSAwZyIiIiISAMYzImIiIiINOAXHvyJz3UgB68AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from shutil import copy\n",
        "# copy('trained_models/' + config + '.h5', models_dir)\n",
        "# joblib.dump(history, models_dir + \\\n",
        "#             'History_' + config + '.joblib')"
      ],
      "metadata": {
        "id": "vMdT1UpJSExp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def history_plot(history):\n",
        "#     # list all dictionaries in history\n",
        "#     print(history.history.keys())\n",
        "#     # summarize history for error\n",
        "#     plt.figure(figsize=(11,7))\n",
        "#     plt.subplot(2,1,1)\n",
        "#     plt.plot(history.history['out_mean_squared_error'])\n",
        "#     plt.plot(history.history['val_out_mean_squared_error'])\n",
        "#     plt.title('Model Error Performance')\n",
        "#     plt.ylabel('Error')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.legend(['Train', 'Val'], loc='upper right')\n",
        "#     #   plt.ylim([0, 3])\n",
        "#     # plt.show()\n",
        "#     # summarize history for loss\n",
        "#     plt.figure(figsize=(11,7))\n",
        "#     plt.subplot(2,1,2)\n",
        "#     plt.plot(history.history['out_loss'])\n",
        "#     plt.plot(history.history['val_out_loss'])\n",
        "#     plt.title('Model Loss')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.legend(['Train', 'Val'], loc='upper right')\n",
        "#     #   plt.ylim([0, 3])\n",
        "#     plt.savefig(figures_dir + 'LC_' + config + '.png')\n",
        "#     plt.show()\n",
        "# #\n",
        "# history_plot(history)"
      ],
      "metadata": {
        "id": "6UENS6A6SHEi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if D_S == 0:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(14, 9))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth')\n",
        "        plt.plot(y_pred.ravel(), label='Prediction')\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {MAE}\")\n",
        "        plt.legend();\n",
        "    plt.show()\n",
        "    \n",
        "elif D_S == 1:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(14, 9))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[0][random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth', linewidth=3)\n",
        "        plt.plot(y_pred.ravel(), label='Prediction', linewidth=3)\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {round(MAE, 4)}\")\n",
        "        plt.legend()\n",
        "    plt.savefig(figures_dir + 'Examples_Fy_' + config + '.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZCpo5JbBSMw8",
        "outputId": "ca1b7a6b-9b26-43e5-ed03-9e01dcd83f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dffd6d168bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mD_S\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mrandom_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_Test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_Test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'D_S' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Construction_Error(GRND, Pred):\n",
        "    construction_err = []\n",
        "    rmse = []\n",
        "    corr_coef = []\n",
        "    bad_indices = []\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(GRND)):\n",
        "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
        "        RMSE = mean_squared_error(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel(), squared=False)\n",
        "        R = np.corrcoef(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel())[0, 1]\n",
        "        if MAE < 1:\n",
        "            construction_err.append(MAE)\n",
        "            rmse.append(RMSE)\n",
        "            corr_coef.append(R)\n",
        "        elif MAE >= 1:\n",
        "            count = count + 1\n",
        "            bad_indices.append(i)\n",
        "\n",
        "    print(f'Construction Error : {round(np.mean(construction_err), 3)} +/- {round(np.std(construction_err), 3)}')\n",
        "    print(f'RMSE : {round(np.mean(rmse), 3)} +/- {round(np.std(rmse), 3)}')\n",
        "    print(f'R : {round(np.mean(corr_coef), 3)} +/- {round(np.std(corr_coef), 3)}')\n",
        "    print(f'Number of Bad Predictions = {count}')\n",
        "\n",
        "    bad_indices = set(bad_indices)\n",
        "    all_indices = set(np.arange(len(GRND)))\n",
        "    good_indices = np.array(list(all_indices - bad_indices))\n",
        "    GRND_NEW = GRND[good_indices]\n",
        "    PRED_NEW = Pred[good_indices]\n",
        "\n",
        "    return GRND_NEW, PRED_NEW"
      ],
      "metadata": {
        "id": "3UI2ycsUSPAi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[A,B] = Construction_Error(Y_Test, GRF_pred[0])"
      ],
      "metadata": {
        "id": "0VGab5XISRGW",
        "outputId": "2f806e88-8d56-4c2b-889a-66d0afc04d3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construction Error : 0.071 +/- 0.032\n",
            "RMSE : 0.091 +/- 0.041\n",
            "R : 0.95 +/- 0.06\n",
            "Number of Bad Predictions = 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "cc12dd51c08fff59e312c49b5273cbf2a12939660509ea1a4d83cd89b0463726"
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('Ai': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "transfer_learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}