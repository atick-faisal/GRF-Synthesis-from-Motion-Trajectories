{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "grf_synthesis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "b9e8b413bd195f698761fbb7f1cc8940f40efdff75a04973931ad61a6fc56074"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/GRF-Synthesis-from-Motion-Trajectories/blob/main/src/training/grf_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ny5F1D53uh9Q",
        "outputId": "0b3a8ac3-fa2f-4174-86ca-1e6e6c1d9949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_r50aIez0f",
        "outputId": "e933c2f9-1179-424c-d2f2-b648cba75c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "# !pip install --upgrade pandas==1.3.4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.3.4\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j15cTL2I4oNu"
      },
      "source": [
        "# Download and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJcRThWM5LF5",
        "outputId": "896422f4-ab3f-4425-d08f-2d7d12db7e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !gdown --id \"1kNGwWBQp6kmAioLP-_zWx7r42SzoEO3v\"\n",
        "# !unzip UNet.zip\n",
        "\n",
        "# !gdown --id \"1TKUBIw0AQY_2KjHeSMmEfvN6vVhy3JwT\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kNGwWBQp6kmAioLP-_zWx7r42SzoEO3v\n",
            "To: /content/UNet.zip\n",
            "\r  0% 0.00/737k [00:00<?, ?B/s]\r100% 737k/737k [00:00<00:00, 47.4MB/s]\n",
            "Archive:  UNet.zip\n",
            "  inflating: 1D_CNN_Segmentation_End2End_Pipeline.ipynb  \n",
            "  inflating: AlbuNet_1DCNN.py        \n",
            "  inflating: Dense_Inception_UNet_1DCNN.py  \n",
            "  inflating: FPN_1DCNN.py            \n",
            "  inflating: LinkNet_1DCNN.py        \n",
            "  inflating: PSPNet_1DCNN.py         \n",
            "  inflating: README.txt              \n",
            "  inflating: TernausNet_1DCNN.py     \n",
            "  inflating: UNet_1DCNN.py           \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TKUBIw0AQY_2KjHeSMmEfvN6vVhy3JwT\n",
            "To: /content/data1_f15_t3_n1_combined.joblib\n",
            "100% 216M/216M [00:02<00:00, 84.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6XtngBS4oNt"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kpZ_68Q43j9",
        "outputId": "2d7b07ee-3f82-4eed-8d79-ec1dea6d3b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "import h5py\n",
        "import scipy\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from UNet_1DCNN import UNet\n",
        "from FPN_1DCNN import FPN\n",
        "from AlbuNet_1DCNN import AlbUNet\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style(\"darkgrid\", {'font.family':'serif', 'font.serif':'Times New Roman'})\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/DB1/ModelsZ/'\n",
        "figures_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/DB1/FiguresZ/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-28d65caa2165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mUNet_1DCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mFPN_1DCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFPN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mAlbuNet_1DCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlbUNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'UNet_1DCNN'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCAfz544oNz"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwNGZwEg4oNz"
      },
      "source": [
        "# data = joblib.load('/content/data2_f72_t3_n1_combined.joblib')\n",
        "data = joblib.load('/content/data1_f15_t3_n1_combined.joblib')\n",
        "X_Train = data['train_X']\n",
        "X_Test = data['test_X']\n",
        "Y_Train = data['train_y']\n",
        "Y_Test = data['test_y']\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Axis Selection"
      ],
      "metadata": {
        "id": "k_ePJvmtegj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = X_Train[:, :, [10, 11, 12, 13, 14]]\n",
        "X_Test = X_Test[:, :, [10, 11, 12, 13, 14]]\n",
        "Y_Train = Y_Train[:, :, 2]\n",
        "Y_Test = Y_Test[:, :, 2]\n",
        "\n",
        "X_Train = np.nan_to_num(X_Train)\n",
        "X_Test = np.nan_to_num(X_Test)\n",
        "Y_Train = np.nan_to_num(Y_Train)\n",
        "Y_Test = np.nan_to_num(Y_Test)\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "metadata": {
        "id": "WZKBWCVTeFFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(Y_Train[:, :].T)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "avon12Ydqs5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_LEN = X_Train.shape[1]\n",
        "N_CHANNELS = X_Train.shape[2]"
      ],
      "metadata": {
        "id": "RIHHro2IeqP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1FRPZ74oN0"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30i1r3Ln4oN0"
      },
      "source": [
        "model_name = 'MultiResUNet' # UNet or UNetPP\n",
        "signal_length = FRAME_LEN # Length of each Segment\n",
        "model_depth = 5 # Number of Level in the CNN Model\n",
        "model_width = 64 # Width of the Initial Layer, subsequent layers start from here\n",
        "kernel_size = 3 # Size of the Kernels/Filter\n",
        "num_channel = N_CHANNELS # Number of Channels in the Model\n",
        "D_S = 1 # Turn on Deep Supervision\n",
        "A_E = 0 # Turn on AutoEncoder Mode for Feature Extraction\n",
        "A_G = 0 # Attention Guided\n",
        "problem_type = 'Regression'\n",
        "output_nums = 1 # Number of Class for Classification Problems, always '1' for Regression Problems\n",
        "'''Only required if the AutoEncoder Mode is turned on'''\n",
        "feature_number = 1024 # Number of Features to be Extracted\n",
        "'''Only required for MultiResUNet'''\n",
        "alpha = 1 # Model Width Expansion Parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdOOVOa4oN2"
      },
      "source": [
        "# Prepare for deep supervision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c61dm5v94oN3"
      },
      "source": [
        "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
        "  def approximate(inp, w_len, signal_length):\n",
        "    op = np.zeros((len(inp),signal_length//w_len))\n",
        "    for i in range(0,signal_length,w_len):\n",
        "      try:\n",
        "        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(i)\n",
        "  \t\n",
        "    return op\n",
        "\n",
        "  out = {}\n",
        "  Y_Train_dict = {}\n",
        "  out['out'] = np.array(y)\n",
        "  Y_Train_dict['out'] = out['out']\n",
        "  for i in range(1, (model_depth+1)):\n",
        "    name = f'level{i}'\n",
        "    if ((model_name == 'UNet') or (model_name == 'MultiResUNet') or (model_name == 'FPN')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n",
        "    elif ((model_name == 'UNetE') or (model_name == 'UNetP') or (model_name == 'UNetPP')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**0, signal_length),axis = 2)\n",
        "    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
        "  \n",
        "  return out, Y_Train_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQqyFLjl4oN4"
      },
      "source": [
        "X_Train1 = X_Train\n",
        "X_Test1 = X_Test\n",
        "[Y_Train1, Y_Train_dict] = prepareTrainDict(Y_Train, model_depth, signal_length, model_name)\n",
        "[Y_Test1, Y_Test_dict] = prepareTrainDict(Y_Test, model_depth, signal_length, model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_V8_Ht-4oN5"
      },
      "source": [
        "loss_weights = np.zeros(model_depth)\n",
        "\n",
        "for i in range(0, model_depth):\n",
        "   loss_weights[i] = 1-(i*0.1)\n",
        "   \n",
        "loss_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK_K_tyy4oN5"
      },
      "source": [
        "# Build and compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-nTNzLM4oN5"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = 0.001\n",
        ")\n",
        "\n",
        "if D_S == 0:\n",
        "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "    # D_S = 0, A_E = 0\n",
        "    model = UNet(\n",
        "        signal_length,\n",
        "        model_depth,\n",
        "        num_channel, \n",
        "        model_width, \n",
        "        kernel_size, \n",
        "        problem_type=problem_type, \n",
        "        output_nums=output_nums, \n",
        "        ds=D_S,\n",
        "        ae=A_E, \n",
        "        ag=A_G,\n",
        "        alpha=alpha\n",
        "    ).UNet()\n",
        "\n",
        "    model.compile(\n",
        "        loss= 'mean_absolute_error', \n",
        "        optimizer= optimizer, \n",
        "        metrics= ['mean_squared_error','accuracy']\n",
        "    )\n",
        "    #\n",
        "elif D_S == 1:\n",
        "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "    # D_S = 1, A_E = 0\n",
        "    model = UNet(\n",
        "        signal_length, model_depth, \n",
        "        num_channel, \n",
        "        model_width, \n",
        "        kernel_size, \n",
        "        problem_type=problem_type, \n",
        "        output_nums=output_nums, \n",
        "        ds=D_S,\n",
        "        ae=A_E,\n",
        "        ag=A_G, \n",
        "        alpha=alpha\n",
        "    ).MultiResUNet()\n",
        "\n",
        "    model.compile(\n",
        "        loss= 'mean_absolute_error', \n",
        "        optimizer= optimizer, \n",
        "        metrics= ['mean_squared_error'], \n",
        "        loss_weights= loss_weights\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlLB1flN4oN6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model_name + '_' + str(signal_length) + '_' \\\n",
        "            + str(N_CHANNELS) + '_' + str(model_width) +'_' \\\n",
        "            + str(model_depth) + '_' + str(num_channel) + '_all_vel_' \\\n",
        "            + str(D_S) + str(A_G) + '_axis_corrected'"
      ],
      "metadata": {
        "id": "IoH2_k-yPFkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7URRYEK4oN6"
      },
      "source": [
        "if D_S == 0:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config +'.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train, Y_Train, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "elif D_S == 1:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config + '.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train1, \n",
        "        Y_Train_dict, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MflY4cE04oN7"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-4OM5vr4oN7"
      },
      "source": [
        "if D_S == 0:\n",
        "    GRF_pred = model.predict(X_Test, verbose=1)\n",
        "    print(GRF_pred.shape)\n",
        "elif D_S == 1:\n",
        "    GRF_pred = model.predict(X_Test1, verbose=1)\n",
        "    print(GRF_pred[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_mean = np.mean(Y_Test, axis=0)\n",
        "ground_truth_std = np.std(Y_Test, axis=0)\n",
        "prediction = np.nan_to_num(GRF_pred[0][:, :, 0])\n",
        "prediction_mean = np.mean(prediction, axis=0)\n",
        "prediction_std = np.std(prediction, axis=0)"
      ],
      "metadata": {
        "id": "moGbBtKgG_MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.corrcoef(ground_truth_mean, prediction_mean)[0, 1]"
      ],
      "metadata": {
        "id": "3Kl-nvENMN-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "x = np.linspace(0, 100, FRAME_LEN)\n",
        "plt.plot(x, ground_truth_mean, linewidth=3, label='Ground Truth')\n",
        "plt.fill_between(\n",
        "    x, ground_truth_mean + ground_truth_std,\n",
        "    ground_truth_mean - ground_truth_std, alpha=0.3,\n",
        "    label='Ground Truth +/- STD'\n",
        ")\n",
        "plt.plot(x, prediction_mean, linewidth=3, label='Prediction')\n",
        "plt.fill_between(\n",
        "    x, prediction_mean + prediction_std,\n",
        "    prediction_mean - prediction_std, alpha=0.3,\n",
        "    label='Prediction +/- STD'\n",
        ")\n",
        "plt.legend()\n",
        "plt.title(f'R = {round(R, 5)}')\n",
        "plt.xlabel('Stance Phase (%)')\n",
        "plt.ylabel('GRF N/Kg (Normalized)')\n",
        "plt.savefig(figures_dir + 'Results_' + config + '.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WcIRU_sFH-WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "cVQTMzbguxgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copy\n",
        "copy('trained_models/' + config + '.h5', models_dir)\n",
        "joblib.dump(history, models_dir + \\\n",
        "            'History_' + config + '.joblib')"
      ],
      "metadata": {
        "id": "kOt1Gg7SutXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsW0dNyN4oN7"
      },
      "source": [
        "# Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Z2otb84oN7"
      },
      "source": [
        "def history_plot(history):\n",
        "    # list all dictionaries in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for error\n",
        "    plt.figure(figsize=(11,7))\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(history.history['out_mean_squared_error'])\n",
        "    plt.plot(history.history['val_out_mean_squared_error'])\n",
        "    plt.title('Model Error Performance')\n",
        "    plt.ylabel('Error')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    #   plt.ylim([0, 3])\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.figure(figsize=(11,7))\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(history.history['out_loss'])\n",
        "    plt.plot(history.history['val_out_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    #   plt.ylim([0, 3])\n",
        "    plt.savefig(figures_dir + 'LC_' + config + '.png')\n",
        "    plt.show()\n",
        "#\n",
        "history_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYBK8ann4oN8"
      },
      "source": [
        "# Visualize outcomes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f-mpSA34oN8"
      },
      "source": [
        "if D_S == 0:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(14, 9))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth')\n",
        "        plt.plot(y_pred.ravel(), label='Prediction')\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {MAE}\")\n",
        "        plt.legend();\n",
        "    plt.show()\n",
        "    \n",
        "elif D_S == 1:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(14, 9))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[0][random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth', linewidth=3)\n",
        "        plt.plot(y_pred.ravel(), label='Prediction', linewidth=3)\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {round(MAE, 4)}\")\n",
        "        plt.legend()\n",
        "    plt.savefig(figures_dir + 'Examples_Fy_' + config + '.png')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3elq2yvb4oN8"
      },
      "source": [
        "# Construction Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tivpIeIt738C"
      },
      "source": [
        "def Construction_Error(GRND, Pred):\n",
        "    construction_err = []\n",
        "    rmse = []\n",
        "    corr_coef = []\n",
        "    bad_indices = []\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(GRND)):\n",
        "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
        "        RMSE = mean_squared_error(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel(), squared=False)\n",
        "        R = np.corrcoef(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel())[0, 1]\n",
        "        if MAE < 1:\n",
        "            construction_err.append(MAE)\n",
        "            rmse.append(RMSE)\n",
        "            corr_coef.append(R)\n",
        "        elif MAE >= 1:\n",
        "            count = count + 1\n",
        "            bad_indices.append(i)\n",
        "\n",
        "    print(f'Construction Error : {round(np.mean(construction_err), 3)} +/- {round(np.std(construction_err), 3)}')\n",
        "    print(f'RMSE : {round(np.mean(rmse), 3)} +/- {round(np.std(rmse), 3)}')\n",
        "    print(f'R : {round(np.mean(corr_coef), 3)} +/- {round(np.std(corr_coef), 3)}')\n",
        "    print(f'Number of Bad Predictions = {count}')\n",
        "\n",
        "    bad_indices = set(bad_indices)\n",
        "    all_indices = set(np.arange(len(GRND)))\n",
        "    good_indices = np.array(list(all_indices - bad_indices))\n",
        "    GRND_NEW = GRND[good_indices]\n",
        "    PRED_NEW = Pred[good_indices]\n",
        "\n",
        "    return GRND_NEW, PRED_NEW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5LZy8284oN9"
      },
      "source": [
        "[A,B] = Construction_Error(Y_Test, GRF_pred[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1P2HQPM4oN9"
      },
      "source": [
        "# Infinite loop to keep colab alive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HEA3DYo4oN9"
      },
      "source": [
        "# while True:\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}