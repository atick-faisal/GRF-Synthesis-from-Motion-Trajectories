{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "grf_synthesis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "b9e8b413bd195f698761fbb7f1cc8940f40efdff75a04973931ad61a6fc56074"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/GRF-Synthesis-from-Motion-Trajectories/blob/main/src/training/grf_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ny5F1D53uh9Q",
        "outputId": "8bbd841a-a0d7-49b5-c133-db1b940ca59e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_r50aIez0f"
      },
      "source": [
        "# !pip install --upgrade pandas==1.3.4"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j15cTL2I4oNu"
      },
      "source": [
        "# Download and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJcRThWM5LF5"
      },
      "source": [
        "# !gdown --id \"1kNGwWBQp6kmAioLP-_zWx7r42SzoEO3v\"\n",
        "# !unzip UNet.zip\n",
        "\n",
        "# !gdown --id \"1MafN0ICjXTriabpt1xHeJX2Ml_6gjfkt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6XtngBS4oNt"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kpZ_68Q43j9"
      },
      "source": [
        "import h5py\n",
        "import scipy\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from UNet_1DCNN import UNet\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=1.2)\n",
        "sns.set_style(\"darkgrid\", {'font.family':'serif', 'font.serif':'Times New Roman'})"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCAfz544oNz"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwNGZwEg4oNz",
        "outputId": "6bd32954-c968-4bf7-de05-ddf797a03756"
      },
      "source": [
        "data = joblib.load('/content/data2_f72_t3_n1_combined.joblib')\n",
        "X_Train = data['train_X']\n",
        "X_Test = data['test_X']\n",
        "Y_Train = data['train_y']\n",
        "Y_Test = data['test_y']\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3136, 1024, 36)\n",
            "(784, 1024, 36)\n",
            "(3136, 1024, 3)\n",
            "(784, 1024, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Axis Selection"
      ],
      "metadata": {
        "id": "k_ePJvmtegj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = X_Train[:, :, 30:]\n",
        "X_Test = X_Test[:, :, 30:]\n",
        "Y_Train = Y_Train[:, :, 2]\n",
        "Y_Test = Y_Test[:, :, 2]\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "metadata": {
        "id": "WZKBWCVTeFFC",
        "outputId": "107d5e7c-30fa-4b5c-8b3e-07d853c31f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3136, 1024, 6)\n",
            "(784, 1024, 6)\n",
            "(3136, 1024)\n",
            "(784, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_LEN = X_Train.shape[1]\n",
        "N_CHANNELS = X_Train.shape[2]"
      ],
      "metadata": {
        "id": "RIHHro2IeqP9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1FRPZ74oN0"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30i1r3Ln4oN0"
      },
      "source": [
        "model_name = 'UNet' # UNet or UNetPP\n",
        "signal_length = FRAME_LEN # Length of each Segment\n",
        "model_depth = 5 # Number of Level in the CNN Model\n",
        "model_width = 64 # Width of the Initial Layer, subsequent layers start from here\n",
        "kernel_size = 3 # Size of the Kernels/Filter\n",
        "num_channel = N_CHANNELS # Number of Channels in the Model\n",
        "D_S = 1 # Turn on Deep Supervision\n",
        "A_E = 0 # Turn on AutoEncoder Mode for Feature Extraction\n",
        "A_G = 0 # Attention Guided\n",
        "problem_type = 'Regression'\n",
        "output_nums = 1 # Number of Class for Classification Problems, always '1' for Regression Problems\n",
        "'''Only required if the AutoEncoder Mode is turned on'''\n",
        "feature_number = 1024 # Number of Features to be Extracted\n",
        "'''Only required for MultiResUNet'''\n",
        "alpha = 1 # Model Width Expansion Parameter"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdOOVOa4oN2"
      },
      "source": [
        "# Prepare for deep supervision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c61dm5v94oN3"
      },
      "source": [
        "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
        "  def approximate(inp, w_len, signal_length):\n",
        "    op = np.zeros((len(inp),signal_length//w_len))\n",
        "    for i in range(0,signal_length,w_len):\n",
        "      try:\n",
        "        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(i)\n",
        "  \t\n",
        "    return op\n",
        "\n",
        "  out = {}\n",
        "  Y_Train_dict = {}\n",
        "  out['out'] = np.array(y)\n",
        "  Y_Train_dict['out'] = out['out']\n",
        "  for i in range(1, (model_depth+1)):\n",
        "    name = f'level{i}'\n",
        "    if ((model_name == 'UNet') or (model_name == 'MultiResUNet') or (model_name == 'FPN')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n",
        "    elif ((model_name == 'UNetE') or (model_name == 'UNetP') or (model_name == 'UNetPP')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**0, signal_length),axis = 2)\n",
        "    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
        "  \n",
        "  return out, Y_Train_dict"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQqyFLjl4oN4"
      },
      "source": [
        "X_Train1 = X_Train\n",
        "X_Test1 = X_Test\n",
        "[Y_Train1, Y_Train_dict] = prepareTrainDict(Y_Train, model_depth, signal_length, model_name)\n",
        "[Y_Test1, Y_Test_dict] = prepareTrainDict(Y_Test, model_depth, signal_length, model_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_V8_Ht-4oN5",
        "outputId": "d075ad66-d7da-467e-aca7-35682d04c3df"
      },
      "source": [
        "loss_weights = np.zeros(model_depth)\n",
        "\n",
        "for i in range(0, model_depth):\n",
        "   loss_weights[i] = 1-(i*0.1)\n",
        "   \n",
        "loss_weights"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 0.9, 0.8, 0.7, 0.6])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK_K_tyy4oN5"
      },
      "source": [
        "# Build and compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-nTNzLM4oN5"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = 0.001\n",
        ")\n",
        "\n",
        "if D_S == 0:\n",
        "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "    # D_S = 0, A_E = 0\n",
        "    model = UNet(\n",
        "        signal_length,\n",
        "        model_depth,\n",
        "        num_channel, \n",
        "        model_width, \n",
        "        kernel_size, \n",
        "        problem_type=problem_type, \n",
        "        output_nums=output_nums, \n",
        "        ds=D_S,\n",
        "        ae=A_E, \n",
        "        ag=A_G,\n",
        "        alpha=alpha\n",
        "    ).UNet()\n",
        "\n",
        "    model.compile(\n",
        "        loss= 'mean_absolute_error', \n",
        "        optimizer= optimizer, \n",
        "        metrics= ['mean_squared_error','accuracy']\n",
        "    )\n",
        "    #\n",
        "elif D_S == 1:\n",
        "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "    # D_S = 1, A_E = 0\n",
        "    model = UNet(\n",
        "        signal_length, model_depth, \n",
        "        num_channel, \n",
        "        model_width, \n",
        "        kernel_size, \n",
        "        problem_type=problem_type, \n",
        "        output_nums=output_nums, \n",
        "        ds=D_S,\n",
        "        ae=A_E,\n",
        "        ag=A_G, \n",
        "        alpha=alpha\n",
        "    ).UNet()\n",
        "\n",
        "    model.compile(\n",
        "        loss= 'mean_absolute_error', \n",
        "        optimizer= optimizer, \n",
        "        metrics= ['mean_squared_error'], \n",
        "        loss_weights= loss_weights\n",
        "    )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlLB1flN4oN6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7URRYEK4oN6",
        "outputId": "775283ff-ae3a-4c42-cebf-6aed22258299"
      },
      "source": [
        "if D_S == 0:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/'+model_name+'_'+str(signal_length)+'_'+ \\\n",
        "            '_' + str(N_CHANNELS) + '_' + str(model_width)+'_' \\\n",
        "                +str(model_depth)+'_'+str(num_channel)+'_'+str(D_S)+'.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train, Y_Train, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "elif D_S == 1:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/'+model_name+'_'+str(signal_length)+'_'+str(model_width)+'_' \\\n",
        "                +str(model_depth)+'_'+str(num_channel)+'_'+str(D_S)+'.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_out_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train1, \n",
        "        Y_Train_dict, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.8243 - out_loss: 0.1893 - level1_loss: 0.1845 - level2_loss: 0.2209 - level3_loss: 0.3066 - level4_loss: 0.5600 - level5_loss: 0.8190 - out_mean_squared_error: 0.1125 - level1_mean_squared_error: 0.1089 - level2_mean_squared_error: 0.2526 - level3_mean_squared_error: 0.8314 - level4_mean_squared_error: 2.6286 - level5_mean_squared_error: 5.7296\n",
            "Epoch 00001: val_out_loss improved from inf to 0.49096, saving model to trained_models/UNet_1024_64_5_6_1.h5\n",
            "40/40 [==============================] - 58s 1s/step - loss: 1.8243 - out_loss: 0.1893 - level1_loss: 0.1845 - level2_loss: 0.2209 - level3_loss: 0.3066 - level4_loss: 0.5600 - level5_loss: 0.8190 - out_mean_squared_error: 0.1125 - level1_mean_squared_error: 0.1089 - level2_mean_squared_error: 0.2526 - level3_mean_squared_error: 0.8314 - level4_mean_squared_error: 2.6286 - level5_mean_squared_error: 5.7296 - val_loss: 1.8537 - val_out_loss: 0.4910 - val_level1_loss: 0.4716 - val_level2_loss: 0.4281 - val_level3_loss: 0.2968 - val_level4_loss: 0.2747 - val_level5_loss: 0.3548 - val_out_mean_squared_error: 0.2873 - val_level1_mean_squared_error: 0.2640 - val_level2_mean_squared_error: 0.2162 - val_level3_mean_squared_error: 0.1032 - val_level4_mean_squared_error: 0.1056 - val_level5_mean_squared_error: 0.1478\n",
            "Epoch 2/300\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.5676 - out_loss: 0.1042 - level1_loss: 0.1030 - level2_loss: 0.1078 - level3_loss: 0.1185 - level4_loss: 0.1266 - level5_loss: 0.1494 - out_mean_squared_error: 0.0231 - level1_mean_squared_error: 0.0244 - level2_mean_squared_error: 0.0261 - level3_mean_squared_error: 0.0274 - level4_mean_squared_error: 0.0315 - level5_mean_squared_error: 0.0460\n",
            "Epoch 00002: val_out_loss improved from 0.49096 to 0.34514, saving model to trained_models/UNet_1024_64_5_6_1.h5\n",
            "40/40 [==============================] - 39s 987ms/step - loss: 0.5676 - out_loss: 0.1042 - level1_loss: 0.1030 - level2_loss: 0.1078 - level3_loss: 0.1185 - level4_loss: 0.1266 - level5_loss: 0.1494 - out_mean_squared_error: 0.0231 - level1_mean_squared_error: 0.0244 - level2_mean_squared_error: 0.0261 - level3_mean_squared_error: 0.0274 - level4_mean_squared_error: 0.0315 - level5_mean_squared_error: 0.0460 - val_loss: 1.2609 - val_out_loss: 0.3451 - val_level1_loss: 0.3324 - val_level2_loss: 0.2842 - val_level3_loss: 0.2181 - val_level4_loss: 0.2047 - val_level5_loss: 0.1917 - val_out_mean_squared_error: 0.1397 - val_level1_mean_squared_error: 0.1284 - val_level2_mean_squared_error: 0.0965 - val_level3_mean_squared_error: 0.0738 - val_level4_mean_squared_error: 0.0767 - val_level5_mean_squared_error: 0.0737\n",
            "Epoch 3/300\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.4837 - out_loss: 0.0956 - level1_loss: 0.0943 - level2_loss: 0.1033 - level3_loss: 0.0990 - level4_loss: 0.1087 - level5_loss: 0.1037 - out_mean_squared_error: 0.0198 - level1_mean_squared_error: 0.0200 - level2_mean_squared_error: 0.0231 - level3_mean_squared_error: 0.0208 - level4_mean_squared_error: 0.0267 - level5_mean_squared_error: 0.0225\n",
            "Epoch 00003: val_out_loss improved from 0.34514 to 0.26235, saving model to trained_models/UNet_1024_64_5_6_1.h5\n",
            "40/40 [==============================] - 40s 994ms/step - loss: 0.4837 - out_loss: 0.0956 - level1_loss: 0.0943 - level2_loss: 0.1033 - level3_loss: 0.0990 - level4_loss: 0.1087 - level5_loss: 0.1037 - out_mean_squared_error: 0.0198 - level1_mean_squared_error: 0.0200 - level2_mean_squared_error: 0.0231 - level3_mean_squared_error: 0.0208 - level4_mean_squared_error: 0.0267 - level5_mean_squared_error: 0.0225 - val_loss: 1.1342 - val_out_loss: 0.2623 - val_level1_loss: 0.2633 - val_level2_loss: 0.2202 - val_level3_loss: 0.2001 - val_level4_loss: 0.2307 - val_level5_loss: 0.2410 - val_out_mean_squared_error: 0.0860 - val_level1_mean_squared_error: 0.0878 - val_level2_mean_squared_error: 0.0742 - val_level3_mean_squared_error: 0.0741 - val_level4_mean_squared_error: 0.1164 - val_level5_mean_squared_error: 0.1135\n",
            "Epoch 4/300\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.4633 - out_loss: 0.0898 - level1_loss: 0.0910 - level2_loss: 0.0967 - level3_loss: 0.0968 - level4_loss: 0.0999 - level5_loss: 0.1048 - out_mean_squared_error: 0.0179 - level1_mean_squared_error: 0.0188 - level2_mean_squared_error: 0.0200 - level3_mean_squared_error: 0.0189 - level4_mean_squared_error: 0.0235 - level5_mean_squared_error: 0.0244\n",
            "Epoch 00004: val_out_loss improved from 0.26235 to 0.22827, saving model to trained_models/UNet_1024_64_5_6_1.h5\n",
            "40/40 [==============================] - 42s 1s/step - loss: 0.4633 - out_loss: 0.0898 - level1_loss: 0.0910 - level2_loss: 0.0967 - level3_loss: 0.0968 - level4_loss: 0.0999 - level5_loss: 0.1048 - out_mean_squared_error: 0.0179 - level1_mean_squared_error: 0.0188 - level2_mean_squared_error: 0.0200 - level3_mean_squared_error: 0.0189 - level4_mean_squared_error: 0.0235 - level5_mean_squared_error: 0.0244 - val_loss: 1.0661 - val_out_loss: 0.2283 - val_level1_loss: 0.2229 - val_level2_loss: 0.2013 - val_level3_loss: 0.1962 - val_level4_loss: 0.2283 - val_level5_loss: 0.2555 - val_out_mean_squared_error: 0.0755 - val_level1_mean_squared_error: 0.0808 - val_level2_mean_squared_error: 0.0788 - val_level3_mean_squared_error: 0.0790 - val_level4_mean_squared_error: 0.1144 - val_level5_mean_squared_error: 0.1211\n",
            "Epoch 5/300\n",
            "11/40 [=======>......................] - ETA: 26s - loss: 0.4458 - out_loss: 0.0897 - level1_loss: 0.0899 - level2_loss: 0.0984 - level3_loss: 0.0926 - level4_loss: 0.0907 - level5_loss: 0.0960 - out_mean_squared_error: 0.0175 - level1_mean_squared_error: 0.0179 - level2_mean_squared_error: 0.0193 - level3_mean_squared_error: 0.0177 - level4_mean_squared_error: 0.0174 - level5_mean_squared_error: 0.0190"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MflY4cE04oN7"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-4OM5vr4oN7"
      },
      "source": [
        "if D_S == 0:\n",
        "    GRF_pred = model.predict(X_Test, verbose=1)\n",
        "    print(GRF_pred.shape)\n",
        "elif D_S == 1:\n",
        "    GRF_pred = model.predict(X_Test1, verbose=1)\n",
        "    print(GRF_pred[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "cVQTMzbguxgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copy\n",
        "copy('trained_models/'+ model_name+'_'+str(signal_length)+'_'+str(model_width)+'_' \\\n",
        "        +str(model_depth)+'_'+str(num_channel)+'_'+str(D_S)+'.h5', \n",
        "        '/content/drive/MyDrive/Research/GRF Data Synthesis/Models/')\n",
        "joblib.dump(history, '/content/drive/MyDrive/Research/GRF Data Synthesis/Models/History_' + \\\n",
        "            model_name+'_'+str(signal_length)+'_'+str(model_width)+'_' \\\n",
        "            +str(model_depth)+'_'+str(num_channel)+'_'+str(D_S)+ '.joblib')"
      ],
      "metadata": {
        "id": "kOt1Gg7SutXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsW0dNyN4oN7"
      },
      "source": [
        "# Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Z2otb84oN7"
      },
      "source": [
        "def history_plot(history):\n",
        "    # list all dictionaries in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for error\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(history.history['out_mean_squared_error'])\n",
        "    plt.plot(history.history['val_out_mean_squared_error'])\n",
        "    plt.title('Model Error Performance')\n",
        "    plt.ylabel('Error')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    #   plt.ylim([0, 3])\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(history.history['out_loss'])\n",
        "    plt.plot(history.history['val_out_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    #   plt.ylim([0, 3])\n",
        "    plt.show()\n",
        "#\n",
        "history_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYBK8ann4oN8"
      },
      "source": [
        "# Visualize outcomes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f-mpSA34oN8"
      },
      "source": [
        "if D_S == 0:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth')\n",
        "        plt.plot(y_pred.ravel(), label='Prediction')\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {MAE}\")\n",
        "        plt.legend();\n",
        "    plt.show()\n",
        "    \n",
        "elif D_S == 1:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(12, 9))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[0][random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth', linewidth=3)\n",
        "        plt.plot(y_pred.ravel(), label='Prediction', linewidth=3)\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {round(MAE, 4)}\")\n",
        "        plt.legend();\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3elq2yvb4oN8"
      },
      "source": [
        "# Construction Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tivpIeIt738C"
      },
      "source": [
        "def Construction_Error(GRND, Pred):\n",
        "    construction_err = []\n",
        "    bad_indices = []\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(GRND)):\n",
        "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
        "        if MAE < 1:\n",
        "            construction_err.append(MAE)\n",
        "        elif MAE >= 1:\n",
        "            count = count + 1\n",
        "            bad_indices.append(i)\n",
        "\n",
        "    print(f'Construction Error : {round(np.mean(construction_err), 3)} +/- {round(np.std(construction_err), 3)}')\n",
        "    print(f'Number of Bad Predictions = {count}')\n",
        "\n",
        "    bad_indices = set(bad_indices)\n",
        "    all_indices = set(np.arange(len(GRND)))\n",
        "    good_indices = np.array(list(all_indices - bad_indices))\n",
        "    GRND_NEW = GRND[good_indices]\n",
        "    PRED_NEW = Pred[good_indices]\n",
        "\n",
        "    return GRND_NEW, PRED_NEW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5LZy8284oN9"
      },
      "source": [
        "[A,B] = Construction_Error(Y_Test, GRF_pred[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1P2HQPM4oN9"
      },
      "source": [
        "# Infinite loop to keep colab alive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HEA3DYo4oN9"
      },
      "source": [
        "while True:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}