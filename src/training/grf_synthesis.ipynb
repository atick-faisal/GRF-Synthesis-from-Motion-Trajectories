{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get 1D-CNN lib from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/atick-faisal/GRF-Synthesis-from-Motion-Trajectories.git\n",
    "%cd /content/GRF-Synthesis-from-Motion-Trajectories/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import h5py\n",
    "import scipy\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from models import UNet\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!gdown --id \"1F3crQEwoV_nmBrlHAXZNera58zUkYVU6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = 2\n",
    "FRAME_LEN = 1024\n",
    "\n",
    "data = joblib.load(f'data{DATASET_ID}.joblib')\n",
    "features = data['X']\n",
    "target = data['y']\n",
    "\n",
    "if DATASET_ID == 1:\n",
    "    features_z = features[['fal_z', 'tam_z', 'fcc_z', 'fm1_z', 'fm2_z', 'fm5_z']].to_numpy()\n",
    "else:\n",
    "    features_z = features[['7_z', '8_z', '9_z', '10_z', '11_z']].to_numpy()\n",
    "\n",
    "N_CHANNELS = features_z.shape[1]\n",
    "\n",
    "# feature_scaler_1 = StandardScaler()\n",
    "feature_scaler_2 = MinMaxScaler()\n",
    "# feature_scaler_1.fit(features_z)\n",
    "# features_z = feature_scaler_1.transform(features_z)\n",
    "feature_scaler_2.fit(features_z)\n",
    "features_z = feature_scaler_2.transform(features_z)\n",
    "\n",
    "# target_scaler_1 = StandardScaler()\n",
    "target_scaler_2 = MinMaxScaler()\n",
    "# target_scaler_1.fit(target_z)\n",
    "# target_z = target_scaler_1.transform(target_z)\n",
    "target_scaler_2.fit(target_z)\n",
    "target_z = target_scaler_2.transform(target_z)\n",
    "\n",
    "X = features_z.reshape(-1, FRAME_LEN, N_CHANNELS)\n",
    "y = target_z.reshape(-1, FRAME_LEN, 1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    plt.plot(X[i, :, 0])\n",
    "plt.sshow()\n",
    "\n",
    "for i in range(1800,1830):\n",
    "    plt.plot(y[i, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Y_Train = np.squeeze(Y_Train, axis=2)\n",
    "Y_Test = np.squeeze(Y_Test, axis=2)\n",
    "Y_Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_length = FRAME_LEN # Length of each Segment\n",
    "model_depth = 3 # Number of Level in the CNN Model\n",
    "model_width = 64 # Width of the Initial Layer, subsequent layers start from here\n",
    "kernel_size = 3 # Size of the Kernels/Filter\n",
    "num_channel = N_CHANNELS # Number of Channels in the Model\n",
    "D_S = 1 # Turn on Deep Supervision\n",
    "A_E = 0 # Turn on AutoEncoder Mode for Feature Extraction\n",
    "problem_type = 'Regression'\n",
    "output_nums = 1 # Number of Class for Classification Problems, always '1' for Regression Problems\n",
    "'''Only required if the AutoEncoder Mode is turned on'''\n",
    "feature_number = 1024 # Number of Features to be Extracted\n",
    "'''Only required for MultiResUNet'''\n",
    "alpha = 1 # Model Width Expansion Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for deep supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
    "  def approximate(inp, w_len, signal_length):\n",
    "    op = np.zeros((len(inp),signal_length//w_len))\n",
    "    for i in range(0,signal_length,w_len):\n",
    "      try:\n",
    "        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        print(i)\n",
    "  \t\n",
    "    return op\n",
    "\n",
    "  out = {}\n",
    "  Y_Train_dict = {}\n",
    "  out['out'] = np.array(y)\n",
    "  Y_Train_dict['out'] = out['out']\n",
    "  for i in range(1, (model_depth+1)):\n",
    "    name = f'level{i}'\n",
    "    if ((model_name == 'UNet') or (model_name == 'MultiResUNet') or (model_name == 'FPN')):\n",
    "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n",
    "    elif ((model_name == 'UNetE') or (model_name == 'UNetP') or (model_name == 'UNetPP')):\n",
    "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**0, signal_length),axis = 2)\n",
    "    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
    "  \n",
    "  return out, Y_Train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'UNet' # UNet or UNetPP\n",
    "X_Train1 = X_Train\n",
    "X_Test1 = X_Test\n",
    "[Y_Train1, Y_Train_dict] = prepareTrainDict(Y_Train, model_depth, signal_length, model_name)\n",
    "[Y_Test1, Y_Test_dict] = prepareTrainDict(Y_Test, model_depth, signal_length, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = np.zeros(model_depth)\n",
    "\n",
    "for i in range(0, model_depth):\n",
    "   loss_weights[i] = 1-(i*0.1)\n",
    "   \n",
    "loss_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate = 0.001\n",
    ")\n",
    "\n",
    "if D_S == 0:\n",
    "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
    "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
    "    # D_S = 0, A_E = 0\n",
    "    model = UNet(\n",
    "        signal_length,\n",
    "        model_depth,\n",
    "        num_channel, \n",
    "        model_width, \n",
    "        kernel_size, \n",
    "        problem_type=problem_type, \n",
    "        output_nums=output_nums, \n",
    "        ds=D_S, ae=A_E, \n",
    "        alpha=alpha\n",
    "    ).UNet()\n",
    "\n",
    "    model.compile(\n",
    "        loss= 'mean_absolute_error', \n",
    "        optimizer= optimizer, \n",
    "        metrics= ['mean_squared_error','accuracy']\n",
    "    )\n",
    "    #\n",
    "elif D_S == 1:\n",
    "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
    "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
    "    # D_S = 1, A_E = 0\n",
    "    model = UNet(\n",
    "        signal_length, model_depth, \n",
    "        num_channel, \n",
    "        model_width, \n",
    "        kernel_size, \n",
    "        problem_type=problem_type, \n",
    "        output_nums=output_nums, \n",
    "        ds=D_S, ae=A_E, \n",
    "        alpha=alpha\n",
    "    ).UNet()\n",
    "\n",
    "    model.compile(\n",
    "        loss= 'mean_absolute_error', \n",
    "        optimizer= optimizer, \n",
    "        metrics= ['mean_squared_error'], \n",
    "        loss_weights= loss_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if D_S == 0:\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min'), \n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'trained_models/'+model_name+'_'+str(signal_length)+'_'+str(model_width)+'_' \\\n",
    "                +str(model_depth)+'_'+str(num_channel)+'_'+str(D_S)+'.h5', \n",
    "                verbose=1, \n",
    "                monitor='val_loss', \n",
    "                save_best_only=True, \n",
    "                mode='min'\n",
    "        )\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_Train, Y_Train, \n",
    "        epochs=300, \n",
    "        batch_size=64, \n",
    "        verbose=1, \n",
    "        validation_split=0.2, \n",
    "        shuffle=True, \n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "elif D_S == 1:\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min'), \n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'trained_models/'+model_name+'_'+str(signal_length)+'_'+str(model_width)+'_' \\\n",
    "                +str(model_depth)+'_'+str(num_channel)+'_'+str(D_S)+'.h5', \n",
    "                verbose=1, \n",
    "                monitor='val_out_loss', \n",
    "                save_best_only=True, \n",
    "                mode='min'\n",
    "        )\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_Train1, \n",
    "        Y_Train_dict, \n",
    "        epochs=300, \n",
    "        batch_size=64, \n",
    "        verbose=1, \n",
    "        validation_split=0.2, \n",
    "        shuffle=True, \n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if D_S == 0:\n",
    "    GRF_pred = model.predict(X_Test, verbose=1)\n",
    "    print(GRF_pred.shape)\n",
    "elif D_S == 1:\n",
    "    GRF_pred = model.predict(X_Test1, verbose=1)\n",
    "    print(GRF_pred[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    # list all dictionaries in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for error\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(history.history['out_mean_squared_error'])\n",
    "    plt.plot(history.history['val_out_mean_squared_error'])\n",
    "    plt.title('Model Error Performance')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    #   plt.ylim([0, 3])\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(history.history['out_loss'])\n",
    "    plt.plot(history.history['val_out_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    #   plt.ylim([0, 3])\n",
    "    plt.show()\n",
    "#\n",
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if D_S == 0:\n",
    "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(6):\n",
    "        y_true = Y_Test[random_idx[i]]\n",
    "        y_pred = GRF_pred[random_idx[i]]\n",
    "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
    "        plt.subplot(3, 2, i + 1)\n",
    "        plt.plot(y_true, label='Ground Truth')\n",
    "        plt.plot(y_pred.ravel(), label='Prediction')\n",
    "        plt.title(f\"vGRF -- Sample Number {random_idx[i]} -- MAE = {MAE}\")\n",
    "        plt.legend();\n",
    "    plt.show()\n",
    "    \n",
    "elif D_S == 1:\n",
    "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(6):\n",
    "        y_true = Y_Test[random_idx[i]]\n",
    "        y_pred = GRF_pred[0][random_idx[i]]\n",
    "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
    "        plt.subplot(3, 2, i + 1)\n",
    "        plt.plot(y_true, label='Ground Truth')\n",
    "        plt.plot(y_pred.ravel(), label='Prediction')\n",
    "        plt.title(f\"vGRF -- Sample Number {random_idx[i]} -- MAE = {MAE}\")\n",
    "        plt.legend();\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def Construction_Error(GRND, Pred):\n",
    "    construction_err = []\n",
    "    bad_indices = []\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(GRND)):\n",
    "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
    "        if MAE < 1:\n",
    "            construction_err.append(MAE)\n",
    "        elif MAE >= 1:\n",
    "            count = count + 1\n",
    "            bad_indices.append(i)\n",
    "\n",
    "    print(f'Construction Error : {round(np.mean(construction_err), 3)} +/- {round(np.std(construction_err), 3)}')\n",
    "    print(f'Number of Bad Predictions = {count}')\n",
    "\n",
    "    bad_indices = set(bad_indices)\n",
    "    all_indices = set(np.arange(len(GRND)))\n",
    "    good_indices = np.array(list(all_indices - bad_indices))\n",
    "    GRND_NEW = GRND[good_indices]\n",
    "    PRED_NEW = Pred[good_indices]\n",
    "\n",
    "    return GRND_NEW, PRED_NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[A,B] = Construction_Error(Y_Test, GRF_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite loop to keep colab alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9e8b413bd195f698761fbb7f1cc8940f40efdff75a04973931ad61a6fc56074"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
