{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "grf_synthesis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "b9e8b413bd195f698761fbb7f1cc8940f40efdff75a04973931ad61a6fc56074"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/GRF-Synthesis-from-Motion-Trajectories/blob/main/src/training/grf_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ny5F1D53uh9Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_r50aIez0f"
      },
      "source": [
        "# !pip install --upgrade pandas==1.3.4"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j15cTL2I4oNu"
      },
      "source": [
        "# Download and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJcRThWM5LF5"
      },
      "source": [
        "# !gdown --id \"1kNGwWBQp6kmAioLP-_zWx7r42SzoEO3v\"\n",
        "# !unzip -u UNet.zip\n",
        "\n",
        "# !gdown --id \"1-0-jQ_yS6ojnosNeEYpy8f5tnpYLAd5b\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6XtngBS4oNt"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kpZ_68Q43j9"
      },
      "source": [
        "import h5py\n",
        "import scipy\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "from UNet_1DCNN import UNet\n",
        "from FPN_1DCNN import FPN\n",
        "from AlbuNet_1DCNN import AlbUNet\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style(\"darkgrid\", {'font.family':'serif', 'font.serif':'Times New Roman'})\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/DB2/ModelsY/'\n",
        "figures_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/DB2/FiguresY/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corr-Coef Loss"
      ],
      "metadata": {
        "id": "t9atbUC7ANA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "def correlation_coefficient_loss(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\n",
        "    error = mae(y_true, y_pred)\n",
        "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "    \n",
        "    return (1 - r) * error"
      ],
      "metadata": {
        "id": "8y2FYextAP7c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCAfz544oNz"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwNGZwEg4oNz",
        "outputId": "4ed26e9e-8958-4620-f8f5-32f92ad6dfd7"
      },
      "source": [
        "# data = joblib.load('/content/data2_f72_t3_n1_combined.joblib')\n",
        "data = joblib.load('/content/data2_f15_t6_n1_all_vel_combined.joblib')\n",
        "X_Train = data['train_X']\n",
        "X_Test = data['test_X']\n",
        "Y_Train = data['train_y']\n",
        "Y_Test = data['test_y']\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3136, 1024, 15)\n",
            "(784, 1024, 15)\n",
            "(3136, 1024, 6)\n",
            "(784, 1024, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Axis Selection"
      ],
      "metadata": {
        "id": "k_ePJvmtegj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = X_Train[:, :, [10, 11, 12, 13, 14]]\n",
        "X_Test = X_Test[:, :, [10, 11, 12, 13, 14]]\n",
        "Y_Train = Y_Train[:, :, 4]\n",
        "Y_Test = Y_Test[:, :, 4]\n",
        "\n",
        "X_Train = np.nan_to_num(X_Train)\n",
        "X_Test = np.nan_to_num(X_Test)\n",
        "Y_Train = np.nan_to_num(Y_Train)\n",
        "Y_Test = np.nan_to_num(Y_Test)\n",
        "\n",
        "print(X_Train.shape)\n",
        "print(X_Test.shape)\n",
        "print(Y_Train.shape)\n",
        "print(Y_Test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZKBWCVTeFFC",
        "outputId": "e7c5be47-b4f4-442e-ea28-1c0fdaa6ac5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3136, 1024, 5)\n",
            "(784, 1024, 5)\n",
            "(3136, 1024)\n",
            "(784, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(Y_Train[:, :].T)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "avon12Ydqs5J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_LEN = X_Train.shape[1]\n",
        "N_CHANNELS = X_Train.shape[2]"
      ],
      "metadata": {
        "id": "RIHHro2IeqP9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1FRPZ74oN0"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30i1r3Ln4oN0"
      },
      "source": [
        "model_name = 'MultiResUNet' # UNet or UNetPP\n",
        "signal_length = FRAME_LEN # Length of each Segment\n",
        "model_depth = 5 # Number of Level in the CNN Model\n",
        "model_width = 64 # Width of the Initial Layer, subsequent layers start from here\n",
        "kernel_size = 3 # Size of the Kernels/Filter\n",
        "num_channel = N_CHANNELS # Number of Channels in the Model\n",
        "D_S = 1 # Turn on Deep Supervision\n",
        "A_E = 0 # Turn on AutoEncoder Mode for Feature Extraction\n",
        "A_G = 1 # Attention Guided\n",
        "problem_type = 'Regression'\n",
        "output_nums = 1 # Number of Class for Classification Problems, always '1' for Regression Problems\n",
        "'''Only required if the AutoEncoder Mode is turned on'''\n",
        "feature_number = 1024 # Number of Features to be Extracted\n",
        "'''Only required for MultiResUNet'''\n",
        "alpha = 1 # Model Width Expansion Parameter"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdOOVOa4oN2"
      },
      "source": [
        "# Prepare for deep supervision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c61dm5v94oN3"
      },
      "source": [
        "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
        "  def approximate(inp, w_len, signal_length):\n",
        "    op = np.zeros((len(inp),signal_length//w_len))\n",
        "    for i in range(0,signal_length,w_len):\n",
        "      try:\n",
        "        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(i)\n",
        "  \t\n",
        "    return op\n",
        "\n",
        "  out = {}\n",
        "  Y_Train_dict = {}\n",
        "  out['out'] = np.array(y)\n",
        "  Y_Train_dict['out'] = out['out']\n",
        "  for i in range(1, (model_depth+1)):\n",
        "    name = f'level{i}'\n",
        "    if ((model_name == 'UNet') or (model_name == 'MultiResUNet') or (model_name == 'FPN')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n",
        "    elif ((model_name == 'UNetE') or (model_name == 'UNetP') or (model_name == 'UNetPP')):\n",
        "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**0, signal_length),axis = 2)\n",
        "    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
        "  \n",
        "  return out, Y_Train_dict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQqyFLjl4oN4"
      },
      "source": [
        "X_Train1 = X_Train\n",
        "X_Test1 = X_Test\n",
        "[Y_Train1, Y_Train_dict] = prepareTrainDict(Y_Train, model_depth, signal_length, model_name)\n",
        "[Y_Test1, Y_Test_dict] = prepareTrainDict(Y_Test, model_depth, signal_length, model_name)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_V8_Ht-4oN5",
        "outputId": "70009604-285e-4c4b-c1dd-5ad85ec02aa1"
      },
      "source": [
        "loss_weights = np.zeros(model_depth)\n",
        "\n",
        "for i in range(0, model_depth):\n",
        "   loss_weights[i] = 1-(i*0.1)\n",
        "   \n",
        "loss_weights"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 0.9, 0.8, 0.7, 0.6])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK_K_tyy4oN5"
      },
      "source": [
        "# Build and compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-nTNzLM4oN5"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate = 0.001\n",
        ")\n",
        "\n",
        "if D_S == 0:\n",
        "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "    # D_S = 0, A_E = 0\n",
        "    model = UNet(\n",
        "        signal_length,\n",
        "        model_depth,\n",
        "        num_channel, \n",
        "        model_width, \n",
        "        kernel_size, \n",
        "        problem_type=problem_type, \n",
        "        output_nums=output_nums, \n",
        "        ds=D_S,\n",
        "        ae=A_E, \n",
        "        ag=A_G,\n",
        "        alpha=alpha\n",
        "    ).UNet()\n",
        "\n",
        "    model.compile(\n",
        "        loss= 'mean_absolute_error', \n",
        "        optimizer= optimizer, \n",
        "        metrics= ['mean_squared_error','accuracy']\n",
        "    )\n",
        "    #\n",
        "elif D_S == 1:\n",
        "    # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "    # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "    # D_S = 1, A_E = 0\n",
        "    model = UNet(\n",
        "        signal_length, model_depth, \n",
        "        num_channel, \n",
        "        model_width, \n",
        "        kernel_size, \n",
        "        problem_type=problem_type, \n",
        "        output_nums=output_nums, \n",
        "        ds=D_S,\n",
        "        ae=A_E,\n",
        "        ag=A_G, \n",
        "        alpha=alpha\n",
        "    ).MultiResUNet()\n",
        "\n",
        "    model.compile(\n",
        "        loss= 'mean_absolute_error', \n",
        "        optimizer= optimizer, \n",
        "        metrics= ['mean_squared_error'], \n",
        "        loss_weights= loss_weights\n",
        "    )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlLB1flN4oN6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = model_name + '_moment_' + str(signal_length) + '_' \\\n",
        "            + str(N_CHANNELS) + '_' + str(model_width) +'_' \\\n",
        "            + str(model_depth) + '_' + str(num_channel) + '_all_vel_' \\\n",
        "            + str(D_S) + str(A_G) + '_axis_corrected'"
      ],
      "metadata": {
        "id": "IoH2_k-yPFkP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7URRYEK4oN6",
        "outputId": "c923a4f6-b705-460d-c831-edfbcad4b444"
      },
      "source": [
        "if D_S == 0:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config +'.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train, Y_Train, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "elif D_S == 1:\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min'), \n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'trained_models/' + config + '.h5', \n",
        "                verbose=1, \n",
        "                monitor='val_loss', \n",
        "                save_best_only=True, \n",
        "                mode='min'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        X_Train1, \n",
        "        Y_Train_dict, \n",
        "        epochs=300, \n",
        "        batch_size=64, \n",
        "        verbose=1, \n",
        "        validation_split=0.2, \n",
        "        shuffle=True, \n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MflY4cE04oN7"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-4OM5vr4oN7"
      },
      "source": [
        "if D_S == 0:\n",
        "    GRF_pred = model.predict(X_Test, verbose=1)\n",
        "    print(GRF_pred.shape)\n",
        "elif D_S == 1:\n",
        "    GRF_pred = model.predict(X_Test1, verbose=1)\n",
        "    print(GRF_pred[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_mean = np.mean(Y_Test, axis=0)\n",
        "ground_truth_std = np.std(Y_Test, axis=0)\n",
        "prediction = np.nan_to_num(GRF_pred[0][:, :, 0])\n",
        "prediction_mean = np.mean(prediction, axis=0)\n",
        "prediction_std = np.std(prediction, axis=0)"
      ],
      "metadata": {
        "id": "moGbBtKgG_MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.corrcoef(ground_truth_mean, prediction_mean)[0, 1]"
      ],
      "metadata": {
        "id": "3Kl-nvENMN-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "x = np.linspace(0, 100, FRAME_LEN)\n",
        "plt.plot(x, ground_truth_mean, linewidth=3, label='Ground Truth')\n",
        "plt.fill_between(\n",
        "    x, ground_truth_mean + ground_truth_std,\n",
        "    ground_truth_mean - ground_truth_std, alpha=0.3,\n",
        "    label='Ground Truth +/- STD'\n",
        ")\n",
        "plt.plot(x, prediction_mean, linewidth=3, label='Prediction')\n",
        "plt.fill_between(\n",
        "    x, prediction_mean + prediction_std,\n",
        "    prediction_mean - prediction_std, alpha=0.3,\n",
        "    label='Prediction +/- STD'\n",
        ")\n",
        "plt.legend()\n",
        "plt.title(f'R = {round(R, 5)}')\n",
        "plt.xlabel('Stance Phase (%)')\n",
        "plt.ylabel('Moment Nm (Normalized)')\n",
        "plt.savefig(figures_dir + 'Results_' + config + '.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WcIRU_sFH-WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "cVQTMzbguxgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copy\n",
        "copy('trained_models/' + config + '.h5', models_dir)\n",
        "joblib.dump(history, models_dir + \\\n",
        "            'History_' + config + '.joblib')"
      ],
      "metadata": {
        "id": "kOt1Gg7SutXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsW0dNyN4oN7"
      },
      "source": [
        "# Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Z2otb84oN7"
      },
      "source": [
        "def history_plot(history):\n",
        "    # list all dictionaries in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for error\n",
        "    plt.figure(figsize=(11,7))\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(history.history['out_mean_squared_error'])\n",
        "    plt.plot(history.history['val_out_mean_squared_error'])\n",
        "    plt.title('Model Error Performance')\n",
        "    plt.ylabel('Error')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    #   plt.ylim([0, 3])\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.figure(figsize=(11,7))\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(history.history['out_loss'])\n",
        "    plt.plot(history.history['val_out_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    #   plt.ylim([0, 3])\n",
        "    plt.savefig(figures_dir + 'LC_' + config + '.png')\n",
        "    plt.show()\n",
        "#\n",
        "history_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYBK8ann4oN8"
      },
      "source": [
        "# Visualize outcomes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f-mpSA34oN8"
      },
      "source": [
        "if D_S == 0:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(14, 9))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth')\n",
        "        plt.plot(y_pred.ravel(), label='Prediction')\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {MAE}\")\n",
        "        plt.legend();\n",
        "    plt.show()\n",
        "    \n",
        "elif D_S == 1:\n",
        "    random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "    plt.figure(figsize=(14, 9))\n",
        "    for i in range(6):\n",
        "        y_true = Y_Test[random_idx[i]]\n",
        "        y_pred = GRF_pred[0][random_idx[i]]\n",
        "        MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.plot(y_true, label='Ground Truth', linewidth=3)\n",
        "        plt.plot(y_pred.ravel(), label='Prediction', linewidth=3)\n",
        "        plt.title(f\"MAE [{random_idx[i]}] : {round(MAE, 4)}\")\n",
        "        plt.legend()\n",
        "    plt.savefig(figures_dir + 'Examples_Fy_' + config + '.png')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3elq2yvb4oN8"
      },
      "source": [
        "# Construction Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tivpIeIt738C"
      },
      "source": [
        "def Construction_Error(GRND, Pred):\n",
        "    construction_err = []\n",
        "    rmse = []\n",
        "    corr_coef = []\n",
        "    bad_indices = []\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(GRND)):\n",
        "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
        "        RMSE = mean_squared_error(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel(), squared=False)\n",
        "        R = np.corrcoef(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel())[0, 1]\n",
        "        if MAE < 1:\n",
        "            construction_err.append(MAE)\n",
        "            rmse.append(RMSE)\n",
        "            corr_coef.append(R)\n",
        "        elif MAE >= 1:\n",
        "            count = count + 1\n",
        "            bad_indices.append(i)\n",
        "\n",
        "    print(f'Construction Error : {round(np.mean(construction_err), 3)} +/- {round(np.std(construction_err), 3)}')\n",
        "    print(f'RMSE : {round(np.mean(rmse), 3)} +/- {round(np.std(rmse), 3)}')\n",
        "    print(f'R : {round(np.mean(corr_coef), 3)} +/- {round(np.std(corr_coef), 3)}')\n",
        "    print(f'Number of Bad Predictions = {count}')\n",
        "\n",
        "    bad_indices = set(bad_indices)\n",
        "    all_indices = set(np.arange(len(GRND)))\n",
        "    good_indices = np.array(list(all_indices - bad_indices))\n",
        "    # GRND_NEW = GRND[good_indices]\n",
        "    # PRED_NEW = Pred[good_indices]\n",
        "\n",
        "    # return GRND_NEW, PRED_NEW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5LZy8284oN9"
      },
      "source": [
        "[A,B] = Construction_Error(Y_Test, GRF_pred[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1P2HQPM4oN9"
      },
      "source": [
        "# Infinite loop to keep colab alive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HEA3DYo4oN9"
      },
      "source": [
        "# while True:\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}