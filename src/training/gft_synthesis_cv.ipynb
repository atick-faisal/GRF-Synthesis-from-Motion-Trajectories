{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/GRF-Synthesis-from-Motion-Trajectories/blob/main/src/training/gft_synthesis_cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "we6vHagM4QRn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M47TX4sRxgzB"
      },
      "outputs": [],
      "source": [
        "# !gdown --id \"1kNGwWBQp6kmAioLP-_zWx7r42SzoEO3v\"\n",
        "# !unzip -u UNet.zip\n",
        "\n",
        "# DB2\n",
        "# !gdown --id \"19ysOVgso3enZZTCeCmKTouBdRo1UFSz0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pandas==1.3.4"
      ],
      "metadata": {
        "id": "44pb2CiA2dIJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DbfaqO_ixgzD"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import scipy\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "from UNet_1DCNN import UNet\n",
        "from FPN_1DCNN import FPN\n",
        "from AlbuNet_1DCNN import AlbUNet\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style(\"darkgrid\", {'font.family': 'serif',\n",
        "              'font.serif': 'Times New Roman'})\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/Results/DB2/Force/Models/X/'\n",
        "figures_dir = '/content/drive/MyDrive/Research/GRF Data Synthesis/Results/DB2/Force/Figures/X/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GYMFxKYvxgzE"
      },
      "outputs": [],
      "source": [
        "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
        "    def approximate(inp, w_len, signal_length):\n",
        "        op = np.zeros((len(inp), signal_length//w_len))\n",
        "        for i in range(0, signal_length, w_len):\n",
        "            try:\n",
        "                op[:, i//w_len] = np.mean(inp[:, i:i+w_len], axis=1)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(i)\n",
        "\n",
        "        return op\n",
        "\n",
        "    out = {}\n",
        "    Y_Train_dict = {}\n",
        "    out['out'] = np.array(y)\n",
        "    Y_Train_dict['out'] = out['out']\n",
        "    for i in range(1, (model_depth+1)):\n",
        "        name = f'level{i}'\n",
        "        if ((model_name == 'UNet') or (model_name == 'MultiResUNet') or (model_name == 'FPN')):\n",
        "            out[name] = np.expand_dims(approximate(\n",
        "                np.squeeze(y), 2**i, signal_length), axis=2)\n",
        "        elif ((model_name == 'UNetE') or (model_name == 'UNetP') or (model_name == 'UNetPP')):\n",
        "            out[name] = np.expand_dims(approximate(\n",
        "                np.squeeze(y), 2**0, signal_length), axis=2)\n",
        "        Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
        "\n",
        "    return out, Y_Train_dict\n",
        "\n",
        "\n",
        "def Construction_Error(GRND, Pred):\n",
        "    construction_err = []\n",
        "    rmse = []\n",
        "    corr_coef = []\n",
        "    corr_coef2 = []\n",
        "    bad_indices = []\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(GRND)):\n",
        "        MAE = np.mean(np.abs(Pred[i].ravel() - GRND[i].ravel()))\n",
        "        RMSE = mean_squared_error(np.nan_to_num(\n",
        "            Pred[i].ravel()), GRND[i].ravel(), squared=False)\n",
        "        R = np.corrcoef(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel())[0, 1]\n",
        "        R2 = pearsonr(np.nan_to_num(Pred[i].ravel()), GRND[i].ravel())[0]\n",
        "        if MAE < 1:\n",
        "            construction_err.append(MAE)\n",
        "            rmse.append(RMSE)\n",
        "            corr_coef.append(R)\n",
        "            corr_coef2.append(R2)\n",
        "\n",
        "        elif MAE >= 1:\n",
        "            count = count + 1\n",
        "            bad_indices.append(i)\n",
        "\n",
        "    print(\n",
        "        f'Construction Error : {round(np.mean(construction_err), 3)} +/- {round(np.std(construction_err), 3)}')\n",
        "    print(f'RMSE : {round(np.mean(rmse), 3)} +/- {round(np.std(rmse), 3)}')\n",
        "    print(\n",
        "        f'R1 : {round(np.mean(corr_coef), 3)} +/- {round(np.std(corr_coef), 3)}')\n",
        "    print(\n",
        "        f'R2 : {round(np.mean(corr_coef2), 3)} +/- {round(np.std(corr_coef2), 3)}')\n",
        "    print(f'Number of Bad Predictions = {count}')\n",
        "\n",
        "    bad_indices = set(bad_indices)\n",
        "    all_indices = set(np.arange(len(GRND)))\n",
        "    good_indices = np.array(list(all_indices - bad_indices))\n",
        "    GRND_NEW = GRND[good_indices]\n",
        "    PRED_NEW = Pred[good_indices]\n",
        "\n",
        "    return construction_err, rmse, corr_coef, corr_coef2\n",
        "\n",
        "def history_plot(history):\n",
        "        # list all dictionaries in history\n",
        "        print(history.history.keys())\n",
        "        # summarize history for error\n",
        "        plt.figure(figsize=(11,14))\n",
        "        plt.subplot(2,1,1)\n",
        "        plt.plot(history.history['out_mean_squared_error'], linewidth=2)\n",
        "        plt.plot(history.history['val_out_mean_squared_error'], linewidth=2)\n",
        "        plt.title('Model Error Performance')\n",
        "        plt.ylabel('Error (MSE)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Val'], loc='upper right')\n",
        "        #   plt.ylim([0, 3])\n",
        "        # plt.savefig(figures_dir + 'EC_' + config + '.png')\n",
        "        # plt.show()\n",
        "        # summarize history for loss\n",
        "        # plt.figure(figsize=(11,7))\n",
        "        plt.subplot(2,1,2)\n",
        "        plt.plot(history.history['out_loss'], linewidth=2)\n",
        "        plt.plot(history.history['val_out_loss'], linewidth=2)\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Val'], loc='upper right')\n",
        "        #   plt.ylim([0, 3])\n",
        "        plt.savefig(figures_dir + 'LC_' + config + '.png')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eklObN0JxgzE"
      },
      "outputs": [],
      "source": [
        "model_name = 'MultiResUNet'  # UNet or UNetPP\n",
        "# signal_length = FRAME_LEN  # Length of each Segment\n",
        "model_depth = 5  # Number of Level in the CNN Model\n",
        "model_width = 64  # Width of the Initial Layer, subsequent layers start from here\n",
        "kernel_size = 3  # Size of the Kernels/Filter\n",
        "# num_channel = N_CHANNELS  # Number of Channels in the Model\n",
        "D_S = 1  # Turn on Deep Supervision\n",
        "A_E = 0  # Turn on AutoEncoder Mode for Feature Extraction\n",
        "A_G = 0  # Attention Guided\n",
        "problem_type = 'Regression'\n",
        "# Number of Class for Classification Problems, always '1' for Regression Problems\n",
        "output_nums = 1\n",
        "'''Only required if the AutoEncoder Mode is turned on'''\n",
        "feature_number = 1024  # Number of Features to be Extracted\n",
        "'''Only required for MultiResUNet'''\n",
        "alpha = 1  # Model Width Expansion Parameter\n",
        "\n",
        "EPOCHS = 300\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOAvMN3xgzF"
      },
      "source": [
        "# 5 Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HKu_qnHlxgzG",
        "outputId": "47730f5f-9043-4c0c-ba7f-4540fb7c4042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3920, 1024, 15)\n",
            "(3920, 1024, 6)\n"
          ]
        }
      ],
      "source": [
        "data = joblib.load(\"/content/data2_f15_t6_n1_all_vel.joblib\")\n",
        "features = data['X']\n",
        "target = data['y']\n",
        "\n",
        "X = features.to_numpy().reshape(-1, 1024, 15)\n",
        "y = target.to_numpy().reshape(-1, 1024, 6)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "X = X[:, :, [10, 11, 12, 13, 14]]\n",
        "y = y[:, :, 0] # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "X = np.nan_to_num(X)\n",
        "y = np.nan_to_num(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN57MJSOxgzG",
        "outputId": "0006771e-e226-4564-9965-5f22fff2f864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================== FOLD 1 ====================================\n",
            "(3136, 1024, 5)\n",
            "(784, 1024, 5)\n",
            "(3136, 1024)\n",
            "(784, 1024)\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.10061, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 - 73s - loss: 1.8940 - out_loss: 0.4255 - level1_loss: 0.3639 - level2_loss: 0.3089 - level3_loss: 0.3106 - level4_loss: 0.3581 - level5_loss: 0.6005 - out_mean_squared_error: 0.5478 - level1_mean_squared_error: 0.5427 - level2_mean_squared_error: 0.4153 - level3_mean_squared_error: 0.5979 - level4_mean_squared_error: 1.1400 - level5_mean_squared_error: 1.4890 - val_loss: 1.1006 - val_out_loss: 0.2636 - val_level1_loss: 0.2202 - val_level2_loss: 0.2178 - val_level3_loss: 0.2449 - val_level4_loss: 0.2170 - val_level5_loss: 0.2122 - val_out_mean_squared_error: 0.0891 - val_level1_mean_squared_error: 0.0714 - val_level2_mean_squared_error: 0.0729 - val_level3_mean_squared_error: 0.0791 - val_level4_mean_squared_error: 0.0751 - val_level5_mean_squared_error: 0.0759 - 73s/epoch - 2s/step\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.10061 to 1.09750, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 30s - loss: 0.6789 - out_loss: 0.1317 - level1_loss: 0.1266 - level2_loss: 0.1277 - level3_loss: 0.1318 - level4_loss: 0.1441 - level5_loss: 0.1867 - out_mean_squared_error: 0.0341 - level1_mean_squared_error: 0.0333 - level2_mean_squared_error: 0.0399 - level3_mean_squared_error: 0.0545 - level4_mean_squared_error: 0.0692 - level5_mean_squared_error: 0.0824 - val_loss: 1.0975 - val_out_loss: 0.2200 - val_level1_loss: 0.2286 - val_level2_loss: 0.2296 - val_level3_loss: 0.2524 - val_level4_loss: 0.2187 - val_level5_loss: 0.2226 - val_out_mean_squared_error: 0.0715 - val_level1_mean_squared_error: 0.0718 - val_level2_mean_squared_error: 0.0721 - val_level3_mean_squared_error: 0.0838 - val_level4_mean_squared_error: 0.0705 - val_level5_mean_squared_error: 0.0705 - 30s/epoch - 741ms/step\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.09750\n",
            "40/40 - 27s - loss: 0.6157 - out_loss: 0.1149 - level1_loss: 0.1155 - level2_loss: 0.1190 - level3_loss: 0.1285 - level4_loss: 0.1235 - level5_loss: 0.1683 - out_mean_squared_error: 0.0255 - level1_mean_squared_error: 0.0274 - level2_mean_squared_error: 0.0412 - level3_mean_squared_error: 0.1004 - level4_mean_squared_error: 0.0404 - level5_mean_squared_error: 0.1222 - val_loss: 1.1242 - val_out_loss: 0.2212 - val_level1_loss: 0.2330 - val_level2_loss: 0.2394 - val_level3_loss: 0.2630 - val_level4_loss: 0.2191 - val_level5_loss: 0.2294 - val_out_mean_squared_error: 0.0712 - val_level1_mean_squared_error: 0.0731 - val_level2_mean_squared_error: 0.0759 - val_level3_mean_squared_error: 0.0904 - val_level4_mean_squared_error: 0.0729 - val_level5_mean_squared_error: 0.0713 - 27s/epoch - 685ms/step\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.09750\n",
            "40/40 - 27s - loss: 0.5578 - out_loss: 0.1081 - level1_loss: 0.1095 - level2_loss: 0.1100 - level3_loss: 0.1130 - level4_loss: 0.1186 - level5_loss: 0.1380 - out_mean_squared_error: 0.0213 - level1_mean_squared_error: 0.0247 - level2_mean_squared_error: 0.0270 - level3_mean_squared_error: 0.0352 - level4_mean_squared_error: 0.0491 - level5_mean_squared_error: 0.0533 - val_loss: 1.1445 - val_out_loss: 0.2224 - val_level1_loss: 0.2369 - val_level2_loss: 0.2425 - val_level3_loss: 0.2727 - val_level4_loss: 0.2239 - val_level5_loss: 0.2321 - val_out_mean_squared_error: 0.0711 - val_level1_mean_squared_error: 0.0746 - val_level2_mean_squared_error: 0.0774 - val_level3_mean_squared_error: 0.0971 - val_level4_mean_squared_error: 0.0714 - val_level5_mean_squared_error: 0.0719 - 27s/epoch - 685ms/step\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.09750\n",
            "40/40 - 27s - loss: 0.5497 - out_loss: 0.1055 - level1_loss: 0.1078 - level2_loss: 0.1108 - level3_loss: 0.1101 - level4_loss: 0.1130 - level5_loss: 0.1399 - out_mean_squared_error: 0.0206 - level1_mean_squared_error: 0.0259 - level2_mean_squared_error: 0.0367 - level3_mean_squared_error: 0.0339 - level4_mean_squared_error: 0.0370 - level5_mean_squared_error: 0.0718 - val_loss: 1.1819 - val_out_loss: 0.2260 - val_level1_loss: 0.2452 - val_level2_loss: 0.2455 - val_level3_loss: 0.2864 - val_level4_loss: 0.2314 - val_level5_loss: 0.2430 - val_out_mean_squared_error: 0.0714 - val_level1_mean_squared_error: 0.0784 - val_level2_mean_squared_error: 0.0790 - val_level3_mean_squared_error: 0.1068 - val_level4_mean_squared_error: 0.0737 - val_level5_mean_squared_error: 0.0774 - 27s/epoch - 685ms/step\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.09750\n",
            "40/40 - 27s - loss: 0.5167 - out_loss: 0.1023 - level1_loss: 0.1032 - level2_loss: 0.1038 - level3_loss: 0.1054 - level4_loss: 0.1084 - level5_loss: 0.1227 - out_mean_squared_error: 0.0196 - level1_mean_squared_error: 0.0218 - level2_mean_squared_error: 0.0237 - level3_mean_squared_error: 0.0301 - level4_mean_squared_error: 0.0369 - level5_mean_squared_error: 0.0365 - val_loss: 1.1628 - val_out_loss: 0.2254 - val_level1_loss: 0.2438 - val_level2_loss: 0.2424 - val_level3_loss: 0.2762 - val_level4_loss: 0.2255 - val_level5_loss: 0.2401 - val_out_mean_squared_error: 0.0714 - val_level1_mean_squared_error: 0.0777 - val_level2_mean_squared_error: 0.0774 - val_level3_mean_squared_error: 0.0991 - val_level4_mean_squared_error: 0.0722 - val_level5_mean_squared_error: 0.0754 - 27s/epoch - 667ms/step\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.09750\n",
            "40/40 - 27s - loss: 0.5046 - out_loss: 0.1004 - level1_loss: 0.1009 - level2_loss: 0.1011 - level3_loss: 0.1042 - level4_loss: 0.1058 - level5_loss: 0.1184 - out_mean_squared_error: 0.0185 - level1_mean_squared_error: 0.0195 - level2_mean_squared_error: 0.0205 - level3_mean_squared_error: 0.0298 - level4_mean_squared_error: 0.0326 - level5_mean_squared_error: 0.0337 - val_loss: 1.1654 - val_out_loss: 0.2293 - val_level1_loss: 0.2531 - val_level2_loss: 0.2498 - val_level3_loss: 0.2835 - val_level4_loss: 0.2215 - val_level5_loss: 0.2195 - val_out_mean_squared_error: 0.0722 - val_level1_mean_squared_error: 0.0827 - val_level2_mean_squared_error: 0.0810 - val_level3_mean_squared_error: 0.1042 - val_level4_mean_squared_error: 0.0697 - val_level5_mean_squared_error: 0.0660 - 27s/epoch - 666ms/step\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.09750\n",
            "40/40 - 27s - loss: 0.4940 - out_loss: 0.0980 - level1_loss: 0.0987 - level2_loss: 0.0993 - level3_loss: 0.1003 - level4_loss: 0.1027 - level5_loss: 0.1185 - out_mean_squared_error: 0.0173 - level1_mean_squared_error: 0.0184 - level2_mean_squared_error: 0.0207 - level3_mean_squared_error: 0.0238 - level4_mean_squared_error: 0.0292 - level5_mean_squared_error: 0.0417 - val_loss: 1.1669 - val_out_loss: 0.2286 - val_level1_loss: 0.2464 - val_level2_loss: 0.2374 - val_level3_loss: 0.2705 - val_level4_loss: 0.2236 - val_level5_loss: 0.2522 - val_out_mean_squared_error: 0.0719 - val_level1_mean_squared_error: 0.0790 - val_level2_mean_squared_error: 0.0748 - val_level3_mean_squared_error: 0.0948 - val_level4_mean_squared_error: 0.0701 - val_level5_mean_squared_error: 0.0835 - 27s/epoch - 685ms/step\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.09750\n",
            "40/40 - 27s - loss: 0.5313 - out_loss: 0.1015 - level1_loss: 0.1023 - level2_loss: 0.1029 - level3_loss: 0.1045 - level4_loss: 0.1103 - level5_loss: 0.1425 - out_mean_squared_error: 0.0201 - level1_mean_squared_error: 0.0220 - level2_mean_squared_error: 0.0243 - level3_mean_squared_error: 0.0274 - level4_mean_squared_error: 0.0424 - level5_mean_squared_error: 0.1096 - val_loss: 1.1114 - val_out_loss: 0.2279 - val_level1_loss: 0.2475 - val_level2_loss: 0.2351 - val_level3_loss: 0.2547 - val_level4_loss: 0.2061 - val_level5_loss: 0.2179 - val_out_mean_squared_error: 0.0710 - val_level1_mean_squared_error: 0.0795 - val_level2_mean_squared_error: 0.0733 - val_level3_mean_squared_error: 0.0850 - val_level4_mean_squared_error: 0.0639 - val_level5_mean_squared_error: 0.0650 - 27s/epoch - 667ms/step\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.09750 to 1.04873, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 29s - loss: 0.5005 - out_loss: 0.0980 - level1_loss: 0.0988 - level2_loss: 0.1004 - level3_loss: 0.1020 - level4_loss: 0.1087 - level5_loss: 0.1177 - out_mean_squared_error: 0.0173 - level1_mean_squared_error: 0.0186 - level2_mean_squared_error: 0.0224 - level3_mean_squared_error: 0.0266 - level4_mean_squared_error: 0.0505 - level5_mean_squared_error: 0.0377 - val_loss: 1.0487 - val_out_loss: 0.2296 - val_level1_loss: 0.2441 - val_level2_loss: 0.2242 - val_level3_loss: 0.2366 - val_level4_loss: 0.1936 - val_level5_loss: 0.1829 - val_out_mean_squared_error: 0.0705 - val_level1_mean_squared_error: 0.0773 - val_level2_mean_squared_error: 0.0675 - val_level3_mean_squared_error: 0.0741 - val_level4_mean_squared_error: 0.0585 - val_level5_mean_squared_error: 0.0510 - 29s/epoch - 720ms/step\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.04873 to 0.97000, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 28s - loss: 0.4799 - out_loss: 0.0957 - level1_loss: 0.0964 - level2_loss: 0.0965 - level3_loss: 0.0996 - level4_loss: 0.0991 - level5_loss: 0.1124 - out_mean_squared_error: 0.0166 - level1_mean_squared_error: 0.0179 - level2_mean_squared_error: 0.0190 - level3_mean_squared_error: 0.0302 - level4_mean_squared_error: 0.0234 - level5_mean_squared_error: 0.0401 - val_loss: 0.9700 - val_out_loss: 0.2162 - val_level1_loss: 0.2273 - val_level2_loss: 0.2026 - val_level3_loss: 0.2170 - val_level4_loss: 0.1802 - val_level5_loss: 0.1693 - val_out_mean_squared_error: 0.0642 - val_level1_mean_squared_error: 0.0687 - val_level2_mean_squared_error: 0.0580 - val_level3_mean_squared_error: 0.0644 - val_level4_mean_squared_error: 0.0522 - val_level5_mean_squared_error: 0.0450 - 28s/epoch - 703ms/step\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.97000 to 0.84514, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 29s - loss: 0.4767 - out_loss: 0.0951 - level1_loss: 0.0959 - level2_loss: 0.0960 - level3_loss: 0.0974 - level4_loss: 0.1016 - level5_loss: 0.1098 - out_mean_squared_error: 0.0166 - level1_mean_squared_error: 0.0179 - level2_mean_squared_error: 0.0191 - level3_mean_squared_error: 0.0231 - level4_mean_squared_error: 0.0402 - level5_mean_squared_error: 0.0332 - val_loss: 0.8451 - val_out_loss: 0.1914 - val_level1_loss: 0.2002 - val_level2_loss: 0.1690 - val_level3_loss: 0.1813 - val_level4_loss: 0.1631 - val_level5_loss: 0.1515 - val_out_mean_squared_error: 0.0521 - val_level1_mean_squared_error: 0.0547 - val_level2_mean_squared_error: 0.0428 - val_level3_mean_squared_error: 0.0468 - val_level4_mean_squared_error: 0.0440 - val_level5_mean_squared_error: 0.0403 - 29s/epoch - 726ms/step\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.84514 to 0.77914, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 28s - loss: 0.4734 - out_loss: 0.0942 - level1_loss: 0.0946 - level2_loss: 0.0960 - level3_loss: 0.0972 - level4_loss: 0.0985 - level5_loss: 0.1113 - out_mean_squared_error: 0.0165 - level1_mean_squared_error: 0.0170 - level2_mean_squared_error: 0.0224 - level3_mean_squared_error: 0.0248 - level4_mean_squared_error: 0.0238 - level5_mean_squared_error: 0.0458 - val_loss: 0.7791 - val_out_loss: 0.1755 - val_level1_loss: 0.1850 - val_level2_loss: 0.1591 - val_level3_loss: 0.1676 - val_level4_loss: 0.1461 - val_level5_loss: 0.1407 - val_out_mean_squared_error: 0.0442 - val_level1_mean_squared_error: 0.0473 - val_level2_mean_squared_error: 0.0381 - val_level3_mean_squared_error: 0.0407 - val_level4_mean_squared_error: 0.0357 - val_level5_mean_squared_error: 0.0385 - 28s/epoch - 703ms/step\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.77914\n",
            "40/40 - 27s - loss: 0.4857 - out_loss: 0.0961 - level1_loss: 0.0967 - level2_loss: 0.0976 - level3_loss: 0.0985 - level4_loss: 0.1023 - level5_loss: 0.1160 - out_mean_squared_error: 0.0173 - level1_mean_squared_error: 0.0183 - level2_mean_squared_error: 0.0218 - level3_mean_squared_error: 0.0227 - level4_mean_squared_error: 0.0370 - level5_mean_squared_error: 0.0613 - val_loss: 0.8066 - val_out_loss: 0.1689 - val_level1_loss: 0.1829 - val_level2_loss: 0.1607 - val_level3_loss: 0.1834 - val_level4_loss: 0.1478 - val_level5_loss: 0.1646 - val_out_mean_squared_error: 0.0415 - val_level1_mean_squared_error: 0.0569 - val_level2_mean_squared_error: 0.0405 - val_level3_mean_squared_error: 0.0832 - val_level4_mean_squared_error: 0.0360 - val_level5_mean_squared_error: 0.0692 - 27s/epoch - 684ms/step\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.77914 to 0.75974, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 29s - loss: 0.4987 - out_loss: 0.0980 - level1_loss: 0.0996 - level2_loss: 0.1004 - level3_loss: 0.1025 - level4_loss: 0.1059 - level5_loss: 0.1170 - out_mean_squared_error: 0.0173 - level1_mean_squared_error: 0.0196 - level2_mean_squared_error: 0.0219 - level3_mean_squared_error: 0.0272 - level4_mean_squared_error: 0.0358 - level5_mean_squared_error: 0.0429 - val_loss: 0.7597 - val_out_loss: 0.1571 - val_level1_loss: 0.1662 - val_level2_loss: 0.1471 - val_level3_loss: 0.1718 - val_level4_loss: 0.1565 - val_level5_loss: 0.1510 - val_out_mean_squared_error: 0.0382 - val_level1_mean_squared_error: 0.0495 - val_level2_mean_squared_error: 0.0356 - val_level3_mean_squared_error: 0.0808 - val_level4_mean_squared_error: 0.0782 - val_level5_mean_squared_error: 0.0642 - 29s/epoch - 719ms/step\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.75974 to 0.68461, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 28s - loss: 0.4704 - out_loss: 0.0940 - level1_loss: 0.0944 - level2_loss: 0.0947 - level3_loss: 0.0963 - level4_loss: 0.1006 - level5_loss: 0.1081 - out_mean_squared_error: 0.0160 - level1_mean_squared_error: 0.0165 - level2_mean_squared_error: 0.0176 - level3_mean_squared_error: 0.0218 - level4_mean_squared_error: 0.0394 - level5_mean_squared_error: 0.0319 - val_loss: 0.6846 - val_out_loss: 0.1432 - val_level1_loss: 0.1497 - val_level2_loss: 0.1407 - val_level3_loss: 0.1445 - val_level4_loss: 0.1408 - val_level5_loss: 0.1368 - val_out_mean_squared_error: 0.0333 - val_level1_mean_squared_error: 0.0353 - val_level2_mean_squared_error: 0.0335 - val_level3_mean_squared_error: 0.0349 - val_level4_mean_squared_error: 0.0341 - val_level5_mean_squared_error: 0.0339 - 28s/epoch - 705ms/step\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.68461 to 0.65864, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 28s - loss: 0.4665 - out_loss: 0.0932 - level1_loss: 0.0937 - level2_loss: 0.0944 - level3_loss: 0.0951 - level4_loss: 0.0975 - level5_loss: 0.1092 - out_mean_squared_error: 0.0157 - level1_mean_squared_error: 0.0166 - level2_mean_squared_error: 0.0188 - level3_mean_squared_error: 0.0210 - level4_mean_squared_error: 0.0265 - level5_mean_squared_error: 0.0463 - val_loss: 0.6586 - val_out_loss: 0.1364 - val_level1_loss: 0.1401 - val_level2_loss: 0.1370 - val_level3_loss: 0.1374 - val_level4_loss: 0.1355 - val_level5_loss: 0.1369 - val_out_mean_squared_error: 0.0335 - val_level1_mean_squared_error: 0.0341 - val_level2_mean_squared_error: 0.0340 - val_level3_mean_squared_error: 0.0336 - val_level4_mean_squared_error: 0.0334 - val_level5_mean_squared_error: 0.0329 - 28s/epoch - 703ms/step\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.65864 to 0.65626, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 28s - loss: 0.4498 - out_loss: 0.0903 - level1_loss: 0.0905 - level2_loss: 0.0915 - level3_loss: 0.0921 - level4_loss: 0.0957 - level5_loss: 0.1021 - out_mean_squared_error: 0.0150 - level1_mean_squared_error: 0.0153 - level2_mean_squared_error: 0.0177 - level3_mean_squared_error: 0.0194 - level4_mean_squared_error: 0.0329 - level5_mean_squared_error: 0.0284 - val_loss: 0.6563 - val_out_loss: 0.1339 - val_level1_loss: 0.1375 - val_level2_loss: 0.1348 - val_level3_loss: 0.1368 - val_level4_loss: 0.1365 - val_level5_loss: 0.1408 - val_out_mean_squared_error: 0.0321 - val_level1_mean_squared_error: 0.0363 - val_level2_mean_squared_error: 0.0333 - val_level3_mean_squared_error: 0.0366 - val_level4_mean_squared_error: 0.0385 - val_level5_mean_squared_error: 0.0347 - 28s/epoch - 702ms/step\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.65626 to 0.62961, saving model to trained_models/GRF_F1_MultiResUNet1024_5_64_5_5_all_vel_10_axis_corrected.h5\n",
            "40/40 - 28s - loss: 0.4466 - out_loss: 0.0898 - level1_loss: 0.0909 - level2_loss: 0.0910 - level3_loss: 0.0921 - level4_loss: 0.0934 - level5_loss: 0.1010 - out_mean_squared_error: 0.0148 - level1_mean_squared_error: 0.0168 - level2_mean_squared_error: 0.0167 - level3_mean_squared_error: 0.0206 - level4_mean_squared_error: 0.0229 - level5_mean_squared_error: 0.0266 - val_loss: 0.6296 - val_out_loss: 0.1307 - val_level1_loss: 0.1320 - val_level2_loss: 0.1291 - val_level3_loss: 0.1301 - val_level4_loss: 0.1294 - val_level5_loss: 0.1356 - val_out_mean_squared_error: 0.0304 - val_level1_mean_squared_error: 0.0313 - val_level2_mean_squared_error: 0.0308 - val_level3_mean_squared_error: 0.0328 - val_level4_mean_squared_error: 0.0315 - val_level5_mean_squared_error: 0.0409 - 28s/epoch - 704ms/step\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.62961\n",
            "40/40 - 27s - loss: 0.4475 - out_loss: 0.0892 - level1_loss: 0.0896 - level2_loss: 0.0900 - level3_loss: 0.0905 - level4_loss: 0.0931 - level5_loss: 0.1070 - out_mean_squared_error: 0.0146 - level1_mean_squared_error: 0.0157 - level2_mean_squared_error: 0.0163 - level3_mean_squared_error: 0.0180 - level4_mean_squared_error: 0.0259 - level5_mean_squared_error: 0.0706 - val_loss: 0.6940 - val_out_loss: 0.1437 - val_level1_loss: 0.1431 - val_level2_loss: 0.1427 - val_level3_loss: 0.1422 - val_level4_loss: 0.1452 - val_level5_loss: 0.1505 - val_out_mean_squared_error: 0.0378 - val_level1_mean_squared_error: 0.0375 - val_level2_mean_squared_error: 0.0375 - val_level3_mean_squared_error: 0.0375 - val_level4_mean_squared_error: 0.0390 - val_level5_mean_squared_error: 0.0438 - 27s/epoch - 667ms/step\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.62961\n",
            "40/40 - 27s - loss: 0.5220 - out_loss: 0.0976 - level1_loss: 0.0975 - level2_loss: 0.1000 - level3_loss: 0.1041 - level4_loss: 0.1114 - level5_loss: 0.1419 - out_mean_squared_error: 0.0181 - level1_mean_squared_error: 0.0180 - level2_mean_squared_error: 0.0214 - level3_mean_squared_error: 0.0286 - level4_mean_squared_error: 0.0424 - level5_mean_squared_error: 0.0856 - val_loss: 0.7089 - val_out_loss: 0.1553 - val_level1_loss: 0.1481 - val_level2_loss: 0.1440 - val_level3_loss: 0.1443 - val_level4_loss: 0.1448 - val_level5_loss: 0.1496 - val_out_mean_squared_error: 0.0434 - val_level1_mean_squared_error: 0.0398 - val_level2_mean_squared_error: 0.0374 - val_level3_mean_squared_error: 0.0377 - val_level4_mean_squared_error: 0.0397 - val_level5_mean_squared_error: 0.0402 - 27s/epoch - 685ms/step\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.62961\n",
            "40/40 - 27s - loss: 0.4583 - out_loss: 0.0913 - level1_loss: 0.0917 - level2_loss: 0.0922 - level3_loss: 0.0928 - level4_loss: 0.0997 - level5_loss: 0.1052 - out_mean_squared_error: 0.0157 - level1_mean_squared_error: 0.0159 - level2_mean_squared_error: 0.0170 - level3_mean_squared_error: 0.0185 - level4_mean_squared_error: 0.0377 - level5_mean_squared_error: 0.0286 - val_loss: 0.6963 - val_out_loss: 0.1509 - val_level1_loss: 0.1453 - val_level2_loss: 0.1436 - val_level3_loss: 0.1450 - val_level4_loss: 0.1410 - val_level5_loss: 0.1446 - val_out_mean_squared_error: 0.0406 - val_level1_mean_squared_error: 0.0388 - val_level2_mean_squared_error: 0.0373 - val_level3_mean_squared_error: 0.0386 - val_level4_mean_squared_error: 0.0397 - val_level5_mean_squared_error: 0.0414 - 27s/epoch - 667ms/step\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.62961\n",
            "40/40 - 27s - loss: 0.4526 - out_loss: 0.0902 - level1_loss: 0.0905 - level2_loss: 0.0910 - level3_loss: 0.0918 - level4_loss: 0.0946 - level5_loss: 0.1076 - out_mean_squared_error: 0.0152 - level1_mean_squared_error: 0.0156 - level2_mean_squared_error: 0.0169 - level3_mean_squared_error: 0.0188 - level4_mean_squared_error: 0.0233 - level5_mean_squared_error: 0.0454 - val_loss: 0.6646 - val_out_loss: 0.1420 - val_level1_loss: 0.1382 - val_level2_loss: 0.1376 - val_level3_loss: 0.1372 - val_level4_loss: 0.1379 - val_level5_loss: 0.1377 - val_out_mean_squared_error: 0.0380 - val_level1_mean_squared_error: 0.0362 - val_level2_mean_squared_error: 0.0372 - val_level3_mean_squared_error: 0.0360 - val_level4_mean_squared_error: 0.0380 - val_level5_mean_squared_error: 0.0390 - 27s/epoch - 666ms/step\n",
            "Epoch 24/300\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=700)\n",
        "fold = 1\n",
        "\n",
        "results = pd.DataFrame()\n",
        "\n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_Train, X_Test = X[train_idx], X[test_idx]\n",
        "    Y_Train, Y_Test = y[train_idx], y[test_idx]\n",
        "\n",
        "    print(f\"==================================== FOLD {fold} ====================================\")\n",
        "\n",
        "    print(X_Train.shape)\n",
        "    print(X_Test.shape)\n",
        "    print(Y_Train.shape)\n",
        "    print(Y_Test.shape)\n",
        "\n",
        "    FRAME_LEN = X_Train.shape[1]\n",
        "    N_CHANNELS = X_Train.shape[2]\n",
        "    signal_length = FRAME_LEN\n",
        "    num_channel = N_CHANNELS\n",
        "\n",
        "    #########################################################################################\n",
        "    X_Train1 = X_Train\n",
        "    X_Test1 = X_Test\n",
        "    [Y_Train1, Y_Train_dict] = prepareTrainDict(\n",
        "        Y_Train, model_depth, signal_length, model_name)\n",
        "    [Y_Test1, Y_Test_dict] = prepareTrainDict(\n",
        "        Y_Test, model_depth, signal_length, model_name)\n",
        "    #########################################################################################\n",
        "    loss_weights = np.zeros(model_depth)\n",
        "    for i in range(0, model_depth):\n",
        "        loss_weights[i] = 1-(i*0.1)\n",
        "    loss_weights\n",
        "    #########################################################################################\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.001\n",
        "    )\n",
        "\n",
        "    if D_S == 0:\n",
        "        # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "        # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "        # D_S = 0, A_E = 0\n",
        "        model = UNet(\n",
        "            signal_length,\n",
        "            model_depth,\n",
        "            num_channel,\n",
        "            model_width,\n",
        "            kernel_size,\n",
        "            problem_type=problem_type,\n",
        "            output_nums=output_nums,\n",
        "            ds=D_S,\n",
        "            ae=A_E,\n",
        "            ag=A_G,\n",
        "            alpha=alpha\n",
        "        ).UNet()\n",
        "\n",
        "        model.compile(\n",
        "            loss='mean_absolute_error',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mean_squared_error', 'accuracy']\n",
        "        )\n",
        "        #\n",
        "    elif D_S == 1:\n",
        "        # Build model for EEG Motion Artifact Removal - Deep Unet Architecture\n",
        "        # AutoEncoder should be set at 'FALSE' for the Deep U-net model for Signal Reconstruction\n",
        "        # D_S = 1, A_E = 0\n",
        "        model = UNet(\n",
        "            signal_length, model_depth,\n",
        "            num_channel,\n",
        "            model_width,\n",
        "            kernel_size,\n",
        "            problem_type=problem_type,\n",
        "            output_nums=output_nums,\n",
        "            ds=D_S,\n",
        "            ae=A_E,\n",
        "            ag=A_G,\n",
        "            alpha=alpha\n",
        "        ).MultiResUNet() # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "        model.compile(\n",
        "            loss='mean_absolute_error',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mean_squared_error'],\n",
        "            loss_weights=loss_weights\n",
        "        )\n",
        "    #################################################################################\n",
        "    config = f'GRF_F{fold}_' +  model_name + str(signal_length) + '_' \\\n",
        "        + str(N_CHANNELS) + '_' + str(model_width) + '_' \\\n",
        "        + str(model_depth) + '_' + str(num_channel) + '_all_vel_' \\\n",
        "        + str(D_S) + str(A_G) + '_axis_corrected'\n",
        "    #################################################################################\n",
        "    if D_S == 0:\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, mode='min'), \n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                'trained_models/' + config +'.h5', \n",
        "                    verbose=1, \n",
        "                    monitor='val_loss', \n",
        "                    save_best_only=True, \n",
        "                    mode='min'\n",
        "            )\n",
        "        ]\n",
        "        history = model.fit(\n",
        "            X_Train, Y_Train, \n",
        "            epochs=EPOCHS, \n",
        "            batch_size=64, \n",
        "            verbose=1, \n",
        "            validation_split=0.2, \n",
        "            shuffle=True, \n",
        "            callbacks=callbacks\n",
        "        )\n",
        "        \n",
        "    elif D_S == 1:\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=30, mode='min'), \n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                'trained_models/' + config + '.h5', \n",
        "                    verbose=1, \n",
        "                    monitor='val_loss', \n",
        "                    save_best_only=True, \n",
        "                    mode='min'\n",
        "            )\n",
        "        ]\n",
        "        history = model.fit(\n",
        "            X_Train1, \n",
        "            Y_Train_dict, \n",
        "            epochs=EPOCHS, \n",
        "            batch_size=64, \n",
        "            verbose=2, \n",
        "            validation_split=0.2, \n",
        "            shuffle=True, \n",
        "            callbacks=callbacks\n",
        "        )\n",
        "    #################################################################################\n",
        "    if D_S == 0:\n",
        "        GRF_pred = model.predict(X_Test, verbose=1)\n",
        "        print(GRF_pred.shape)\n",
        "    elif D_S == 1:\n",
        "        GRF_pred = model.predict(X_Test1, verbose=1)\n",
        "        print(GRF_pred[0].shape)\n",
        "    #################################################################################\n",
        "    ground_truth_mean = np.mean(Y_Test, axis=0)\n",
        "    ground_truth_std = np.std(Y_Test, axis=0)\n",
        "    prediction = np.nan_to_num(GRF_pred[0][:, :, 0])\n",
        "    prediction_mean = np.mean(prediction, axis=0)\n",
        "    prediction_std = np.std(prediction, axis=0)\n",
        "    #################################################################################\n",
        "    R = np.corrcoef(ground_truth_mean, prediction_mean)[0, 1]\n",
        "    R2 = pearsonr(ground_truth_mean, prediction_mean)[0]\n",
        "    #################################################################################\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    x = np.linspace(0, 100, FRAME_LEN)\n",
        "    plt.plot(x, ground_truth_mean, linewidth=3, label='Ground Truth')\n",
        "    plt.fill_between(\n",
        "        x, ground_truth_mean + ground_truth_std,\n",
        "        ground_truth_mean - ground_truth_std, alpha=0.3,\n",
        "        label='Ground Truth +/- STD'\n",
        "    )\n",
        "    plt.plot(x, prediction_mean, linewidth=3, label='Prediction')\n",
        "    plt.fill_between(\n",
        "        x, prediction_mean + prediction_std,\n",
        "        prediction_mean - prediction_std, alpha=0.3,\n",
        "        label='Prediction +/- STD'\n",
        "    )\n",
        "    # plt.legend()\n",
        "    plt.title(f'R1 = {round(R, 5)}, R2 = {round(R2, 5)}')\n",
        "    # plt.xlabel('Stance Phase (%)')\n",
        "    # plt.ylabel('Moment Nm (Normalized)')\n",
        "    plt.savefig(figures_dir + 'Results_' + config + '.png')\n",
        "    plt.show()\n",
        "    ################################################################################\n",
        "    from shutil import copy\n",
        "    copy('trained_models/' + config + '.h5', models_dir)\n",
        "    joblib.dump(history, models_dir + \\\n",
        "                'History_' + config + '.joblib')\n",
        "    ################################################################################\n",
        "    history_plot(history)\n",
        "    if D_S == 0:\n",
        "        random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "        plt.figure(figsize=(14, 9))\n",
        "        for i in range(6):\n",
        "            y_true = Y_Test[random_idx[i]]\n",
        "            y_pred = GRF_pred[random_idx[i]]\n",
        "            MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "            plt.subplot(2, 3, i + 1)\n",
        "            plt.plot(y_true, label='Ground Truth')\n",
        "            plt.plot(y_pred.ravel(), label='Prediction')\n",
        "            plt.title(f\"MAE [{random_idx[i]}] : {MAE}\")\n",
        "            plt.legend();\n",
        "        plt.show()\n",
        "        \n",
        "    elif D_S == 1:\n",
        "        random_idx = random.sample(range(0, Y_Test.shape[0]), 6)\n",
        "        plt.figure(figsize=(14, 9))\n",
        "        for i in range(6):\n",
        "            y_true = Y_Test[random_idx[i]]\n",
        "            y_pred = GRF_pred[0][random_idx[i]]\n",
        "            MAE = np.mean(np.abs(y_pred.ravel() - y_true.ravel()))\n",
        "            plt.subplot(2, 3, i + 1)\n",
        "            plt.plot(y_true, label='Ground Truth', linewidth=3)\n",
        "            plt.plot(y_pred.ravel(), label='Prediction', linewidth=3)\n",
        "            plt.title(f\"Sample [{random_idx[i]}] MAE: {round(MAE, 4)}\")\n",
        "            plt.legend()\n",
        "        plt.savefig(figures_dir + 'Examples_' + config + '.png')\n",
        "        plt.show()\n",
        "\n",
        "    #############################################################################\n",
        "    construction_err, rmse, corr_coef, corr_coef2 = Construction_Error(Y_Test, GRF_pred[0])\n",
        "    result = pd.DataFrame(np.array([construction_err, rmse, corr_coef]).T)\n",
        "    results = pd.concat([results, result], axis=0)\n",
        "\n",
        "    fold = fold + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(results, models_dir + 'Results_' + config + '.joblib')"
      ],
      "metadata": {
        "id": "TfSe3nmrBxoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(np.array([construction_err, rmse, corr_coef]).T)"
      ],
      "metadata": {
        "id": "MNkoAawm_9aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zud9uaz-xgzI"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "gft_synthesis_cv.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}